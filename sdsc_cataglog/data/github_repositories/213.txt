repo_name: server
link: https://github.com/triton-inference-server/server
description: Triton Inference Server is an open source inference serving software that streamlines AI inferencing. Triton enables teams to deploy any AI model from multiple deep learning and machine learning frameworks, including TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more. Triton supports inference across cloud, data center,edge and embedded devices on NVIDIA GPUs, x86 and ARM CPU, or AWS Inferentia. Triton delivers optimized performance for many query types, including real time, batched, ensembles and audio/video streaming. Major features include serving models in 3 easy steps and client support and examples, with additional documentation for contributions and troubleshooting tips available through the NVIDIA Developer Triton page.
