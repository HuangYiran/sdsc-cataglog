repo_name: alibi
link: https://github.com/SeldonIO/alibi
description: Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models. If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project alibi-detect. Alibi can be installed from: To install from conda-forge it is recommended to use mamba, which can be installed to the _base_ conda environment with: The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize, fit and explain steps. We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed via `explanation.data['anchor']` (or `explanation.anchor`). The exact details of available fields varies from method to method so we encourage the reader to become familiar with the types of methods supported. The following tables summarize the possible use cases for each method. These algorithms provide **instance-specific** scores measuring the model confidence for making a particular prediction. These algorithms provide a **distilled** view of the dataset and help construct a 1-KNN **interpretable** classifier. * Accumulated Local Effects (ALE, Apley and Zhu, 2016) If you use alibi in your research, please consider citing it.
