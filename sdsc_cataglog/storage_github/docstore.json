{"docstore/metadata": {"b5e292d2-a134-4a9e-86ad-69dc8cda1d3d": {"doc_hash": "5974a82fd70a4841a55dd2f2b6062157b4ec5b7364617b6d6d8cc4778b2f5d91"}, "a33872ec-8199-4568-8cca-87ca03333326": {"doc_hash": "4b57432aa26d691093f028c6f1835ac59299fa1901a1b92e2271261d70a41580"}, "cf6106e0-f8b6-4fd9-8110-83752bade113": {"doc_hash": "eeb501010e2bf7c186da516dcfb523ecf9ffff8c2dc8ba76aea9eff65fa7bf15"}, "38b42cf6-0149-4170-a208-4b53d527d9d2": {"doc_hash": "532026303d170cfa98da189aa01cdef1e60fce06a587076169b97222df9fc63f"}, "351b7993-f448-4ecd-8635-0288100f1c35": {"doc_hash": "ade589436dbfc8eaa3b5966afb4c219b7afae9acc459c7d828157bb34881c676"}, "139b9d1d-b8a9-4a68-8c54-2553083d7e07": {"doc_hash": "4fd17df5e4c8836a10ecc028c6cb1c60f928369ddd5b10971659d1e7a74fce50"}, "aabfb308-4360-4680-b521-e061f877d376": {"doc_hash": "7048bd5882c30a4bae7bd771b354f1376481c86221f6e822b94b2cfaf2833604"}, "6560b13a-ac24-44e2-bafc-5ee91c0f8884": {"doc_hash": "1d9f8af2429ddad803132bfebd4220e7946ac60296207b55c10314a279717fa8"}, "cb57b4af-92f4-41d6-8bac-6e6967e6f8bc": {"doc_hash": "2b27be945e3e411c42bdb13fb05a0f7f5eea81fe9c56c6bb92bfd5c26ccb9735"}, "c97a0073-d1d9-4e6b-88e6-fceeb3e7e31e": {"doc_hash": "28f6cc0de6fbed56d29566c1625b8b5f997f0c073120fc39acd5fd35d0f07a63"}, "baeb0f1a-cc92-46a2-886f-9b3186cd23b9": {"doc_hash": "22c38b942f09300232e07abcb646fb8bd94a75e9e8399a163d3cc17dac241796"}, "0c2b9635-b7c3-4992-bcb0-b2b41675bcf4": {"doc_hash": "930af367995762aff6437e04c6f67d85249a63689c0435b1c1ab9fdb0377dfed"}, "8751e01c-546f-45c9-a18f-9072f84ece02": {"doc_hash": "775e03c1e68954d6db99aae547166bd712da6ca81603e2752dd168cd7ac8309f"}, "96495664-a204-47b7-bbea-f03ce0079376": {"doc_hash": "4efc834447c42fe7b613b0aa0bd0a438d9e1397003dce3d5dd7b763b36861ccd"}, "0b29e417-ccc3-4b22-8dfd-39da9db03f96": {"doc_hash": "819c75d105441d8040f480a9ccb6bfa6374b6a7df68403c3d3a7b857e8ae475c"}, "731197b0-1ce1-4ff6-8452-a3c6ba3e9bc8": {"doc_hash": "b63fa40761ded24ab353897757c6fdae5aeab294aa36f0d9117e594c8f41f301"}, "a272fd28-cfe0-494c-8745-5c6fdbf79897": {"doc_hash": "e7123c42e911cf68b3b264818ada612418fd0050b283e638deccbd444037c122"}, "40ed341b-1507-4cae-9d07-e6e9baf669b7": {"doc_hash": "7b32b980bcedcb497445e1ca4d1d2cd5d5e8c63905648c1dd66184d231ade623"}, "301cd385-0111-44ca-9961-a276a29c7e02": {"doc_hash": "4d814d9f9cc09947292ae418ca7839ac293f0f046877d4339770c87f1728886c"}, "82d4e3c2-b6cc-418b-941c-4e5692901e27": {"doc_hash": "6a462bb2844a1dea718719b9bf1d25eced5aae2a3c77b732b1df3b95154633d8"}, "9069c6f4-c561-45c6-8347-d92d766bf645": {"doc_hash": "25457e62a2a6ccedf7154f2389a0c20fa3c95865e8309369ad041be90b73ad21"}, "cb254119-f0ab-428e-a4d9-804706c1269c": {"doc_hash": "93114a11e0a18cf8ad5ce79193789adb0df6037acd515d7eac0c76e8218b1254"}, "d847c661-f669-4bae-b731-b9cdfe9cfdbf": {"doc_hash": "eaf3bba0171a9de432f3555e43dd4feef4203e226c09f049bb4a602d87a74204"}, "e5297a92-0aab-4af3-baa7-56b2bfe617bf": {"doc_hash": "ce9a09045cd36a3349fd8a7abf8406107b904175ec20c295fe049d72a9dff1bc"}, "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00": {"doc_hash": "7a0d092e5d99618edc4fe58a7095a9e793f7f3ad1845bea9377d5a3fd696dabc"}, "df3de19e-b8da-4d9d-998b-4f2e16fc0998": {"doc_hash": "71d2d36e297dd7184242fa79231bcdadd0d57fd828b59fdeb439f6ace37a2744"}, "e201afc0-6bc7-446c-b9e6-e0e132d9b77c": {"doc_hash": "af8bd90d8cc74508e2bfeec888a22be71849779f997acdfd99d5bf959e52e6f3"}, "f3fb87eb-71b8-4f8d-b9b1-2d37466816bd": {"doc_hash": "fe66e98a6e0cd22c8752ef2e881e96859fb83f27af178c92386249d8aeb88dcc"}, "9e85ca8c-b81e-4143-a7e2-5e66679a2913": {"doc_hash": "01cc189abbccb92f15243199ab2fbf09f9b5917bdba46e2ce7df0e27598230b6"}, "367ec997-53eb-4fd9-85e0-5edfc8935240": {"doc_hash": "688c67506d094488bfbf4c497c845b7d9c89328d5730160d4f1da4a58d3a5d3b"}, "73f46aef-770e-4d59-818c-7d15d8e6b2b9": {"doc_hash": "87048b1423280e3f3ab691fe28014681c58fb56e3dae1ca6bd22ed0f93c994b9"}, "a9ad1924-220e-40f2-bda0-6d0e6eae7aa0": {"doc_hash": "7d25eb54d8dd17f22272d3ea009867d76acf07b66e457a3ffec747c37dbe5649"}, "e9ae78a1-ffdf-4399-843d-6ce5b5247d7c": {"doc_hash": "6c44b64fa58438265613dfcd0ade31dba8d524be7ed58790a48993bfb3e6dbef"}, "3990527f-3cec-47ae-82a2-741edf3041bb": {"doc_hash": "a4e5553c14a31a282b7d1ce6dd4d01c521b11c11850d9b66130f2f9c42cf1321"}, "baafa8fc-3198-4410-8c79-7c0c121b2935": {"doc_hash": "9859ef5eb1805d44c7e90cc1f160576b1092f6bec9e6d7474d754b4c54f72ca9"}, "7212107f-33c2-4ab2-9949-7116da9c38a0": {"doc_hash": "b3b61727a02ce33b0e9d4f1e82748eaa39a76bb5a6b9335ac84367b865a13c6a"}, "59f980eb-bd73-4503-8be4-33e3ebd5e985": {"doc_hash": "b6227d349dc635ff620389e3ea4899dd3b01936648a2e445338ba1b8318d88a6"}, "fb7d2daa-0898-48dd-bfb3-a034cf33a7b9": {"doc_hash": "9bf2f0b4e4dcaff01c6a5c901b99ae352e7f26335b08e3101a1727a850c71019"}, "e765345f-1a30-4e05-8cb5-af9b246e3f8b": {"doc_hash": "855f8f0be0403ad7ab18ec80f517d8af31f277bc49a297bd9a69d26b0d780486"}, "db4fc2c8-86d1-4f48-b0f1-02a702b3aa3c": {"doc_hash": "7f6f11b96e6b1d78a7cdce0365f4e159f068dcd79fe155b85113459296403252"}, "b43f6d2c-aa71-4cc2-bef0-0c9f7a0b9cdf": {"doc_hash": "a97ceed5aaee739cf54da85f84c499617e750bebcfdb2e75a74cd5380049e0e5"}, "122052c7-8a0c-4c26-ae97-7309864955b9": {"doc_hash": "86f947381e7f7df98d275ece0cf05c38e1d50c35a50b864a078041fa85e15d23"}, "7de105af-319a-4cee-96d2-dfa5b88cf140": {"doc_hash": "3c5936378685e43785e50f36dbf28d39cccd7615cd3bf88b56d81eae027bedb8"}, "8b7f361e-3568-45ec-b415-6135472503f8": {"doc_hash": "14e4731e27e7fb69eac1574a6238cf900413b23f30f3deee1982e098b5dec223"}, "4f692231-45d4-4f4b-a63b-b06028d512e9": {"doc_hash": "17dd3189273ec415b3331755c5a0eb720b650c62a0f60ed3cc3ea2c7a9d80637"}, "b3e56570-1c06-4ee2-8547-00212530db25": {"doc_hash": "db0762a87badcdecdf50b9b23c1434ff49ce538dd232a59d1485a746c52205ad"}, "d9f35f6d-bcd8-4023-b9d2-e0398bf942f8": {"doc_hash": "927aa909392d704a263198484f9c44726cfc3efdac4df8671d5951359445ac25"}, "2cb2a6f8-ba4d-4f32-bc2b-43fcb62583cd": {"doc_hash": "2d99df50af2eb9e38ca60f2d07d66d7e813a3b62105674ec573bc3469f2b5cca"}, "7b34569e-b2e0-49ba-b688-14f3ab9c1a9f": {"doc_hash": "3e78d9699634920b811956fa4d9af06f28c305f7a7f05ec1546c4ea4e687b743"}, "89aa0982-d92b-435c-b00b-131dde63e9b6": {"doc_hash": "9b2583104bdb35a43ace73d4e8670c99c2a465974da43cae3b7cbaa17f614629"}, "892183e9-fea9-4b7d-ad31-6ea876f5a171": {"doc_hash": "f4af595d1e366c42b7261ce3ed31d8208e0c34108338dc1f70085f4be24d9480"}, "5659ed1c-d7a7-42bf-b884-e9e048361786": {"doc_hash": "fe07364b3138029b552f3b3bda3476ff9e6c83c714f836e12635f616c0031922"}, "09380548-18aa-4184-ab0c-2454dd0927dd": {"doc_hash": "37e01919dcc2302cd975f0b7c0db8b0e4eeb1f25b1f4854eb8f1d7e6f993e647"}, "36a12fe1-c86e-4bc5-996a-f7d09ee9fee9": {"doc_hash": "90a2dd1938e00da877e47dd38eac259a2a54fe086af2445b6e38e719415824ed"}, "affb6e22-ed28-418a-9f50-05c93afd0763": {"doc_hash": "f55c543b43536b9027053ec9643041b95ed62538ed0247eb5387f6f07162f007"}, "30ce481d-8967-4d3f-832a-c6ade99b46e0": {"doc_hash": "c68e16af802d71c308f21944c291cacecb8a3526de6f2f4c3a364cf314721694"}, "dc9c5841-73b5-4f72-b615-be4b1ba51faa": {"doc_hash": "a2f9c93052f0ce48b52debcb12da6b21b5aa2d17f8df3a361fcf1b077c358915"}, "d13ae74a-8073-453b-8099-5558a4293888": {"doc_hash": "cce7c16c770945969e93174395dc31272b6ce398c8b70ea4412e3420685900aa"}, "d6bbd1c2-97f4-4eff-bb6b-014db6b718d3": {"doc_hash": "36749d2da2deb699241728129ee03c6014cf5be90b7173f0cafd4617341e8885"}, "61dcf961-7eca-4fef-b222-a6c86e9310b8": {"doc_hash": "6e122809f0e4bbc8e02e05c704f12af5ebc4c91d44fd48adfde0f0610a4aa779"}, "0000001c-2244-46db-8588-916c3be6d671": {"doc_hash": "e1f185585f98a5d5d5341102fccf0e2539d2c3f16091d74a6840fcc0cee8a390"}, "e18191f7-ad15-4f14-8c45-a7f6879d485e": {"doc_hash": "1b607938eac9cca6de7daea25f2441c94ed10c0868dd42a27672b3d97a25ee4e"}, "d89ad8cd-a5b3-4c66-8626-63e7e25878f6": {"doc_hash": "f08791547bbea4c489b9702fccfbfe8c5fda0832c9e4030c4106fdd1d7a78ef8"}, "a3cb979b-edef-4ad3-9457-0246e60c8f8e": {"doc_hash": "20c436ebcdaa93421db6db6d1f6736ae6c9925e7bf9e43da9718fd36ea9bc393"}, "c03a63ed-4b7b-4f31-99b7-6b0475267417": {"doc_hash": "4f66c4b93aa21db11bfb7f8cb4d49ceb40ac4d2d5afdfe78cd336eec98ff0a5d"}, "27f9f4d0-045c-4dec-bc9d-5f2b33bc10c1": {"doc_hash": "b37547096e70c0f147de8e6c27840d3d743d2c063db8728c17e6a08125766800"}, "a264463d-7e21-4ae7-8366-e236e310daa8": {"doc_hash": "67fc915f5f065f8b04242a0c849643ea8a11b5ec009b6daa687cad38867e109d"}, "38f0bd10-1ce2-4015-bb6a-6f6ff085216d": {"doc_hash": "d63e9a1782b3b33a4b74ba104115e89475abeab965dbe6414753186f52dd2bfd"}, "462d6170-8c76-444e-b308-e4900cdf71fe": {"doc_hash": "ebee45f4d1b9133a8f3cf5e4c7a139f8fd4ccb36befe583348ccdfc491d86169"}, "dc1181aa-9fc5-4538-986d-617ea6ee6245": {"doc_hash": "a3bcf498ef33b43430a399613c357ae8d4653be57b67a1c48d3a591182b9c052"}, "716ce4a5-53ce-4e42-a5de-c0c5dfd9f60e": {"doc_hash": "7edb8f249e266a27fc367a2e5f7597d5296d40309dae30533d6c4d1af56e956c"}, "e83a0796-4953-4f1e-bf3d-3333d778ef97": {"doc_hash": "cc30aa75acdb9dd13f4bc430b0ccf41acec9fb64a8f00404fb801ce129cb1d71"}, "63c60423-ec78-4506-bce9-0bd23fa1c367": {"doc_hash": "2f875ae99ab49492e53584c228187b2e77053b5c1a037417ec879cd2a9468cd2"}, "49dccfd9-335c-480d-870c-8943fae4b87d": {"doc_hash": "e446d3fb39c430a37eca3e41a0630537516facdc4dacebd046ca60c19c09bbae"}, "010a51e2-c61f-42e3-bad5-64c74c7c09aa": {"doc_hash": "a9830671a54cecfdf71d1d4d6cd04597882173de4158a778d66a15fcd06d89cb"}, "78bf74e2-f33f-4af2-a01c-f9caa88df9fd": {"doc_hash": "a09ddd413ca8c523d8ed7bd90f70c76bc3415a9d41ab78901e4cdf9478526409"}, "87ee2a70-6212-4e20-ba00-4de444782e49": {"doc_hash": "3b967bbf9ad5390862a4a8cb9d91001af6024341f12a8d8029cacb59febddc11"}, "4871d211-2c6b-42a2-b9ce-0d377c287e1e": {"doc_hash": "fd94ddf5016ecf269eca9a46734a2d64b589ef655bb12fe2c38d1924387ac1a4"}, "37b7993b-8c2e-40f9-963e-9b6f99317064": {"doc_hash": "a0f61223019f4ba686b3a2de67416dcf0abf0ee34ede6706b952dfe23605a910"}, "934d0538-759a-44c8-80f7-eadfde381392": {"doc_hash": "37bce3afff06238f758f1d860a364df9d7a2cadec259c19099a31d082596666e"}, "7d114870-ea8a-4678-b740-30cc2f4dd978": {"doc_hash": "81aaa4ab347eb589c498214d087de7bd53d29dcf4b1cd58ac933716cae7313ae"}, "4acd7e19-c74d-4352-a754-6c0abbe6974d": {"doc_hash": "89bc94b5043dd46835781b2a106f5d651b22f311d5d385af69b74c5dcd66ef62"}, "789ceda0-8b18-4d86-8e7b-dc5a1e73ce97": {"doc_hash": "d51ca57af46d7b61896586287dcd3f42b73c96819be8d88db015fba6d1799a81"}, "a7b4fc38-010b-4fad-be2e-cb1516718134": {"doc_hash": "f4053cec75f993bd028510dbd7bf23b3e3d454a0dcaa8247a48fadeba44c78fa"}, "cd80c7a4-e1a6-49b9-923c-7b9d7312f54e": {"doc_hash": "4b887eb0951caf67980d58f29541899cb682044b8c44c57c2519eef289a809b7"}, "afb3ead4-8175-4f44-985f-641cbabbf28c": {"doc_hash": "b3a78489e5c62886bcb83b4a8af1198455935511c5d09a5f5f6973e395364323"}, "157ba0a8-bdfd-4208-9cbe-69a021d510f7": {"doc_hash": "95b481e322fbbbeacfdd37db4f539a77b6761f95e647cda5b78b9db065d682c5"}, "670c0479-fd8f-4b3e-adb4-d20c8cde8510": {"doc_hash": "54f65b7e7eebe3f2cecd797c8f3dc72c7ff4c8a68a8fa37fdee0c0db761433ca"}, "dee94cef-3594-48b7-ac4d-209f1fa4dc1f": {"doc_hash": "7e38efd157d0735b05dd8fb6e1b99080666ad40fc0ddb991f90d3ba95f46eeb4"}, "2d41b507-718b-40fa-8d38-9aa41c328430": {"doc_hash": "a715303cd583af7be4b9b004617f192b937a8b05ff83c0ced59535652e6e18c9"}, "13162bcf-3ee3-4920-b0ed-18649219bbf4": {"doc_hash": "14d6e0cdcffe308e6f8472c0c5f6cd46471ed2236f4fe3d93cece865083cd2a5"}, "2ccaae91-a34f-4989-92dd-625d712654f6": {"doc_hash": "f417ff9e9afb61df33c1d81a678e95693a6a15a8e770c13dc049e30a432936d3"}, "e2534ab4-7337-4321-bbb2-bff3c7e5327f": {"doc_hash": "e1514c5b93783d6fc983e911d9441cd5a1fe2294f0349942d999a7822fd976eb"}, "4ec41745-8d3c-4639-8ace-b7b8e8dcef4e": {"doc_hash": "d32a52277d4cf8afc216edf3ea22a1a3b1ddc0522b0cd1a4baf2b1ca9973e721"}, "d4c8753b-98f9-4463-9544-9c6650380038": {"doc_hash": "c7ecba61b87fa4c21544f0bf1f13e64d1b68577a3ebb0dec6b2177899233a389"}, "0dc9ecce-6402-4382-9260-3330b72c19f3": {"doc_hash": "4e05e7cf85c0f8771b378de3db382e6fd691bd53179fd00b90c20bcf4afb5dd9"}, "e368e7f8-2a49-4fb6-81e8-7635954a667f": {"doc_hash": "bbb873cb0e775ed61eadf5c2b5883fd62ae9721d7cc18c0fb581eddc7d4ebd80"}, "89ee3ec4-6c4e-43b7-8c13-6d24b39fa6a4": {"doc_hash": "1218ba8015b90b9cc721da82881fe860ea9fe29f17172a4ab95c21749ab7c1e7"}, "236e95b0-c5ef-4e84-a2a9-90c9aef518d2": {"doc_hash": "b639dff3fb1df64b33c83fee1e5414045fc929bd6764f8bd375aca13e95d22be"}, "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b": {"doc_hash": "8820a9f6c54cb0d19a550a481a917ba67a8d9c02ff574a385fd8975bed9149dc"}, "6d89dfac-cd2c-4930-9e00-798a1a6d809f": {"doc_hash": "dffb989d9b136d2e5dc989d00537b11217b7acba0db4655d4d43c3daf0c22ba0"}, "5dab64b8-9499-4b96-8bc9-daa95a7b6f48": {"doc_hash": "513986530e84d098855d88b2d819895b226c5b8b3d64657c8efd38236d3990a5"}, "264916e0-4c2e-4106-a71c-8fb69e8ef786": {"doc_hash": "995aa80ee984bd183b6f65c0da10bd0ea8b8aa24d30ac9c82047bc0c58c508d2"}, "df6f39d5-7d24-41bb-a50c-2a0781b36859": {"doc_hash": "d188af2a0db22bc2ebd1f35c978d07383e92ae969018008a5e868600d0a7fd26"}, "8fcec702-fb6e-4017-81a5-166f163b922c": {"doc_hash": "735d01f7041f675f83248590df22c8a860865adc74f937341fc1f1abe0f1d4e2"}, "b7f1251e-a42d-46f9-8496-e1b620f6364d": {"doc_hash": "97dc5e9aec2bfcf7fbb8fe05e4266ba338c86f29780743ac4a36072fbdaf072f"}, "6d16cda4-babc-4bc4-960a-1bacfab43bda": {"doc_hash": "5c8658fd3dda81d39bd5b489b07460fd74d18a2c81d43654c1a5c3926e4afabe"}, "22c55992-fb13-40f8-806b-0611273c8547": {"doc_hash": "feff674f88ca224913c086340c7abe55c651b9a8af3a3dbc31163b4f41972162"}, "24dc7cf5-5234-4aed-868c-d22f29185bae": {"doc_hash": "758585948ae3e682d4d256de3bebd32d3956b0e84ac718d978df712f8438c49d"}, "a5b1707b-c225-41a7-90ea-20a7fe5a7c72": {"doc_hash": "5d078ce7564b5a0a744ef3eae864a3ed1236333e29ffd11bc37cc1127baae4a3"}, "2b6ec820-c642-45a6-b29e-74c002eea37b": {"doc_hash": "7806ee1b80bf0dbb098feb67c2d2527823bb616196263ef30730a28db0456853"}, "9330043a-e89b-4a01-bd74-7ee21012814d": {"doc_hash": "eab5171987623ade90fc6893d6a7ec16e014b8e6690c2a2ff94df8dbbda36eef"}, "1b2ba341-d75d-4f78-a10c-634f35ab3bab": {"doc_hash": "77762e06e416d926609ccc5f0cf9b54dff0f3229542eb85fd195cd0288e2c2d3"}, "c7f6fea0-7202-41e8-b8a8-6941cb6d0b60": {"doc_hash": "322f274514154b6e0be47c428044906d9640f64a6fa7d02739c91cf7e03f3b1d"}, "ee049f5d-5fa1-41ba-867e-f34dbbc1b52a": {"doc_hash": "5637fb012e317b911eb22d318b900aeba055d50f34c8b9dc2dbc405495974b84"}, "2605691e-e220-4ece-a16c-f715af41d148": {"doc_hash": "cada060f67b3f30c37ec6406188dd4b1079a00997cc9ea59c34a619fdee0ffdf"}, "2eadce5d-225b-4b96-baf5-d94f122f10c3": {"doc_hash": "c8a9bfca37a8f6372565e586b573410147530ad8f535dbf0b53bfabbec84bdd4"}, "58468981-c200-480c-afe3-3d74915299c0": {"doc_hash": "9d2acf0db35e561c64e8a85ef1be16da4171e063fb72c763ff4231043ddf64f8"}, "6a62499a-831f-423d-a292-0f923580462e": {"doc_hash": "4a96f6c18d00949ae17cce3a7a09ec50837521a080c724403122b8311f813c0c"}, "0c04190e-dc1d-4cf4-ad1b-4b4b2219d1ea": {"doc_hash": "71c4000e6131c08a640826181fc8a33ce493b2371ca352277d737de1c2aaa57e"}, "e469725b-c578-4a35-8d2a-f04eb507b78c": {"doc_hash": "e9ecb0ebf721b472b2b672d475cfd22847d017e92b886d064a4745c303784a76"}, "23350226-4c59-40e4-918e-83833f39ef1e": {"doc_hash": "bc734204673af921b7592ca16abf49ea64ce4de9c473ce990a1a51d8f99829c8"}, "9de24fff-46db-406e-8f9f-5d9fdabd019c": {"doc_hash": "d81099d130b20e39809400db5eeb70e99f10afa284ff795b2845aba279b2a3f3"}, "6da94435-69a2-4804-b6db-5650b426e935": {"doc_hash": "077ecd76a5a3aea3adce633363639bc9a73dd4a03b1347a97031ff2e49731639"}, "0e315a7f-841b-42e9-8caa-7da2831a2a23": {"doc_hash": "044acc9e51a6a068a5e6d91ddd14d1b3996f16523199a6dc1cd9b3f973e57dbd"}, "b7d0d0c3-f210-4a45-88f8-3cd5930ddfa7": {"doc_hash": "e6449f43a58da956830d5e3f7ecbfe77c48daa31ff5dfeea3f150ccf6baac6b7"}, "da8a2c30-5865-4755-9d0c-39b1f3447d96": {"doc_hash": "97e96d3d3f6339db59c98de7213d68c70eb058784cc80fd96182a6655fefed1b"}, "abf962b4-bb99-43a7-bdaa-4bc4281791ee": {"doc_hash": "8b9393fd0c49c5df4d8e340d03c57fa8d1edc2303108cc98b50ebdb6374ace55"}, "49bf831c-28bd-4d22-b81e-edd06f3606d4": {"doc_hash": "c273d6996eb2a2300d56911793e349a4e730aef9d3ae330d58c12f642437ea51"}, "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5": {"doc_hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7"}, "a6ec49f4-2721-4ba8-9946-4b52de895769": {"doc_hash": "54a2bebe4b52bafd32093c0f61e4da5ee0a4eeac392626ae04d40209aada6b10"}, "6dba962f-eda8-4374-a375-69ac4d51998d": {"doc_hash": "eeb39b5524a2f87c10c0b7c0817b7a5c6b4f8d313b492164280e3a3e5337fb89"}, "0c722044-a4cb-40a2-9f4c-6a9b44d69d94": {"doc_hash": "cfa8e0139b669dc91f815d6a8f40146b8e38e794c0169c4e59a688580b83bb35"}, "14d39e40-bfdc-4c06-aadc-0e77c175a183": {"doc_hash": "1ea71533118a8d14addcdd3f3a9bc352893d5fa885bf24bb250bbfaf393d6f64"}, "93f4ac72-646a-448b-8d9f-940d48b8a556": {"doc_hash": "0bb4a9a4db5a4f660890e32b6dada48ce83607e3cff14b5e2929441032362c53"}, "cf1b32f9-cfc3-470e-8f36-a1fee2153929": {"doc_hash": "e680e760d0d751f218a7c31e62b8ae12284fbff61a90f71be7a192b7e75011ee"}, "c9affb97-0c29-4021-baf2-863825851d58": {"doc_hash": "4fed7b509f23711dd295a78d7c967837cad9bcc8113980246a3c3204120c5b7d"}, "e493a907-1754-4919-bb87-2f7bf5cde5f9": {"doc_hash": "3fb1b60b430ee5a599fd41024b7d75a2739995a88a8bb46dfe055673dfaec5f0"}, "d4b63884-e2d3-4a10-80c7-7874da26bddf": {"doc_hash": "96d3ac6df0259c0cf68aeef9b90798648455e23c5ac66f74b6a5d79fa38d17d6"}, "99a871ed-8ab0-4918-9dc5-c75a6957d853": {"doc_hash": "801bf6ca41d6f1a09c5f9cd175ad61a549c4ca00f92000a1770a6283f5191dd6"}, "83412d26-6498-4f0d-8b4c-a5fedef10173": {"doc_hash": "344ee2134a3fea19fcc0e2ab3df6a6ef320aa4a7bde2dbe1ca149442893a8f94"}, "18b97950-5b23-48c2-abaf-b032f82d760c": {"doc_hash": "15e4793128cf176d1f52ec545002ba19af2c335a1bdca0ac261c81d73b5a697c"}, "7186d9f6-cba7-47a8-a2e8-57cb21e56e79": {"doc_hash": "a9467ed5cdc9d0a8b6190f71c1d859d3b96c33d507e364552914fa3154c74556"}, "001e396d-cd75-431f-a765-e29eb8ed0bf1": {"doc_hash": "9212c033f68bb601f72a92794c50bc2ba4f5443779208e565001f700c6ab2e97"}, "4cb0c00b-c699-4718-8736-9d0c2c471abc": {"doc_hash": "b908d7cd46b1ebfca6d49fcdd70db6992f51dc70509d5c26bb1c7af135cbf702"}, "8474b7ab-88ce-423b-972e-e5a20c38d4a6": {"doc_hash": "59050bb8b9d6a3e536afca7b68d3f12cfd9fa8a6f4869dcb3bcb117e7b2d887b"}, "6244de17-417c-453b-be2b-a68eb683fc36": {"doc_hash": "5feea03aa01c9ab6b26ba1fdca1f73b179507e9bdd109d642655a6a6625011e4"}, "4c60d868-559f-42ab-bafb-61d5c7a734b5": {"doc_hash": "2f6aa2c10b7cd7f871f823c926f6f50c9ad9d2b276c7364dc5f4f04d63480a3e"}, "d76129e3-edb6-4b27-b369-742c7e2e3ba5": {"doc_hash": "fb8710d0eb597fe154229f5598780407e6e09dc8b92e3fc2078b8e3ad8e67777"}, "744021de-da76-4255-9386-8184cbc246fb": {"doc_hash": "b4b2aaa2658d6fee479f6491b90e1888e32975f23bf54154e5b82ba88d41baa4"}, "a4a5584f-1c74-46d1-b56c-82e30d75c5cf": {"doc_hash": "d470648b72039f4f37557fc15d1a7bf8deab89821c24411616a15684acf689d5"}, "ab6c6f17-4ab1-4329-b665-1ee1070e2ad2": {"doc_hash": "530e38f753453fb7a9f18676d1cda7443299041f8bedf670d71b27eacf534c5f"}, "8c186466-93be-4e75-bdb4-e9c3683f8678": {"doc_hash": "2b2fe8fe01cbc5b8b99bb438f1c6e66717ac75b0f859b50d916a58ef3cf3327d"}, "067a8088-66ce-4ba6-aeee-0ae4e4bc2f5a": {"doc_hash": "a2a121bd9ed95991a7940c6ccf306e7ec5596bbbe1ec78d8a9a37b222e20159d"}, "f1379036-f5c2-4ce2-9dc4-1f07d99a8846": {"doc_hash": "7e631d3fd5ee0b670789bad54c50a673aabe68700039516dc44226284f791c35"}, "de22733b-502e-4dbf-b070-9d41a6928b9b": {"doc_hash": "e37803aebe1e170dc26f038cc94fedbaff3a566cfa84deebd5886cfd82aea604"}, "7a41f28b-e2a8-478e-99c1-4ce440c2a4b7": {"doc_hash": "f15c05a328921b147b16e124a4bcb82ec73fc02a64c22e0ad73db4392b73b6d6"}, "4b27a818-bd96-4a1f-ace1-345e5d3f058d": {"doc_hash": "4bfe9733ce613bfb3e09ad8d3182dbe3494a6dd41d875ddf0a101ab83c700292"}, "0b9e0307-70ee-4f2b-8019-fc806dd17dc3": {"doc_hash": "fe211141bd5274637ca71c59bfa9fac2eb8881ffe94ab66cb64dca8214bc2887"}, "fbffcb8c-4da0-4423-8925-30d3dbdce156": {"doc_hash": "a28ac099cc6181e98180003c1de3223d3f29f082eed9a07563f4444a57fb35b8"}, "7b25e27f-fa12-4e0f-9e53-4a61010bdcf1": {"doc_hash": "ec435e8d8d7a3c0e21fc3a753ad309517916b5ae0955e28d3a3bd4cf3bc18bdf"}, "692f4a62-d411-4ef8-949c-b73941e653aa": {"doc_hash": "6524083997467b0b08de8565e7609a8aa6bcd37aa6ba17690286617d0cb7ec12"}, "583508e3-d4be-4c1b-85f3-5d557b1d2220": {"doc_hash": "b9ad48f964dfa8ec4896d7afd8146cf2954be82c397d2accee060fd2848d2178"}, "c13c6518-9797-4e0d-9141-7049fe1346d5": {"doc_hash": "7cf5b5de90cba8ae431f72e2764cc54a3323ca75ca6dd6d150f8892fe134e775"}, "d6b670f2-0215-4223-97e2-b2b3a1102108": {"doc_hash": "dae798c412ec35fab50eaa31f1b7781e051d46fc227f63e67e79f1c525e6c1fe"}, "6b36ba21-c6ab-45e4-bf49-a849a2b98c73": {"doc_hash": "febacccd8c023366b1e030801db718352bd05de4058971dd0ff1cb829428387d"}, "e591e5d6-8d4e-45e5-a792-19676a6d9605": {"doc_hash": "f86fb72b3a7b83fa3d534c42fbafc56fe98137f5a586e2532616139746b95416"}, "13983090-f65f-48b9-b71d-bd4c9aa8490d": {"doc_hash": "8c1f3bb0f1237255e366c92cef7f9c4df1b39eef534ab16eae98e12f26f4ed4e"}, "72554f62-e08a-480d-8392-871cf190eb85": {"doc_hash": "8bc747d1d56de1fdc993fe23608bb35c7be71da7b721450ef761190b58a2f864"}, "4fa9a752-9ff4-4586-9201-c2ebb5a2e67f": {"doc_hash": "ff23c6297c47a8c9e69620ff7ac570441660969c80c758ac21ec1af18aacc97a"}, "d3eb513b-74c4-466d-b043-b9fbb99eaea7": {"doc_hash": "ec00643748c76067c4aded80f134abec9ff4989f62e0f17e66c3280ebaa4c985"}, "8658ee9e-5f94-4165-bde8-5eb944f66b4d": {"doc_hash": "c80f5c87353c2062aca301a0408d8f3681dcec98c9bc5f82fa992899ceb0fcbb"}, "37041e6b-f3af-4bbf-83e7-7a6b054dd476": {"doc_hash": "fc862f09188684a1a0c71326160ea64fcd66833d7136d2cdc564313b5a723c41"}, "40b2b11c-e13b-4b31-a3eb-e28c95e4bed2": {"doc_hash": "764122612d2dcdcdee065b92aab461cafe2bfff36cd2d4e17165f4e65d023fc4"}, "bed6ce8a-dbe0-4ef7-8e1c-167947e586b9": {"doc_hash": "6fc147b702aae2b805b253ae485eb17248444bd420460a300018bfbd5f63a844"}, "e442320d-4037-4bc7-8595-edfa35ae9456": {"doc_hash": "f4517d6e4277e2e2626809a58362392b0b2bfc3b04313da88a7793eff3e065f9"}, "b75d4795-94a5-4cf9-8df7-f7ed22423be6": {"doc_hash": "39e8d89e8242523384aa553886263ca71aeb307bbe45569abe8581e27364b1b4"}, "410672fa-014f-4455-a82f-9e20ca8141a4": {"doc_hash": "2128a6099597a4f21ca5c0b88358900fbad7933110b548664531a41a9fa8f11e"}, "6369e180-1485-4fc7-9815-1239aa7b1246": {"doc_hash": "009f65d5f8c797506b59edb4bfad6e67f2378a437e748b87af7091f5d594594c"}, "4f8c9b27-f5c3-4269-ac33-c14d2e3186e5": {"doc_hash": "a47ad8326550868c2b8e1a0833c2f1435746a0712e6c8b4e628a8adaaf2acfeb"}, "653580ba-9804-4817-a658-1d654d434395": {"doc_hash": "28e5378c97b1b5514d2a64c997f8aadb302993c10f9c604e789da65bcb6b8fd1"}, "a7e3d947-82f2-4775-9941-b5643f9482b0": {"doc_hash": "646523f292e006c4c5f9b0d701148f2506ca17125a6f352bccb080eedb5ce0d9"}, "229093f3-3138-4816-9550-b7625c8dd1f9": {"doc_hash": "8949e8c5172a2e7deea18854cf3da8ebb1ff76cfaefc5b29a341367c3d523366"}, "3384521b-7f77-4c93-b6fb-f3e87ba96198": {"doc_hash": "453d5eb78bcfae0268cb3ceb9d4352c80f9cd83178b7bf7d6bb49b7a85ff1f8e"}, "8b1b3d92-6a39-44d3-a7d3-297ccdd96c5a": {"doc_hash": "b6e4a6c7f73d1625e726b966ae7f13d6acd7bd2f4eca1fc4afceddbb7339ed4e"}, "7dc4c0d8-e421-4b38-a621-5b14d3ceddac": {"doc_hash": "97abfd90ff35a2aba9cd714701c9e3bc5e8126d8d08d87e9489e20c7b470a524"}, "d4782ab0-30f2-416f-9324-abf42622fcf8": {"doc_hash": "2f494ee08414e7134d3f5ecde5b8e9c2adbeb6d85f687e705e9c7d72d557b2b0"}, "0f799979-c08b-4342-bc19-8dea7255a769": {"doc_hash": "849a39c1afee9e9b620c4ae6d851a0a79bd93f59dd9271297f840340f3dcd127"}, "ea472040-355b-407c-b0dd-021b5ccaae9a": {"doc_hash": "69f88645a840b60897a992fe514a08347e4817276909d73d85f86dbadf1fa267"}, "21abde37-b7f3-4706-bd70-608bd96f42ad": {"doc_hash": "ff2487aa3e7c6212e4c5394d1db38c0347115f32c9708b32801013d559a0ebcc"}, "fc13cfb7-e4b7-48d7-a96e-fe1551a66467": {"doc_hash": "b14dd679d6cf02c5295e708263ff404df2f4d7b9187c538c9af4bdbadda31a65"}, "02145cbd-934f-4faf-93d3-383d83cc3fa4": {"doc_hash": "72d72d4d7b715dfd37d0e28ced96efa7da674a84228d9e49596e88a61bdf3ca8"}, "8664dcc4-e923-484f-bcee-cfda6f77f0a9": {"doc_hash": "5483662b36fbf32d0f96ee16424d33ab8e4e5aa9a7eefa941a594b3e153eb6c2"}, "ce26248e-e970-4ba2-8d9b-c5a97084f5de": {"doc_hash": "ddaa3c5570a901cda2ebf36fee3525fd426bf679bf5ada09f6c4714670382a6c"}, "ea3dfee1-4caf-4a42-a3ae-770e5e05bbc2": {"doc_hash": "80a0874f8852dfbd103cbe93e577e2d79bb90134a4eeec8c9c046ccec761b21a"}, "83e97aec-bd7b-4da0-acac-b3a27a37e031": {"doc_hash": "6b5f02fc39ee3a3de3dabbaf38d0a895c6e99985c9369a137a9c81fd00e214a4"}, "0c77cd49-6487-41fb-a280-74d979e8a698": {"doc_hash": "e45f5246da13b2979177ada02c824a508572e21d859bcba86da866e553861df2"}, "0da429c1-bb29-42a2-b88c-7aed00f6d1de": {"doc_hash": "220926062da081fb49016ed801d5426109b3363c2dc0aa617dda61fd2c636869"}, "f9fdb3ae-ad98-4985-a026-e07b64492b74": {"doc_hash": "6513d6dec4f1ce787af77713f6378c02d8551ce09b8915870b97a258a75fc1e2"}, "884a6e5e-8476-4039-8546-7d19b636f3c5": {"doc_hash": "a8bf71379be56ca52653033fcd603f68565951ee4f2a14509b67924bcdf2ab34"}, "b85444e2-1908-4b33-97c6-fffa69287381": {"doc_hash": "e925d8a5560a9f0b93b4b2e80e61cd49bfdd75dd2746a194ff14d32649b65f0e"}, "14263c8f-90e0-464c-8555-610a44d14da1": {"doc_hash": "59c75bcc4833e0728c8f5fa2f1f20b26f77f2a068bf91e13aed4cf77fa0a78ff"}, "c2bc1ebb-5c27-4528-ab24-5d371fd4d681": {"doc_hash": "09b681efb7745a467bd285d27e04be04824caa19ed2c85811f12a8372cbca547"}, "40c1e146-3641-495a-87fc-df10d4045174": {"doc_hash": "4cd8957832a22e757bcf337201a0c4bcd8abec7607dee137a52ebb8cdd122259"}, "67e1f39b-a8fc-4a1d-9564-c2704fc50f83": {"doc_hash": "32b802337d327828f8ffc6550dda0a1169a2a0c7a18bef53981e08f000978bde"}, "f8a7ae6a-91f7-4efb-8c9c-8922385a814b": {"doc_hash": "2fd3f9cbc3432904cc726f998face8f24ecf3d42cfa966db66b383b447423fc1"}, "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc": {"doc_hash": "93dc00553822211cecdb68f72a9fdad84ab34b7221998bcb479c88fca8b40e8f"}, "7f7267bc-a13a-43cf-9ef7-db2c9ff1c252": {"doc_hash": "999a4e65b9a300191334640f358f0e3834bf9b7bd12a9e7cbade228ae8c607c9"}, "61e4a964-0740-48d8-8268-f714df1b37f6": {"doc_hash": "2d67929e79d608f47bd4b3ef631679d12c1f389dcbd91a193d9bd41fc1f335a9"}, "dd75d9cc-0703-4376-9260-1172239c346c": {"doc_hash": "5749bec9973c14ea6c9cbfd11a86aeae1ec3cf6f8be2e9fb7f5508cfc8ba3b80"}, "498780bd-084c-436b-b101-114bd40fc00f": {"doc_hash": "7948ab4976af7d1aa4f0f3ee9565974c6c7fa08f7929613b2c053fb60c75ea98"}, "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a": {"doc_hash": "8869a8af3cba498debf5c32acdf00306ab13778f90dbb889c620bafdebf21815"}, "63ad6d6e-f431-4dd2-834c-381c4698680c": {"doc_hash": "1a12668fe53308b69d53e029af1483824aba4eee1781583db2a61114db0a1c28"}, "a7b811d0-431d-4178-aaf5-69971b5cef81": {"doc_hash": "dd309516bd8b90874087cadaefa58b342151c56f387fe70490827dee64e5ea7a"}, "98b7a51e-d10e-4478-bcf1-11a2c20fb61b": {"doc_hash": "98a4b3b5d824e9bcc4b5e5deeb1e8f7554be3daf662176dc6d345fbec806f3cc"}, "ad0d3a70-5a2a-4f0a-8714-24ea680cd55d": {"doc_hash": "95979698ff9d6e1117d216e86f290d1fb528e044d243cb6085a9931b43749984"}, "228b53cf-a254-4aec-87a0-a7d8de7477a1": {"doc_hash": "fdb941dac91520ccee66da8d36a0d5f5d544090245d252bd542dd7cf98823c16"}, "2461ef18-ddee-46ea-8a3c-09b960d4b6b4": {"doc_hash": "f4484c7cb872959fe6a70666595c0b0fc33129e477acaa3b4d3d4951722cf097"}, "0075cbf1-8468-4518-b3fe-a27aeb62c3db": {"doc_hash": "18ed227899eb273c7bcf54baabb7ae977e82edc4698584ea78859b641c1268d5"}, "1bd5f3e2-0a5c-49f8-9a8b-67b5da7d4e2c": {"doc_hash": "d04a8df338d153304bb463f8cc741e17916e9298a688a9da18de4f24216aa8be"}, "df301cba-68c1-432b-8dd8-bb0de0bd1872": {"doc_hash": "57c002346ff97d99a1c94fae4201e3613ad39b245bf232014b26e58d2bfe89b1"}, "ff31062d-2f43-423a-bf02-d48212ee969b": {"doc_hash": "8dcd32ce0ea1f9660a7a12e1a8579a04ed718adb7fca4717130cb83527a5c9b0"}, "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd": {"doc_hash": "419d90bf6532c1a8ab759a14e90dbb716b5f3011ee56888a3fed11f2d824dfe7"}, "fd0d0fcc-2b2e-4b73-b503-deb56f81463e": {"doc_hash": "3f0353fa866c6b6ba3dcfb512881313061a4b488f207b21e84fb8149bd18a25c"}, "63428642-3509-467a-ba84-47ea8d8e92ad": {"doc_hash": "6e07dfec5bd5315fd62038068d3427253c1322242e46f8738464f1e615874886"}, "6313a2c7-5be6-422e-a676-771d1ba19f96": {"doc_hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e"}, "228687d9-185d-42d4-bf1b-6a0ebf05ed48": {"doc_hash": "302a69300e8a4b5fb2f1b0f7b664823e1a45c1effd1d81cee35d7b3f7514907a"}, "4d41022e-a1c8-48c4-8925-cd58306a3947": {"doc_hash": "d4399d9a13d44ba5cbfdc035b6d251c735a1c0189765ec9719bf1d9195591d49"}, "9e6772e8-479e-497b-83ce-b5728aafc463": {"doc_hash": "c1973206d69f5b3922217c3d557c27b4309ea60ed844db827a7e6541d9213a59"}, "d2ef52d7-8f67-44d8-be0b-1539687c0ef2": {"doc_hash": "b233e9a2a99bd36a86411cadcb5868c9176595bc7509512b52ed213d96732d4d"}, "2f4609be-679b-454d-9662-bf69d41bf921": {"doc_hash": "bedfe39f6d67f832fcff8bae6a87a11b77e6d0eb0a4516334cf329a7203954d6"}, "e4597f15-99cf-435f-9a04-ed4c844c96a3": {"doc_hash": "55ccfe3b8abb81a4b4db0925e6190725fbff93890096756e33df1ec9da234a84"}, "fedf7284-8bd7-449d-8583-15c4741e9a40": {"doc_hash": "a2935403a1da27ed3c7e7a8fb8acf0a6a69600aa756cadce3ee5763ef92adf6a"}, "15ea1a97-9daf-40f6-b912-518052c443f5": {"doc_hash": "aa82850139ee89523101361ff9d2531bc8554c15ba7f064a89ad53f781d15624"}, "d82a2a6b-826d-4e9e-815f-2e66499598bd": {"doc_hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730"}, "a49a291e-81cf-44f8-ab6a-29ed0d9e2a7e": {"doc_hash": "bfee21c9963d8d00c18e5283582ef863843353903a5bf46e4e0237117b2fd11e"}, "a999894d-7b98-4e77-b962-13f1a9ed7ddc": {"doc_hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65"}, "60ade05e-ea05-4c4a-923e-714df5e6b162": {"doc_hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b"}, "af4db40e-101d-4953-9ef1-9bae0a723f27": {"doc_hash": "3a15fe6a5f65f9bdcbdf76aae79ccf2209c6d7989f53f0b2f8f60d06c9b4527a"}, "40130f56-0714-4ad9-9a60-30154ed89569": {"doc_hash": "6b738309e249e8627f7857e81e7ff2210a1f4aff75a0a6de69e959d1bc74d7ce"}, "25a12674-4683-44b2-90a8-fc3ca4814280": {"doc_hash": "135a65166921f206504ffb58b07f634d2671c853c3ec9ce0423184a53fe2aa39"}, "3cab912f-730d-4daa-82b6-c812d016f809": {"doc_hash": "958eb1b2e66cbbccc8ec4d1c848c0d30f146c9f1455afe45c1c690975290cbb3"}, "723f36c7-767e-4956-986c-7c26a50a3958": {"doc_hash": "457a6ff746c4df5f99e2059afd1c675b2bf84f5302005991b257f9267a19bf0b"}, "501117bb-59b3-4f0e-8c4b-d9fa547ef48e": {"doc_hash": "2133ae3d6b3c70bd954b1a915a2d26d775a6a6220c433a001fb8c4cdfa06dfbd"}, "9a20f4ac-f1fa-441a-8512-1f1fccb73a5d": {"doc_hash": "2e9b32cb777ed6afb752bde8fe2843ccdfca1601bbe1bf38b5559a2f792083b2"}, "906298b4-363f-467d-bb6a-e30c9f7a047d": {"doc_hash": "d7471b6cf69a89a333dd3b11ee88dbdc64ff657e2162525521be64ab0b1f53a6"}, "4fcb5c77-561a-4bf9-a03a-b9960b0a6e8f": {"doc_hash": "2d3d6f29858fa406d747a480b3052d247a65f58824e3a49580761b5521a12c48"}, "8c122559-c8be-416d-a1c5-36181ac1ee65": {"doc_hash": "3d444884a69cc93d386a7d418205b4f998c0e3c4cf01e1348e47104baddcb55e"}, "122baf94-a4e4-4a13-b77c-03ff283b931c": {"doc_hash": "dc92cce79cd9ca4af08bd374f48551019e11a2bb6662d6a095f7de09f97305e6"}, "94880efd-8fbb-469f-9dca-545dd3277949": {"doc_hash": "303610a8b64d4b60607b27741810cba966dd19f5cacf2f50ca5696521f1c37df"}, "08354a60-7e6d-40d4-a8cb-37270b552ef1": {"doc_hash": "b8b1535cd0cb583f70d0a1cddd24f9eda60ab1be50684cba0b12da77c1097c07"}, "d9c3b170-0bbd-4182-be76-ff3b15d7cc91": {"doc_hash": "61ad3404ba01e84c798a07a14a89ee0614c00832bb14f8e86e904ccf222a3e23"}, "af821be8-f438-40d9-b136-b205dbf07270": {"doc_hash": "985334fb42c32f299a736bbaa337d4b42819d4adfe30a268f2c880d210e37eb0"}, "add56e7b-15f4-4e48-b9a3-e63af372e931": {"doc_hash": "d88f794adda618211141cb04741e1f6c76ea48849203e7027d129fa98cef6ce4"}, "25decb75-1742-45d0-b05f-f433d285fc43": {"doc_hash": "a7a93dbb961bae4ef587511aa829d151203da666c5e6cf6c7ddbf08871355ab3"}, "a4e2410e-cb0f-48de-b8b4-0bb4a201f854": {"doc_hash": "be634d91f4609790bbe447f6714e8b842019a61f488cf6de19b83d4f0437bc2d"}, "e7b8cb34-e822-4487-aa4e-f0530099b8b4": {"doc_hash": "9d7cee1f3dba4be1c3ade8b890803de770c0bbaff124c4a3e7165c6fbe2f30d5"}, "61dd2dff-57e9-42e1-b727-cadc5badbd9a": {"doc_hash": "da41435fb06df8252de4604f1ea36f457e52536d1877de487fd7c30efe1deb25"}, "48947c14-e046-4c7e-965b-ae53e9da43ef": {"doc_hash": "26a7c7b637af7dc8fc7487a074f81ecfb1be7e6f84dccf899dd107d77c60e5e6"}, "60f7fb2d-59b7-408d-b53b-2563e901b26a": {"doc_hash": "eec07760eba1d1f2db9fc8efe505cb478e5a12ca1cf1c0140fe7842f1aed41de"}, "ca6985b7-73e4-4d36-abb0-0f7e317f1196": {"doc_hash": "50e44d1766b3383114b8b0cf6fff044f1bd1608be241d2dfeffa1b0175be44bf"}, "a5f58532-b563-42cb-97b8-180547eae720": {"doc_hash": "ad3e63804742bbcd9057cf53d93a20a7b73a1cee013072f5515d29d79ef1537f"}, "5047b9fb-a464-4744-ad60-6223e2b36fc0": {"doc_hash": "7f4b16258bef6e82449b02b8a603c5f0eeb6d4e0e5687ea9a102d267e49bdf98"}, "4a8bfb97-6ec5-43db-b980-1dc2c1ac6544": {"doc_hash": "e48299979d686f325aa401e3586b152609eecd09893a5b6ecba8f8ea5fe30306"}, "1bc7ae7c-fd0b-4120-a5a1-44880b1d2f43": {"doc_hash": "6ded3a6da1bab161764cfd863e91c256704b901cee22f908a3be8a3d9af7937b"}, "2a310b6c-d302-4fda-ba62-15c8d623ed6e": {"doc_hash": "4f2e9d88224aae01959afced768d8a6ffb54760ef8c528efc9fa3ec0e3221f53"}, "d872e017-c394-4b92-85c9-fed2a7d72483": {"doc_hash": "d19d116267b43ef228c755cb1b498bc4368b31ba156a2283457876f8c36b547d"}, "f057991d-4465-4fcc-b9d5-4afbf680e892": {"doc_hash": "5e62ad458742ae6fac9b64bc0afab32e4cd1517b6807db946eca2741e5d1276e"}, "b25d4a08-14cd-4e16-86c0-cb0ad3112787": {"doc_hash": "f97835debdc007ab10976d8b1687088da18192eac0a2bb70dc5b69a3f1ecaf37"}, "0f5d66d8-7b45-4e0d-a988-3ef1de75b0db": {"doc_hash": "105a5b7adab090d411c5d88f6f1bb2c347479de480ed65bd31c629d6ad822e98"}, "3c564ca4-ed6a-4aa7-81e9-4d09d27db662": {"doc_hash": "decaff9be8218d95ed5c12ef5686e8cd21e7c7089d58933146b752321fe80db6"}, "3ae318a9-5e4c-4570-ba7a-016dc0483cc9": {"doc_hash": "e43f715900cf469d9f215ca91dba276900a21b0c3c63446af36de18b5d6b7395"}, "95c64dc9-e4d0-48a0-a35f-8cbd0295e49a": {"doc_hash": "085b8fc587d3699b9a67ddc445164e5492d29f4e619ca6ae3184b683a16ca06f"}, "c0b3bf09-dfcd-49f7-84b0-d4c2d938405c": {"doc_hash": "fe8e2f79aaefc9d977f3bc52eda89be29ba676713aab1521c2efb15b67acf341"}, "9100a7ef-bb38-4af8-b781-2a2da09b770e": {"doc_hash": "98a39be0165e9586b825f1a7547809231416d010d15a13c3722ce16c76b79402"}, "80e1e671-52b4-4881-8395-75c43e1f5873": {"doc_hash": "e79d0c611d75f3c8e5869026e2cd6ef3f3198857c60332f36c03b5c86de3c7ed"}, "b18e20bb-2c35-4550-a71a-a0eeb94fb2f2": {"doc_hash": "234fd47801327bab8aadcaa761d6801e1e1e62edf1ce63e744856b2d037054d9"}, "f76d1e22-b47b-4e06-a0b2-7b42081c8e69": {"doc_hash": "1efe45a7cfb7a8e0ed322d9eda7bab76c72cb5e48579b824bc08013cacc27eb5"}, "01a714d9-e060-4d29-908d-1e12c2ef9f19": {"doc_hash": "e56fa4c6a85b2c5327565fffcaad2a3a77a92f986f7cd75cf005eefc76c12401"}, "2230f129-480d-4a25-8cbd-5f93a11c60c1": {"doc_hash": "8f3fb664e5b1ed43e9c21cbac5e52c59df69cbee931c7f2ec80627ced081316e"}, "2878648d-faea-4d33-984f-71bb6b8d2481": {"doc_hash": "deefed50d4601eaeb058f9d2a9de839354af4f2c0b8a740fb76b5c16ce0c6905"}, "7f7bb37f-c1b1-49d1-addc-c75f8024b6f3": {"doc_hash": "10d05d905dec910fd85838a6fe58be899b0806354b1aa4a6fe2cd7caf2ccbed1"}, "d8e4cac6-b7dc-4a85-aef4-f4f2dcbc7251": {"doc_hash": "b0fc17c0cff17bbda44d971ff18c2ddad29814e6fdf2eb349c080aa400746722"}, "9d19ab6d-0912-46f6-8a4d-310f45cc45cb": {"doc_hash": "bb1a8367883e15288b50c27077470b336f0beacfcdac38cf2d9dd5278ad01621"}, "52dec9ab-f3e4-4763-9cd8-d383f66aedec": {"doc_hash": "1dfb7bb9121a3c95768f3315cd18305425f64b5b54c9c3e675e6fc73b3329ded"}, "1474d637-b80e-4c6c-8a72-79247cdb139c": {"doc_hash": "fade07b9efebb9941806db64a252cec419c8345da7f4bd41746011d99eb280b5"}, "c2b741a4-1417-44b2-a4cd-1e0b040c3b87": {"doc_hash": "c619e9e93c23be44eb5463d75af5dcc7c92e3354b338d9243313917e30c5b97e"}, "ba841022-58ec-44ca-a43b-39961be87b33": {"doc_hash": "92b8e0557ebc4c00b479c5665f9134c232267c6efbc17fa3b1f3590eb84d1145"}, "84ef711b-781e-425b-9aa4-c5587f9b95da": {"doc_hash": "5775ef6e775d3fbe75b56710d94472f03938b66067b068872f5679e16ccdb6b8"}, "7e6eac33-6916-41b4-8be7-1a725f838684": {"doc_hash": "85168363ac67b2cfaa9800adcadd48c56ba0661d984adbdfa72dffd470b46eb7"}, "f3ee54c0-04da-45f1-9329-437b41468049": {"doc_hash": "24021605fe1e4f782a5fa15fd0dc972cfab7add7387d0dd271f0bff4201e3b66"}, "4b1fd6b4-7d4e-41fa-ad42-a9fbe68f7bf2": {"doc_hash": "48a9aba303061528599be8ec3eb2f1057ea4f12745a99b48fa8747dda84ff496"}, "30dcdd05-eeb3-4645-a18c-d36473030b1c": {"doc_hash": "1a342d0fa48ec543fb223686bd1aca6f7f405ce6a5036cdb7b7336f4dbb30a51"}, "1cea5fff-9f8f-46de-b7d9-43f9d831eefd": {"doc_hash": "9604473f7066d3d80b2b0da4c1c8236f34bbc31c624cbb5be983d9730630f663"}, "b1951614-9d46-45d6-9977-1315fb47372e": {"doc_hash": "c203c4b3ea860298aaa11b06f7d91a3c3ba11d8ab125bfa6dca80b7aaefb2ac2"}, "d9f7c5fa-4284-407e-b043-f602c5f22629": {"doc_hash": "7ed48a38337f6f81524bc0ed3c1bef31b55e7027a1c6be23cb52a58e608c811d"}, "e0d3428d-f1d4-4954-9770-b1acb9f38528": {"doc_hash": "5a15b0f8801a8a76f60d123331a928697a9fb3f42c7e787a863f0adff338d6c8"}, "92f96cc5-fb08-431c-8a82-2243eba56518": {"doc_hash": "2a2af66ed876eca628ce2a98258eb7439f5d6d0dd35c0e55f2febe3fefdeb0c7"}, "e3f87ce7-f81a-4860-9272-74316bb0e0fb": {"doc_hash": "8bb90d4f8644c17a7427229bb096166c92cc2ccb39abf1001ad7b64d9b261aad"}, "51a6cfaf-27f9-47e8-9134-bb649e724485": {"doc_hash": "8a5b346cb3ad591cfdeb7e92d5b6c5e29ecdd50f7a9a09eddc52155a8ce04447"}, "a703a98c-0cf4-4245-ba39-b360db3499c1": {"doc_hash": "ebb09573d2ed42944b82457d64ddf524440eec14f95673103b9a0c9b631f3310"}, "8469d484-e5c1-4c43-a63c-540e9f6ce250": {"doc_hash": "346466234d0898e02dc1587994f46ea5c7704da91e6ac2843741997c91ffeb17"}, "ef36562f-401d-4d3f-8819-01bd317f38ac": {"doc_hash": "399933a437acdb34290fd1f4951030558a04395b3d64cfcc7dcb2cbd701ba048", "ref_doc_id": "b5e292d2-a134-4a9e-86ad-69dc8cda1d3d"}, "c1c41615-7dba-48c5-b7bb-e0278326c20d": {"doc_hash": "f2de9164b8ae056b2a441eeb842ada9fc450cce62f5d42b00c712764c140cd6d", "ref_doc_id": "a33872ec-8199-4568-8cca-87ca03333326"}, "879aa659-c616-4665-848f-3b89ae161684": {"doc_hash": "8c84cf0281b160b4e5972ee172fd7c5a814d77d5a91a7c60494623a9af8248d4", "ref_doc_id": "cf6106e0-f8b6-4fd9-8110-83752bade113"}, "a845ed96-cdae-40a0-922a-1b83f8d38709": {"doc_hash": "c58ee3ec0c6194bacd223151536223c51dba40fccf4a6ae54b3c77c0c30ba3af", "ref_doc_id": "38b42cf6-0149-4170-a208-4b53d527d9d2"}, "df31eaea-ed63-4a41-b411-842b54ea0ec9": {"doc_hash": "70447d0da781c7443549ea2d39ca25fd93dbd85d972ad5b96f4215f8a6e82605", "ref_doc_id": "351b7993-f448-4ecd-8635-0288100f1c35"}, "2a9e1df7-06b5-4099-b55c-b2b8917ab711": {"doc_hash": "0942d2e9e87b391695e127ada359b11b788b4b765d0cde36fc10fe4d16d30a3c", "ref_doc_id": "139b9d1d-b8a9-4a68-8c54-2553083d7e07"}, "babaae1a-c048-45cf-90fe-99d3a6940348": {"doc_hash": "98565490cc1cd992a281179289a0bc7dd814f7ab8edae12dee0e191fd25be527", "ref_doc_id": "139b9d1d-b8a9-4a68-8c54-2553083d7e07"}, "b6f38bfa-f98f-4f3e-80a7-ef7298e0a554": {"doc_hash": "4c90da16c03bd49b3de1371b06a4118029697f47145f7464e68858748f799989", "ref_doc_id": "aabfb308-4360-4680-b521-e061f877d376"}, "c675754f-af3e-49e2-85b1-a5f5e8c7577f": {"doc_hash": "10d598ce69bac914c74684ad9ea4011a1c6d516a02805623d9fd8c8bd6eee230", "ref_doc_id": "6560b13a-ac24-44e2-bafc-5ee91c0f8884"}, "cecd8ee7-58b9-4a0e-98ab-0ecdb5b81a83": {"doc_hash": "83e666a1948d00b8d732b8007a46ca1fb5b378c1e869ba3e8e15e78135f98ea4", "ref_doc_id": "cb57b4af-92f4-41d6-8bac-6e6967e6f8bc"}, "847a89ad-85df-4391-a2ec-f9b6a4ec35f5": {"doc_hash": "ff502a77b03ffe3f569d9e14a4e3c90b85a5e0f0d30d44eb85015cdb33f264f4", "ref_doc_id": "c97a0073-d1d9-4e6b-88e6-fceeb3e7e31e"}, "2df32ffc-1586-4d61-a892-b62ba5932c5f": {"doc_hash": "5fa6cff2af1a461cc897ac3d738c04ade10189e33f71292facd5c1418f74026e", "ref_doc_id": "baeb0f1a-cc92-46a2-886f-9b3186cd23b9"}, "6ac77a62-188b-4ae4-8a92-951eca0ca931": {"doc_hash": "f69f8c146a6e63ada4a8c39e569d86b7f54f54d0ed62180a0c7627f1c0c3accd", "ref_doc_id": "baeb0f1a-cc92-46a2-886f-9b3186cd23b9"}, "fa795666-e029-48fd-8758-1d04d7f03014": {"doc_hash": "57f70b6c3c0bd82d705d5c6c92fd2ea210ad7b6bc5d8355e500a63cdef81c770", "ref_doc_id": "baeb0f1a-cc92-46a2-886f-9b3186cd23b9"}, "539dc552-ce4f-4f08-92e2-2ef8b6a339e4": {"doc_hash": "9acd08f1b006fa8234dd64fca0161a127f10599ff0dd989e095e65bd78bf3bab", "ref_doc_id": "0c2b9635-b7c3-4992-bcb0-b2b41675bcf4"}, "c0948931-3583-440b-ae33-cd6eded834ec": {"doc_hash": "39e7524fd313d2e3c1525fdd5dfd8b22149508517f44a2080be4cc1b9834ed7f", "ref_doc_id": "8751e01c-546f-45c9-a18f-9072f84ece02"}, "3f7cf6b2-255f-47cc-af52-7266f17db243": {"doc_hash": "f889ab631ee4719f6df58abe87853379fc864ba438846ab5787ae204ad698601", "ref_doc_id": "96495664-a204-47b7-bbea-f03ce0079376"}, "6361f438-1636-431c-a9e3-6451497ade6f": {"doc_hash": "c45497fc34c38e1be42601222bea25a2dc9e771c852789d66ba523a0b398cd9c", "ref_doc_id": "0b29e417-ccc3-4b22-8dfd-39da9db03f96"}, "4d6aeb60-b634-4d07-a04a-172a35461673": {"doc_hash": "927b150c7526174f9ea74dac3166d2f84bd2aee007d0fb74a991b247a6af3997", "ref_doc_id": "731197b0-1ce1-4ff6-8452-a3c6ba3e9bc8"}, "2c42ca9d-ce06-4c1e-9a5d-6747b1734d40": {"doc_hash": "121404acebd9de50ff497de2d37410c5a4075612906eee167cdf34475d850b59", "ref_doc_id": "a272fd28-cfe0-494c-8745-5c6fdbf79897"}, "eff87b85-cdc5-4e04-8ec2-0fc9418e8053": {"doc_hash": "9db447504d6bfdeb39e6c89db83d88d2e119614db905a97b620777dc12cb5490", "ref_doc_id": "40ed341b-1507-4cae-9d07-e6e9baf669b7"}, "53bd7ae5-5941-477a-ba1a-db0deb515cd9": {"doc_hash": "f44b651fa4a6c7f2002ec14f9d137f355fcb7c004e21c593cdc683b2733998a4", "ref_doc_id": "40ed341b-1507-4cae-9d07-e6e9baf669b7"}, "d51298aa-aa0d-47ec-a224-ef5bdf6d4949": {"doc_hash": "2e31c9ba342329553663ec06344691f30514da220e081e8561393bdd387a4664", "ref_doc_id": "301cd385-0111-44ca-9961-a276a29c7e02"}, "44c55cae-f536-4a38-9177-afe958025933": {"doc_hash": "28c6e3e868ff659c280567ef986c77db1fbced43e6eae3966347b3062b782835", "ref_doc_id": "82d4e3c2-b6cc-418b-941c-4e5692901e27"}, "81263731-90e8-47fd-9da3-9ef79436b245": {"doc_hash": "746b34767345e0899a53ecd483dbe8cba37150d0f5f10027551d3c175736b695", "ref_doc_id": "9069c6f4-c561-45c6-8347-d92d766bf645"}, "b23d7c4c-e038-4a00-abeb-2de0501ff9d2": {"doc_hash": "33d9a299c4c1301ce196f3448deba39a1212b9d4151145ba67beff2a668c320d", "ref_doc_id": "cb254119-f0ab-428e-a4d9-804706c1269c"}, "6ef4627c-fc8d-4e15-8ffd-278b12d8f36d": {"doc_hash": "1034d6da2dc10708c224a86738d514a71c6741b30f167c11f256181436e2da83", "ref_doc_id": "d847c661-f669-4bae-b731-b9cdfe9cfdbf"}, "4cd11473-6533-4e0d-aff6-bc811e79378f": {"doc_hash": "32c38b83841a836b29fd61c3919dd70104406f9f348653b6a28af15ade772684", "ref_doc_id": "e5297a92-0aab-4af3-baa7-56b2bfe617bf"}, "655f1b90-c6e0-4099-b28a-aeafa6225868": {"doc_hash": "84b0698214b8d146d7937c066f5839cf0e8b796d6314ff9dca75a5c0bb4b0909", "ref_doc_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00"}, "81cdda33-ed53-4777-b414-95ef6943af02": {"doc_hash": "1d9280f9d454c799351f48ac1bf2f12600b51574e4bc6967a6b84cd6805d5cba", "ref_doc_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00"}, "150f858c-ebb7-4b34-875f-26f5a4af09a2": {"doc_hash": "a88f74514db5211eb77950429a6b5b7b9cc8b0fb910a12209579851cff999d65", "ref_doc_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00"}, "18473db6-4bcb-4daf-a165-600f8551e081": {"doc_hash": "f01de903a86ef032ed4c2381b3b476839729b9f2bfdcdc4755094ceecb4e2745", "ref_doc_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00"}, "70c8a4bc-f01e-4542-8f6a-dfd0ca11a234": {"doc_hash": "68be39ebcf4c355976104ebd43e9825c0a178973f5336bdd1105a5c954037c46", "ref_doc_id": "df3de19e-b8da-4d9d-998b-4f2e16fc0998"}, "77669bd8-7407-4c75-8c1d-de4ed1cfb681": {"doc_hash": "7b77cbea74cea184c34638284708a5686117c07bf3f4dab4ca3e151c2975422f", "ref_doc_id": "e201afc0-6bc7-446c-b9e6-e0e132d9b77c"}, "8edd86e4-b1ed-4738-8bd9-4e03b664490d": {"doc_hash": "adc21a872cbf9b14667a750dd28f93036b5129926ea8577ff6a9afedbaa5eaa8", "ref_doc_id": "f3fb87eb-71b8-4f8d-b9b1-2d37466816bd"}, "8999210f-60e5-4433-8832-0e20f0ce5eae": {"doc_hash": "3ff1d572a2f46f49ee665331948954c22042388c7cee1ac48971de7d316549b2", "ref_doc_id": "9e85ca8c-b81e-4143-a7e2-5e66679a2913"}, "c0d71673-7bf9-4f7e-9655-3980e80ae62a": {"doc_hash": "eaa96a89ad8a8b18f1f25761cd3efdb0966de11cb2d641086614947f33d9adb4", "ref_doc_id": "367ec997-53eb-4fd9-85e0-5edfc8935240"}, "894cc91f-110d-4104-a004-129253335b25": {"doc_hash": "4cb112be1fdd61e19d3565d5f987b98a34068676eaa83a101b580292abacaeff", "ref_doc_id": "367ec997-53eb-4fd9-85e0-5edfc8935240"}, "f092cc31-f987-4770-8a56-cf8f8d8c1584": {"doc_hash": "4017cde8bda419f4ce598836da87bf251e2188e5d27f61740fa305de73d194e2", "ref_doc_id": "73f46aef-770e-4d59-818c-7d15d8e6b2b9"}, "02e60a50-9761-4ac2-a284-8a7d11f05729": {"doc_hash": "bbc21462cbdbbecc05b39c36f1e8267acfbdb9044906e0b1c250ce46a3c4b7e7", "ref_doc_id": "a9ad1924-220e-40f2-bda0-6d0e6eae7aa0"}, "08130c11-d1bb-4f01-a4f4-45675abe7b9d": {"doc_hash": "d5ff62fc488b1936b410aa768217f28590eb7572f3bcfe6681ce9d54b4ca6721", "ref_doc_id": "e9ae78a1-ffdf-4399-843d-6ce5b5247d7c"}, "23d2654f-051e-479c-a882-9f2c8ccb80f8": {"doc_hash": "c84aafa88d56fe101e2f018652bbbb2f191ecb3b2658ffd1b8a3aaa94cf17366", "ref_doc_id": "3990527f-3cec-47ae-82a2-741edf3041bb"}, "089cb881-ea18-4d61-9e5f-f5d43c0de8f0": {"doc_hash": "18097e6b28c402b88aa61d5644e3fdbe37738d6c989eeb743e0048e02eac757d", "ref_doc_id": "baafa8fc-3198-4410-8c79-7c0c121b2935"}, "5fbf122a-b3c9-48a1-8a27-4a7e977a1a03": {"doc_hash": "0fadd85f64e582cce39c4cd97665280a028fe64d124ab7c9c924ffb8779d28f9", "ref_doc_id": "7212107f-33c2-4ab2-9949-7116da9c38a0"}, "3ca150dc-dd0f-4f9f-8cba-1de45bf3294b": {"doc_hash": "122dec58184204f28744c6095bd075e9bd9aee397b7483f1fd38d069e810551a", "ref_doc_id": "59f980eb-bd73-4503-8be4-33e3ebd5e985"}, "3691e635-ea60-4d2c-9ed7-35137dd55fed": {"doc_hash": "a47f1c883945b485531208727cf34d45e3a8b6be7a652c222289ec5e6c580b5f", "ref_doc_id": "fb7d2daa-0898-48dd-bfb3-a034cf33a7b9"}, "bbb7aa70-540a-4167-bc7e-685b18e878ab": {"doc_hash": "052937263bd87de40a9b2c917f3d0cda86d140e9031bf64737192fca92ec0c56", "ref_doc_id": "e765345f-1a30-4e05-8cb5-af9b246e3f8b"}, "cacedebd-a22f-4f68-a04d-33daba44a92d": {"doc_hash": "7c4153be7af765b510929bc14cdeb86c83fddc3b91d70a9681583192f4dc2639", "ref_doc_id": "db4fc2c8-86d1-4f48-b0f1-02a702b3aa3c"}, "fb098947-0625-4970-8a89-eccdc3f10e27": {"doc_hash": "e6fe4c2cfc6d776132e7c35a4c41a1742dc2314d9d185f20232b21d9b615ba8b", "ref_doc_id": "b43f6d2c-aa71-4cc2-bef0-0c9f7a0b9cdf"}, "728b8825-0bf0-4a95-a1f9-0cbccee398db": {"doc_hash": "021de772c329b96182b288ea6d1d8f3e3ede776aa10b8c0150bdc30f00703e14", "ref_doc_id": "122052c7-8a0c-4c26-ae97-7309864955b9"}, "9a216bc2-0426-4aa7-bcea-5856638efc00": {"doc_hash": "56fc5e1b21eb80c4c6317ee42c1048039308ec5cd9b828c0383937fcbb7c13cc", "ref_doc_id": "7de105af-319a-4cee-96d2-dfa5b88cf140"}, "e289446f-61ad-44f1-8b85-f3855bb569a9": {"doc_hash": "7007d34b1454f1a73c461468fa4ebaf63ed3085b9199860ca02423c387b5ae93", "ref_doc_id": "8b7f361e-3568-45ec-b415-6135472503f8"}, "5feffe8c-e4f6-433d-bc4b-128f9da33b58": {"doc_hash": "cf31c50799ec01b9f2ae8cccd24ed54bd5cdc594e7d9690ab8e57fadca70e31b", "ref_doc_id": "8b7f361e-3568-45ec-b415-6135472503f8"}, "670b3b83-d006-468d-99c5-d0fb1f101eda": {"doc_hash": "179a5ed4e629f48c603d03097bf54bc5a976a679f8de77c74caa24264f31066f", "ref_doc_id": "8b7f361e-3568-45ec-b415-6135472503f8"}, "adf58f5c-3667-4a70-a67d-eafd6ebc7c38": {"doc_hash": "653be441eeb4b2133cd942576fc967bcb0aa0512cd3910974e19e107299a11ba", "ref_doc_id": "8b7f361e-3568-45ec-b415-6135472503f8"}, "2e8c09c3-c8f2-4b79-b176-943d00c715ec": {"doc_hash": "2374b1498ab30d917cf01e2f0ca44c9762f4693a42e421579dbc0f6b69ee2342", "ref_doc_id": "4f692231-45d4-4f4b-a63b-b06028d512e9"}, "58e0b3b0-7348-4cef-8884-86ed8ff27651": {"doc_hash": "14379fc514bba60b10a03e250bb9a220d418e1b860e1da757c6834dbcbcc1433", "ref_doc_id": "b3e56570-1c06-4ee2-8547-00212530db25"}, "45f9c967-3361-4118-9a2d-9141eaacc236": {"doc_hash": "a223014f6d919db5dc2ba04feb82c9af814ebb2bc7d72272ab601e061edbb711", "ref_doc_id": "d9f35f6d-bcd8-4023-b9d2-e0398bf942f8"}, "0784aad8-4ec9-4654-b590-73023b0eadf6": {"doc_hash": "d4a8a57466a11b1aeabeaa251d7ce1bee8ca3ef560d827f61d5064ff5381a829", "ref_doc_id": "2cb2a6f8-ba4d-4f32-bc2b-43fcb62583cd"}, "060a5051-5217-4504-b8a3-82a61fe369d6": {"doc_hash": "8a6108795437cc062b496f50d63707225a6726fa552f1a2dca17747076a427e2", "ref_doc_id": "7b34569e-b2e0-49ba-b688-14f3ab9c1a9f"}, "eb13c929-acb8-4349-beca-024cea5a60fb": {"doc_hash": "28460262d5cefba3ecc3a623dbcef7bb8b48dcd2448d73bd42b434dc55640ed5", "ref_doc_id": "89aa0982-d92b-435c-b00b-131dde63e9b6"}, "0e652061-42f8-49b0-92ca-ffa4cd3bf00c": {"doc_hash": "81e6f6f2cee9762a94d381d91b3c5c0b885358b5d27b146e80d9dd71089b0290", "ref_doc_id": "892183e9-fea9-4b7d-ad31-6ea876f5a171"}, "b5d8630e-70a9-489f-99f8-27a214bfebd4": {"doc_hash": "b232f77c55f5484ae4a8fe37a1673f8ec72ca54836d7cd2b4cb0930924378924", "ref_doc_id": "5659ed1c-d7a7-42bf-b884-e9e048361786"}, "d07b641f-711d-48e3-9d1a-69b93eca2b4a": {"doc_hash": "963a84e941e48487f39bd1fa63b8a680ac17c9c5fb74ec949bce187b4c6fbe3d", "ref_doc_id": "09380548-18aa-4184-ab0c-2454dd0927dd"}, "6ae764a0-a39b-41e6-b96f-f0deb4dec986": {"doc_hash": "cdedc752900c788df338141f5188e8f15c34096ceda957ccc6307d3dfe5d9e07", "ref_doc_id": "36a12fe1-c86e-4bc5-996a-f7d09ee9fee9"}, "14cfd544-b38e-4c64-89ad-2edebe155068": {"doc_hash": "ebe0d1ac7f9f4a24703d9639937e22b832c4388720973dc8e3217a84457612f3", "ref_doc_id": "affb6e22-ed28-418a-9f50-05c93afd0763"}, "02fb04fb-69dd-4584-8f9e-584843f8f772": {"doc_hash": "45cbcb45fc121ad6096d81b850c3c4f6cd7bdefcc8e3b2a9df604fabf2adc08e", "ref_doc_id": "30ce481d-8967-4d3f-832a-c6ade99b46e0"}, "ca1bfd86-7c1c-436b-9d01-64d9c9b66a86": {"doc_hash": "88833ced720e33cfa5797dc1ec23a4411035c343b19e36016d2720a55940705e", "ref_doc_id": "dc9c5841-73b5-4f72-b615-be4b1ba51faa"}, "7be92c7b-ea10-4014-9e09-9530b76b92e4": {"doc_hash": "e4a420400b8abf245687b043dfddfd798b59a6ce25863dc77aa9dcc87959bdb6", "ref_doc_id": "d13ae74a-8073-453b-8099-5558a4293888"}, "55f77059-cd17-4b44-ac14-81f5483eb85c": {"doc_hash": "07afb0b84566f66e530480634081ec12c7805bcae1a72d1b6b02b7cd0d639342", "ref_doc_id": "d6bbd1c2-97f4-4eff-bb6b-014db6b718d3"}, "88c13260-64a0-45b7-86e2-8b91f958b4bb": {"doc_hash": "532b577b8a25356abd2018d5fd6f038cf31c55f1944fb46df7192e4a1b7e5744", "ref_doc_id": "61dcf961-7eca-4fef-b222-a6c86e9310b8"}, "87be98ee-aa47-4d7f-ade8-f2257ccf98f9": {"doc_hash": "e2c45df93fad5da03be62a3104855990aaec5584301d0ce8f0c327ca417741a4", "ref_doc_id": "0000001c-2244-46db-8588-916c3be6d671"}, "7c9df2ee-beb6-40b7-ac9b-1e76f7023ec2": {"doc_hash": "53b368b95fe50de3f2801831ef0564e4c93ec5890f3532ff0bf950c2aa775e13", "ref_doc_id": "e18191f7-ad15-4f14-8c45-a7f6879d485e"}, "dbd1ff79-0952-4c42-828a-e475a0632f01": {"doc_hash": "3cd3f58b3c793576ab74cc2ed0e5b606e730e5440cbcbd05de038d07fb0cb217", "ref_doc_id": "d89ad8cd-a5b3-4c66-8626-63e7e25878f6"}, "bd08f12e-b42a-485c-b313-5e98f5139222": {"doc_hash": "ef18bbe3bb2b49dcda01250b40dd862d3bc8e8eb9d7cf55cb22dae5e48f71e92", "ref_doc_id": "a3cb979b-edef-4ad3-9457-0246e60c8f8e"}, "67fd19c3-6488-4156-ad13-7c82780cb3bf": {"doc_hash": "96b2ed3329d439e576ea4975788ce04dd20a0092d2af9112d51071d58da3bcb9", "ref_doc_id": "c03a63ed-4b7b-4f31-99b7-6b0475267417"}, "d05eef7f-8a9e-4439-b57b-1ee13b19c2f2": {"doc_hash": "0a79c38ede49e3c36cfe4ecdfdc1b53871c80557ef475fac06d582b8bb6d4fd6", "ref_doc_id": "27f9f4d0-045c-4dec-bc9d-5f2b33bc10c1"}, "4b27b84e-2eea-4b04-b54f-ac5e36da422a": {"doc_hash": "f53fd8bc6d4aaefca92a7fc5f9297a9162e7ce7c17f7cacbb48f5e54481226b1", "ref_doc_id": "a264463d-7e21-4ae7-8366-e236e310daa8"}, "ed5d40ed-0b1c-476f-b7d6-08e5ab8e71b7": {"doc_hash": "a05bdb1189e89baa7ad12beb76f6baacd18803e3a373c5365359b58ee5869b32", "ref_doc_id": "38f0bd10-1ce2-4015-bb6a-6f6ff085216d"}, "c4e0ddf3-f606-4dde-afc0-c74e995aa2c4": {"doc_hash": "4c3600a07a145a27745001d62daf038d0605b7160d66e70c265067908e891648", "ref_doc_id": "462d6170-8c76-444e-b308-e4900cdf71fe"}, "6e3162c8-e330-4c45-83ad-29d986a4d643": {"doc_hash": "d97719d4c49663b742cbf7a5565f7424a5493923dfd63bccb34505a3d8ef8c28", "ref_doc_id": "dc1181aa-9fc5-4538-986d-617ea6ee6245"}, "60e86002-b3e8-4d07-a76d-de68a9610980": {"doc_hash": "800ec983609fc42abce7b72386cc3132c3678752b4939637699e0c9a2cddc055", "ref_doc_id": "716ce4a5-53ce-4e42-a5de-c0c5dfd9f60e"}, "947af18e-4ae9-4e47-9f6f-d5603aea7fbb": {"doc_hash": "bd396fd11063e69f8c6c6f9075ab5cd8a1ba9f8687a44d00654d05c3bd16bb4b", "ref_doc_id": "e83a0796-4953-4f1e-bf3d-3333d778ef97"}, "ab44ceb6-b2c5-447a-9a53-312e4b59fcfd": {"doc_hash": "65115fefcceb89d53fe544aba834b8a52d55ed42c68a9139d1f82de2af63b79a", "ref_doc_id": "63c60423-ec78-4506-bce9-0bd23fa1c367"}, "d1bd299c-5eb4-4f0b-bd7d-c312becf0764": {"doc_hash": "3b0d40935f1935e1781d74364cb1519f7d174b9be304b8491ad7e822ade29e7c", "ref_doc_id": "49dccfd9-335c-480d-870c-8943fae4b87d"}, "894a342f-1b66-4722-8b0b-30922c727dc6": {"doc_hash": "570b34f36c7090151d2a97247365adb0b596010fa21b5808e674a6fc222f576a", "ref_doc_id": "010a51e2-c61f-42e3-bad5-64c74c7c09aa"}, "4ecba323-fa85-43fb-aae2-15a011e9629f": {"doc_hash": "af10791904e47db93a65675fe0ee8b4618fe23dd1da296852a198599d3aaf498", "ref_doc_id": "78bf74e2-f33f-4af2-a01c-f9caa88df9fd"}, "a1c1a2a3-9ce7-4897-a5ca-cbb7e0e9f35e": {"doc_hash": "3d6950eb5c398ec8405ee1abda3a11480d3e9e490ce4fae6033725fa46dda95f", "ref_doc_id": "87ee2a70-6212-4e20-ba00-4de444782e49"}, "6139311c-5093-4c2b-9987-653262cedf51": {"doc_hash": "2a258875fa6defcea4e939a0e9cb85655d111c2cb0ce22900f153210acc2f1d9", "ref_doc_id": "4871d211-2c6b-42a2-b9ce-0d377c287e1e"}, "4c3e10b1-7452-42ef-813a-9be497e7f577": {"doc_hash": "6fc30408d88c7b3deeec4238827039eb1dbff6f45cf2553fc19a49d851f26667", "ref_doc_id": "37b7993b-8c2e-40f9-963e-9b6f99317064"}, "662b426c-ac08-4b38-bf62-d893ae49a335": {"doc_hash": "6369b462dc982790fc41c611998cb873aee1e3ef71c5eab116408cdba4d618a8", "ref_doc_id": "934d0538-759a-44c8-80f7-eadfde381392"}, "750e9770-2ebb-497e-8e74-4870bab1bb88": {"doc_hash": "04da761fd65f849f77776750eaab9c9a86873a2929d2271ee2758262c6d04b67", "ref_doc_id": "7d114870-ea8a-4678-b740-30cc2f4dd978"}, "44343dfa-b0ef-496e-97c4-68cb93913d54": {"doc_hash": "4fc601b91628542eff6a385032c48e34e06e04128af4e991e0c2af476d3df8ff", "ref_doc_id": "4acd7e19-c74d-4352-a754-6c0abbe6974d"}, "70398f22-bcf9-4975-a14c-19f0e8731b8d": {"doc_hash": "b9c6605a55b40c818e58c6fded2ac45bfb65a466f97ad89878978e860880d368", "ref_doc_id": "789ceda0-8b18-4d86-8e7b-dc5a1e73ce97"}, "788994aa-9da5-4e99-814a-42a7249566cf": {"doc_hash": "ef2764d5d326fbdb277caf5b19848f7173f28e78f81e9214f8b4d70d90c7e0e3", "ref_doc_id": "a7b4fc38-010b-4fad-be2e-cb1516718134"}, "92ac68fb-6eed-4e8a-a2f9-6e04d7a0f589": {"doc_hash": "a3e73cdfaf33533cb7824a9004f108e64a56696319040227cc99438518d330cb", "ref_doc_id": "cd80c7a4-e1a6-49b9-923c-7b9d7312f54e"}, "e1699c17-785f-4806-8489-40e367456d8f": {"doc_hash": "49678f5782fcef102aab5de3f15c62ce89b3ce55e2a79910a71402f297384852", "ref_doc_id": "afb3ead4-8175-4f44-985f-641cbabbf28c"}, "bea38b7b-5255-404b-bd3c-e5d140c72a80": {"doc_hash": "ad2b77e945fd1bc7ab21aa66bbb76f29d0689cd4a7856028cef32869622cb247", "ref_doc_id": "157ba0a8-bdfd-4208-9cbe-69a021d510f7"}, "fc64a91c-d893-49fa-9a3d-dd3e3a716670": {"doc_hash": "1544db7e86c3aee02267a20851adb18c7e15d0628670c085e2dc7219f1f51d02", "ref_doc_id": "670c0479-fd8f-4b3e-adb4-d20c8cde8510"}, "c6cd7bb9-e4ce-45e7-baac-23d3c3467108": {"doc_hash": "cfeb6fb56fcd7f2f65b4ebf5d6bab692f1dc850c90e16b4b88e5deda0a7bc3ee", "ref_doc_id": "dee94cef-3594-48b7-ac4d-209f1fa4dc1f"}, "37393fa8-02db-4f56-9ffc-4b5eddc8e996": {"doc_hash": "3d98272d4a0e72774bae1f982347b3f90d3e1cb8335d746d5423411073bf7d6e", "ref_doc_id": "2d41b507-718b-40fa-8d38-9aa41c328430"}, "ecdf3d9f-62f7-4686-bec7-19df5e419bd4": {"doc_hash": "8b3cdc603494f9146b1d9e642b6cf43db278e1508a6156c8e3550f89e798badf", "ref_doc_id": "13162bcf-3ee3-4920-b0ed-18649219bbf4"}, "9f9f19cf-4aa5-4a58-a08c-81283e904931": {"doc_hash": "a6ba5c436af824d42b4f7070c587a7ca14326fecd8a2091d7f8bfd63c1d2bba1", "ref_doc_id": "2ccaae91-a34f-4989-92dd-625d712654f6"}, "ac8d788b-8b84-410a-bfda-1ad5777a38ce": {"doc_hash": "b875bfadf358799d0d991a62eb8b9a18aa15901001258216ab0831d342f7b5a9", "ref_doc_id": "e2534ab4-7337-4321-bbb2-bff3c7e5327f"}, "952220d7-bd0f-4664-a658-1c2acb371e2a": {"doc_hash": "d533e844d9f9eb7402ea20205b2d1c532f3d2f73c890f0b9c710eee26f4cdf97", "ref_doc_id": "e2534ab4-7337-4321-bbb2-bff3c7e5327f"}, "0786c093-8f72-4fa3-be4c-67c39d1cf21c": {"doc_hash": "af2250de6894e79d4275b344b382281038acc4959f287d4532fe1ecc9a03c76f", "ref_doc_id": "e2534ab4-7337-4321-bbb2-bff3c7e5327f"}, "f70fd2b5-fcba-4b8b-a160-f6e8f38f73bd": {"doc_hash": "f650cb2d0222d9d3201addf95f4e6a138be90b3956d62a972c3a39d85b3632ac", "ref_doc_id": "4ec41745-8d3c-4639-8ace-b7b8e8dcef4e"}, "11ce88d1-df24-479b-b1b4-deeb7da2c84f": {"doc_hash": "8e124fb96d4a424be6180cdfeecc21cc71a2647d5aa04f968f76ed9c97cde8a7", "ref_doc_id": "d4c8753b-98f9-4463-9544-9c6650380038"}, "598a7c8c-c7d4-43c3-809f-f66819cc0fa8": {"doc_hash": "855a58ddac89a2ae07309e5c9d44f0ea9bb8f5da5626ffd0ecfbf6f42eef3ea8", "ref_doc_id": "0dc9ecce-6402-4382-9260-3330b72c19f3"}, "bf831700-8db1-4bd1-876f-67ddd104a245": {"doc_hash": "0c3ac0dba8d840d4b401d6c1ae423de528acb30d708084188ac2c32af72402c0", "ref_doc_id": "e368e7f8-2a49-4fb6-81e8-7635954a667f"}, "3c3bbe37-895d-41d7-8422-46840ffa2840": {"doc_hash": "f7ff3aecccbd75d7c3a4345397b60c7be05ba802d4bc89f7a352a71542b06d03", "ref_doc_id": "89ee3ec4-6c4e-43b7-8c13-6d24b39fa6a4"}, "6301be1f-5c4e-4d5b-9247-6c63361cb5f4": {"doc_hash": "c32106a59fca29d39bb928829989bae62131ba6a3d0239ec016abee81928efe9", "ref_doc_id": "236e95b0-c5ef-4e84-a2a9-90c9aef518d2"}, "958b8bb8-fac7-41fa-bbd7-d6535173c3b3": {"doc_hash": "4315401e5d9dd901ef7c3070dfa6eb78a158c85779bcc0da48371d7ede4b7492", "ref_doc_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b"}, "2a40ff73-4abf-410c-b2b7-be5835d0c0f8": {"doc_hash": "5badc6606a0a69aa84a69affdbf2b397e0b4a7cb4b3c1fea806085f45f2c8e07", "ref_doc_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b"}, "2ad2bfe7-2c5e-4b3b-b398-b0e7e0d9533e": {"doc_hash": "730a76b2febd73f551c00064def7ef4ada2857b86e2ca74a346e3a0ede9759e5", "ref_doc_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b"}, "6816778e-e7ae-4578-b77b-e7ad59928a26": {"doc_hash": "b8de0a7b37a43336587d272af7174740f4aa96ed59c4c4476f2794a82f60153c", "ref_doc_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b"}, "acce73f6-d7c0-40f8-95b7-07873412ebc7": {"doc_hash": "60215f41e0750c3280cfa834b731a3b18b9cfd4f5bb5b75bb16a3c14e587d861", "ref_doc_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b"}, "4745de73-139f-42ec-86eb-0c2eee49d58d": {"doc_hash": "bf1602d05ba25c1f0813cadedbafc3c68ed707dc9ae83c74baf923256b2a061b", "ref_doc_id": "6d89dfac-cd2c-4930-9e00-798a1a6d809f"}, "312a26f9-6326-4455-9802-447184d75875": {"doc_hash": "c8952bc53db405a8b9360227171bab64353ca6e212d1e0f16900389def4b9156", "ref_doc_id": "5dab64b8-9499-4b96-8bc9-daa95a7b6f48"}, "3351d336-6e7d-4e87-afc8-521de6999497": {"doc_hash": "99532be29591cab7b4c97f574068fc8eacf24845f24665a5ebbf9acbe186c259", "ref_doc_id": "264916e0-4c2e-4106-a71c-8fb69e8ef786"}, "f36b09a1-13e3-4d48-b7a3-bb2fddfdf65c": {"doc_hash": "3c26fa39608d1fe2f9b55e9b1324be32e30f70876f53e8617eedbc57516a5138", "ref_doc_id": "df6f39d5-7d24-41bb-a50c-2a0781b36859"}, "83aa5243-3c09-4926-95e2-c3a7c6ce7afe": {"doc_hash": "dd027fd8603c58142d1927ed4d52645044cf418a4e792bfff1ea2f3a49c7619c", "ref_doc_id": "8fcec702-fb6e-4017-81a5-166f163b922c"}, "fab305d7-dafd-4290-9a89-44ce176c7933": {"doc_hash": "51ca3045e27670e344cd4d1c6628c39741ac5983a18b009065ae264cbf47750a", "ref_doc_id": "b7f1251e-a42d-46f9-8496-e1b620f6364d"}, "e84d2644-cf4a-4715-babb-8c696b2a15a1": {"doc_hash": "81ecd194aa98dd077ab678726894e5b359cb47a1557999721a4d900dd6095051", "ref_doc_id": "6d16cda4-babc-4bc4-960a-1bacfab43bda"}, "6cac3d7f-275a-4e5e-8787-c629b18b29cd": {"doc_hash": "969cbd20239eb4dce776b6f7d19c7576cc45ea1ac90244d8d165c3e2c7a3491d", "ref_doc_id": "6d16cda4-babc-4bc4-960a-1bacfab43bda"}, "0f8c3664-2f3b-4840-9662-a99d1b2a60e0": {"doc_hash": "9d9547c123cc424a27760afc28bd761aa6de014f51ac10a49d60770f491bb856", "ref_doc_id": "6d16cda4-babc-4bc4-960a-1bacfab43bda"}, "a6446494-aaf3-4198-9520-6963b190e3d9": {"doc_hash": "8487671ac08ed491d5f23795934f79b21117424f062e1059db5b549a923ee2c1", "ref_doc_id": "22c55992-fb13-40f8-806b-0611273c8547"}, "24ab2465-3c3a-4d45-b72f-2a48c253b351": {"doc_hash": "219993c091a4f645fe2777057c46bd6f8eec74de74d647112f9e7b6aa7361799", "ref_doc_id": "24dc7cf5-5234-4aed-868c-d22f29185bae"}, "63656294-e3d9-4d92-acb4-fbc4e5ba8701": {"doc_hash": "048c488a2598525a2ba8746b2d1bf40ab1a3cb1279ccb2225da9a935f8d29bb5", "ref_doc_id": "a5b1707b-c225-41a7-90ea-20a7fe5a7c72"}, "7f8403a5-a055-4c8e-b968-fb65ddd81724": {"doc_hash": "bfdf74d67bb692e43133682926e1227eb19a7761cc08348544792a121f2a3fe2", "ref_doc_id": "2b6ec820-c642-45a6-b29e-74c002eea37b"}, "1cca2cef-ea53-4391-9ede-4cfd66ee2781": {"doc_hash": "8d64af21015c9221bca9f63753718460a15238f9bbbf9716c32d56f01ece9988", "ref_doc_id": "9330043a-e89b-4a01-bd74-7ee21012814d"}, "ae4fe739-1d37-43aa-95b2-f023540b8c1c": {"doc_hash": "5a4829efc5240b229584da43a1cbacbd63d5b8248723a8531679d2150a81ecc4", "ref_doc_id": "9330043a-e89b-4a01-bd74-7ee21012814d"}, "c1eca416-d4b7-44d1-bc88-2321df80aff0": {"doc_hash": "38179dc80f9b79ae5d2dfd1960e8cae0c1e07fbd36125784ae6b40fbbd758efc", "ref_doc_id": "9330043a-e89b-4a01-bd74-7ee21012814d"}, "752c5dea-e847-4b7a-ba6e-f88fc44a3e46": {"doc_hash": "f9dbb857bbf413992bdcef1dff9cab0be15e57edf8693b12fc887210edfdea14", "ref_doc_id": "1b2ba341-d75d-4f78-a10c-634f35ab3bab"}, "62c2013e-4ec7-43d5-9d5f-f48aa75e8c24": {"doc_hash": "80d06f999bc43012a30226b550d80074e69bc02a8d3220c3de37c6b3cbb9b35e", "ref_doc_id": "c7f6fea0-7202-41e8-b8a8-6941cb6d0b60"}, "8ddc91e7-660b-4569-b184-8e72c9b45557": {"doc_hash": "2402efca3eca46895c5caec0deacbe314195f18c77a409fe355c19d8415df2ba", "ref_doc_id": "ee049f5d-5fa1-41ba-867e-f34dbbc1b52a"}, "f36c5620-cb17-49a3-844b-9748ef2a0751": {"doc_hash": "44114687710598ce1a3069f45ed85fba965da365a49f18afadafd8aea82e1530", "ref_doc_id": "2605691e-e220-4ece-a16c-f715af41d148"}, "26823ed9-fdfa-459d-af1d-029b47342a51": {"doc_hash": "fa23ac1c2b2aa9523f6793ea6b2c59ef016e727dda4cbf75a8c4a075404f90a8", "ref_doc_id": "2eadce5d-225b-4b96-baf5-d94f122f10c3"}, "04ecdf90-0ecd-41d9-8d58-bcfa8b494f7d": {"doc_hash": "e811f1e163e9c0b8ea14efe0f39f1af894744591cae2d13df5cc2b2f95519e91", "ref_doc_id": "58468981-c200-480c-afe3-3d74915299c0"}, "75d590e1-289b-4a34-8810-2f0e6cefbc29": {"doc_hash": "8a1190d64dcb1aec67240f3d90262d7f1bf3a9933a2d364e6e587a575b1b3cbb", "ref_doc_id": "6a62499a-831f-423d-a292-0f923580462e"}, "ca672886-aefa-4d24-bc2b-1b07acc31ae9": {"doc_hash": "ec2188e8bbfbf7883876e1a14075a052bdaa1a06ff5cd95f3ee2b4bceb3e4010", "ref_doc_id": "0c04190e-dc1d-4cf4-ad1b-4b4b2219d1ea"}, "ed9dd215-94d9-45d1-9aa9-413e5e884fe4": {"doc_hash": "4a5ca8155654db3037b100a0b6da924b123b0b6e98d044028e01163b0df8e643", "ref_doc_id": "0c04190e-dc1d-4cf4-ad1b-4b4b2219d1ea"}, "eb91bf48-e379-4f69-ba31-5b99556a9066": {"doc_hash": "38fd0fa98568144521d7aa54042b73448e201e70d6d9382635cd1f9fd32e308d", "ref_doc_id": "e469725b-c578-4a35-8d2a-f04eb507b78c"}, "f6b4e491-e748-4eaa-b25c-9ad59b7f243b": {"doc_hash": "caf2e19240a4a17fa6b6d52db4402ff1f1e0f613c8b28f8378d83398c002fb92", "ref_doc_id": "23350226-4c59-40e4-918e-83833f39ef1e"}, "b4323255-e2a1-4cc9-acec-24a2eb3f55f1": {"doc_hash": "531c5316cfd15cd749c4852d70ced2a7b2a375f57b76ff7752884fa98455386b", "ref_doc_id": "9de24fff-46db-406e-8f9f-5d9fdabd019c"}, "4c17069b-fc3f-4dca-ad04-760ff48b11d1": {"doc_hash": "7c18ca465b8f18ed2f93dd1cf498fe553817045581c281acacfc44b64e42f45a", "ref_doc_id": "6da94435-69a2-4804-b6db-5650b426e935"}, "daf99d3c-e5ee-46b5-bfc7-b3df897585db": {"doc_hash": "e47ac800ca6f1b4fd17408fadb981ea43099c543e1db43e7034653475be245f7", "ref_doc_id": "0e315a7f-841b-42e9-8caa-7da2831a2a23"}, "134b5d8f-36eb-4103-b62d-a076642f0697": {"doc_hash": "345e7c329e3d2f892116ca5eddfb1d7f6bfa531fbf3bd4315bb4ad1ffa1e2bbd", "ref_doc_id": "0e315a7f-841b-42e9-8caa-7da2831a2a23"}, "f25f9bc9-0825-4180-b32d-58ac30620618": {"doc_hash": "0d54f16715abcfae7fc42e86796c8d9b054a435da4eff33ce413d029b1fbe16f", "ref_doc_id": "b7d0d0c3-f210-4a45-88f8-3cd5930ddfa7"}, "e2c5e5f8-b6eb-4c42-bc5b-94d6343a0699": {"doc_hash": "9fc43a667564aadd0ffb86503aec82b33ad932f3ccbbd9e186c6a21463714202", "ref_doc_id": "da8a2c30-5865-4755-9d0c-39b1f3447d96"}, "1a0f882f-b024-4086-baba-6f005b77c993": {"doc_hash": "4f286eed46d8ebea0c1588b802d7385895dc28d9691110c0999b447f9127b30a", "ref_doc_id": "abf962b4-bb99-43a7-bdaa-4bc4281791ee"}, "58e09746-c6a2-4c4a-ad7a-7e16a3c092d9": {"doc_hash": "a333431b7406502bd2a3f7311cf41c7d139ecd427444da297d75fb47c47626ae", "ref_doc_id": "49bf831c-28bd-4d22-b81e-edd06f3606d4"}, "1369991c-bb24-4d9f-b3a3-bbac43b344fb": {"doc_hash": "b9af886b2c1aca1b410060eb4dbb0757deebda9b18029d6a7116de445ae95610", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "bdcc1de5-1d06-404d-b7d0-dec8cb8bbeb1": {"doc_hash": "2e69f030324ad1ce71bd84df7e4449ee65a87a95bfd6ebe4660236eaefed3ef9", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "8645e359-c24e-4831-9e00-2982201276a2": {"doc_hash": "700540bc4917d8754e59b2987528396b01d023d9942bd37c6ae26c31accb48ef", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "3da233cb-2dcd-42dc-aa35-7ab800d73106": {"doc_hash": "29be9cc47ae619eabd304f88ee1e0745689b33d6e2c6d8e0a46e40eacf61d136", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "3e84cf80-c3aa-4b98-ba68-f222210f952d": {"doc_hash": "f6d9994e0e1ac56ebd67dc9bdde6736c3e61bbaf058305cea220cdfe5dbf408a", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "ac9f7bfc-f801-4f82-bbe2-5b99c7af3adb": {"doc_hash": "458fba9a5360e0f75f061d08950d147ccf8a5b55b0248fb51d7ce4822449f030", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "c532ca25-eb20-40e6-9c02-fa71216ac39f": {"doc_hash": "488671e14e25677afdded70846eb44a2da125da719ba3023b5264a0dc0640fcc", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "c114f88a-5e71-4d16-aa73-f6248f66031d": {"doc_hash": "cd619d741e3bacbde0919eb4a9c373ea93b052dad890aaa4f483917385c47bd6", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "266070a7-8688-4e82-8382-6902cc9617c5": {"doc_hash": "1ddddd9eb995340e455be1a6449a9ad21f683ba73934c1896d9baca84584da42", "ref_doc_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5"}, "90d22f7e-09fc-47f4-9fd4-529445041d19": {"doc_hash": "b1db318aa31197eec2273b11d46db47e927b22ba272ba01ecbea9d88ff59270f", "ref_doc_id": "a6ec49f4-2721-4ba8-9946-4b52de895769"}, "b5d73d73-5606-4f11-b112-441c7f84422b": {"doc_hash": "a498d4bf807b87ee6ef23e5a96b188348672e7567cf60f011c548cdd6e6cef00", "ref_doc_id": "6dba962f-eda8-4374-a375-69ac4d51998d"}, "8e233ad7-b95e-4c19-aaa6-a99adea87c30": {"doc_hash": "2abfbe218fd95dca226002855a3ebf5b74c02fb8b907c187cb0bb5c5e699c2a1", "ref_doc_id": "0c722044-a4cb-40a2-9f4c-6a9b44d69d94"}, "658b7a90-6d07-4f5b-93ca-cd718096f955": {"doc_hash": "8184bc3ba43968b8a9d5fe857324fcc8f70f51f409fc5536b5bf865068a4a22a", "ref_doc_id": "14d39e40-bfdc-4c06-aadc-0e77c175a183"}, "bb30e67e-bfda-4f39-a79f-fe614fb6dcc0": {"doc_hash": "32dbe14032ab3f63b4ce53343d610820e3dbbd11f3311cb7d29c8a9c49111602", "ref_doc_id": "93f4ac72-646a-448b-8d9f-940d48b8a556"}, "0cd70036-1699-4a65-bc8f-f90410dafb88": {"doc_hash": "1f7b092c25fa4717757720ca02423a1af0d14aff8aeba25e0a7f4bd078fd6772", "ref_doc_id": "cf1b32f9-cfc3-470e-8f36-a1fee2153929"}, "4a830415-566a-418c-94d4-887cb4f8992d": {"doc_hash": "edf3dfb13f87b57a5168b4987515bd0cff806ad46d50e356ae35398e4be7979e", "ref_doc_id": "c9affb97-0c29-4021-baf2-863825851d58"}, "607e6268-383e-4a35-aacc-2189bc3559a5": {"doc_hash": "207577b96f660d266941a885554cb2bc352f6774ebb8dcc4efcf492c427a0829", "ref_doc_id": "e493a907-1754-4919-bb87-2f7bf5cde5f9"}, "afa355a0-2718-4c88-914b-0bbb313b3eb2": {"doc_hash": "9d0229405a2747fe7a8f95586f4c0edb4d4d8ab000126dcaf7289d4897c4d4d2", "ref_doc_id": "d4b63884-e2d3-4a10-80c7-7874da26bddf"}, "990324bc-d980-4d29-9d83-53322d829963": {"doc_hash": "a21c609da11ee03175703e7cc0e8b831d9a1ca3c7a4e6606da6e20e8bd44b852", "ref_doc_id": "99a871ed-8ab0-4918-9dc5-c75a6957d853"}, "d31e390b-7222-4b4b-9072-97ac49471559": {"doc_hash": "b5f10759fcdaf1e00987eded77e08577be592e31b7012d317e38bcc399d00f26", "ref_doc_id": "83412d26-6498-4f0d-8b4c-a5fedef10173"}, "c76c1843-995f-4b4d-9bfd-2c0bb26a1e37": {"doc_hash": "09a29828e430b3f4cb36e140bfe3203041288c2a337480cdc7ee2abf48e89b56", "ref_doc_id": "18b97950-5b23-48c2-abaf-b032f82d760c"}, "9b6f58dd-24dc-49b3-b5b5-2dd196dcab86": {"doc_hash": "dafca829501a401090a9576ce3f0fe2964775a9fdeff6109dc257846a07c9cdd", "ref_doc_id": "7186d9f6-cba7-47a8-a2e8-57cb21e56e79"}, "818fabd7-f5db-42ec-8561-43baa12fd2d8": {"doc_hash": "d8d46535395f7568419bca5c6e3db81cdfb163b0147d65cbe2d6d5eb5faf52e0", "ref_doc_id": "001e396d-cd75-431f-a765-e29eb8ed0bf1"}, "d5bee6a0-8fa4-46b4-b8f4-4982961e11b2": {"doc_hash": "d3323d91e6319a2fb88d6a8cc4afd01e88f2f77c577f91a6c90f003959187239", "ref_doc_id": "4cb0c00b-c699-4718-8736-9d0c2c471abc"}, "ca6c2b87-7a3d-4b54-8935-7421283a887c": {"doc_hash": "8ec066e08f94e0d000431de039651c0a9574bf8a4adf826b68c50172ddb34737", "ref_doc_id": "8474b7ab-88ce-423b-972e-e5a20c38d4a6"}, "1513ae2b-b41a-43e5-9b4c-538bcd249223": {"doc_hash": "089d06d9150607264ee09bd8d0891b731e53d5e6164e74bdfeb6898a3946f2fd", "ref_doc_id": "8474b7ab-88ce-423b-972e-e5a20c38d4a6"}, "bc490d04-b15a-4c9e-b4fc-e711be129d27": {"doc_hash": "69e81092100858d707e13bdfdff9f348b0a34a390386313304d1554ec4a14f45", "ref_doc_id": "6244de17-417c-453b-be2b-a68eb683fc36"}, "a6857a3f-7f01-40da-8000-50f17e9b0a73": {"doc_hash": "eb37da82ce47aff29c0236e0248cd7864acb9c430ea5e6140f388d5cf3dc6985", "ref_doc_id": "6244de17-417c-453b-be2b-a68eb683fc36"}, "b307fe2e-9636-4211-87c7-f0da18f084cf": {"doc_hash": "3aa8f79885d4e6b4ba096786ec4ad8b7a168eb9acd610c8597b76686c3f07e53", "ref_doc_id": "6244de17-417c-453b-be2b-a68eb683fc36"}, "a35a7f16-c612-4f17-aa65-3ad58eff5cd9": {"doc_hash": "7fff17429a69acb387dd4abda13ba04788c9ea2e40d02cdca917706683836b90", "ref_doc_id": "6244de17-417c-453b-be2b-a68eb683fc36"}, "461436f4-1e93-4540-bef0-d2087c6b4523": {"doc_hash": "3919e8f66b3f13e9199fbfc4443de1e31157b28e746044842fa572ac9e9099be", "ref_doc_id": "4c60d868-559f-42ab-bafb-61d5c7a734b5"}, "3c55ee3d-f8d2-4b7e-b9d6-4b15bb24ae50": {"doc_hash": "b967992c8eb13f223d59ce7095fcd3c0eaad1be8365dbffd309e61498db7efcb", "ref_doc_id": "d76129e3-edb6-4b27-b369-742c7e2e3ba5"}, "bce6c32e-3ab8-4192-8cab-fdc561c1fc9b": {"doc_hash": "ff114a9611c6117b37c40d40308a517237a32d2d5dd67a2b729a4d0d8ad39584", "ref_doc_id": "744021de-da76-4255-9386-8184cbc246fb"}, "ddace1f7-1087-4fa6-bf67-ceff6c13e973": {"doc_hash": "6016cc3e0ff7476934b74bb03924edfe4337fa03dd297bb892e62d5be9a12aaa", "ref_doc_id": "a4a5584f-1c74-46d1-b56c-82e30d75c5cf"}, "5a264cfe-cd3e-40ff-a268-063bbe0160d7": {"doc_hash": "2f9f351fa6a7715f19ffdd993bab109f6b93487aba04b90fe9d99890cb707a26", "ref_doc_id": "ab6c6f17-4ab1-4329-b665-1ee1070e2ad2"}, "3e77ed13-c024-49e9-8e38-0e8903b819e2": {"doc_hash": "eedb3d026469e156f16812c1bef87163e61b96678b2322851180baf988344e16", "ref_doc_id": "8c186466-93be-4e75-bdb4-e9c3683f8678"}, "07cd03f5-11f0-493e-83bc-dc6dd2c98688": {"doc_hash": "16e5e4683f395d00511c6a86cc3f22656f3601bda784a43b012e110b1e5c9eb1", "ref_doc_id": "067a8088-66ce-4ba6-aeee-0ae4e4bc2f5a"}, "e76b53dc-9a2e-449a-be82-3c86f3f6beec": {"doc_hash": "9aa05bd763adaf9b66434747aef5b2e0bd1c1ac5f097aa0e8c45a622e2286467", "ref_doc_id": "f1379036-f5c2-4ce2-9dc4-1f07d99a8846"}, "491469b2-53cd-4588-9489-7611d17f9583": {"doc_hash": "da2d46bfe7b19630cad5c977131e09f5ab8f6386b5b998ff927e9eb2c42a1054", "ref_doc_id": "f1379036-f5c2-4ce2-9dc4-1f07d99a8846"}, "e1547094-a26e-4b38-9d84-1daab33ce0fa": {"doc_hash": "3909c2bceea2e5a1066c760017c061877b7f9720df83eb4524f15c90ce860758", "ref_doc_id": "de22733b-502e-4dbf-b070-9d41a6928b9b"}, "acc5b919-8e92-47ef-a49f-e0302a659093": {"doc_hash": "78f4e2c835209b495f20dd7cb0dccf3f1a02c1e528c4a165e81c974e6d000df0", "ref_doc_id": "7a41f28b-e2a8-478e-99c1-4ce440c2a4b7"}, "b7acdb0a-ec0f-40af-a3be-867695649b73": {"doc_hash": "057fec6ef4cc568f303ab3d4056d9ce193b57bc761344e1f34cdcb1ea1a8cfaa", "ref_doc_id": "4b27a818-bd96-4a1f-ace1-345e5d3f058d"}, "0354f21f-a5f7-4184-90fc-3f5e413361b7": {"doc_hash": "5d15a9c7f7d1057790363c4fbde40fb308df6bd63eb465003d13445cd8869264", "ref_doc_id": "0b9e0307-70ee-4f2b-8019-fc806dd17dc3"}, "c92ef262-2bb8-4865-a1e6-d557c81eaff8": {"doc_hash": "c483f2c05e523a151d6bd27c3767b5bf2d3c64d64677110fe553fb28229b2fcc", "ref_doc_id": "fbffcb8c-4da0-4423-8925-30d3dbdce156"}, "b8ac4fe7-8910-4928-92d9-de2ffc12988b": {"doc_hash": "d6464577151c424999cb43153b4dc70fa2c91b35348046f459c1a0b128a7f731", "ref_doc_id": "7b25e27f-fa12-4e0f-9e53-4a61010bdcf1"}, "01b34120-fa60-436b-a079-1d24eac145ec": {"doc_hash": "07395ca97c45577a4057f4b2aec081a19a7b39ec56f716938d5cb9f65d0854e5", "ref_doc_id": "692f4a62-d411-4ef8-949c-b73941e653aa"}, "d2d7a324-cac0-43d6-abf1-9841f613b5f2": {"doc_hash": "9846ad237d9cc6b116781e1d650c7e6ecd562528d194869919be0933168f056d", "ref_doc_id": "583508e3-d4be-4c1b-85f3-5d557b1d2220"}, "29bbb71b-7d11-42f4-92f3-3fc507391e2b": {"doc_hash": "3ebd38635f3ef48f8fcb01423ed829d4fcdf2b6affae2640eeb69a2c7bdc3c68", "ref_doc_id": "583508e3-d4be-4c1b-85f3-5d557b1d2220"}, "dc3df753-959f-4793-8307-90f642bd3250": {"doc_hash": "20df9eb150b100648291ced7995aa47ef9346ca9b2135d6d64c2e92913398eb2", "ref_doc_id": "583508e3-d4be-4c1b-85f3-5d557b1d2220"}, "1c31bb63-2f5f-44f0-bca8-252890763a6e": {"doc_hash": "a2cf2c2ad1ef1e1086565619052be7dfbb83a18e3ffe416ca5f613563fdb0d38", "ref_doc_id": "c13c6518-9797-4e0d-9141-7049fe1346d5"}, "dda69810-d236-46a3-b769-60adeb812233": {"doc_hash": "af6845b13816fd24b419bb4180cc4af27beb1fe84add64527eff1d6d518cbda1", "ref_doc_id": "c13c6518-9797-4e0d-9141-7049fe1346d5"}, "090b3b46-9a65-446b-addb-6c6f36d618cf": {"doc_hash": "8202b562baf5547dcbd7bceb78b61cef551021aa3fce967c1a6ef20e821e9571", "ref_doc_id": "d6b670f2-0215-4223-97e2-b2b3a1102108"}, "fe7a46cb-cbaa-44b7-a170-95fbceace055": {"doc_hash": "c2a476ab44b50748d73d084dde2e11537b0454c436c0dc9da04d27114df38586", "ref_doc_id": "6b36ba21-c6ab-45e4-bf49-a849a2b98c73"}, "6d614a7c-db0e-4fad-b858-c94398bf472b": {"doc_hash": "0489765092ff9e4e7254b35d0c81baee36a3e9631c6b495c8ad659f8c737b88e", "ref_doc_id": "6b36ba21-c6ab-45e4-bf49-a849a2b98c73"}, "708f2286-03a5-4523-a013-1b3b8409f3ba": {"doc_hash": "69388d2f288aa2cb0320ecf898a853a4ef4208c8ed37aff5d7deb25a41e9afbf", "ref_doc_id": "e591e5d6-8d4e-45e5-a792-19676a6d9605"}, "3c001b22-4e9c-4532-81d0-603b1df4c15c": {"doc_hash": "584f6043d9855bcfb6d579888d6768bc462a83262e2f836d5bc0b1ce12089ce1", "ref_doc_id": "13983090-f65f-48b9-b71d-bd4c9aa8490d"}, "1753368e-0d73-48e8-abb8-dd2c261a18ee": {"doc_hash": "6d3925f1f8ef5e00c1e5e0405874809f1803cdfb207076c3a3471c2ed96ec90e", "ref_doc_id": "72554f62-e08a-480d-8392-871cf190eb85"}, "728eaf11-fd8e-4045-84c5-85c6180a80d3": {"doc_hash": "0e0e8ef11090e95d3933ab003ceca5f7ee8ee4ad0bf2f58fbea34f164826d39a", "ref_doc_id": "4fa9a752-9ff4-4586-9201-c2ebb5a2e67f"}, "2b91ef1a-bf99-4af1-bd0f-abd24fc7602d": {"doc_hash": "d00e0751fb8e57ae2d5a416ccc2f6c1a8401dac41a23fc04aafed29a34c20a50", "ref_doc_id": "d3eb513b-74c4-466d-b043-b9fbb99eaea7"}, "01f74b0d-6c93-4516-a962-0d33fba1d09c": {"doc_hash": "70fdb8f33d755e44a7fc36561668f2df60f59f191fbd98240dfb97b83981c54d", "ref_doc_id": "8658ee9e-5f94-4165-bde8-5eb944f66b4d"}, "7b8d66cb-acf6-4b2a-b811-87a71dd65c0a": {"doc_hash": "58693ddd26735ee12b975e2726472a72e905d24560d07f802e1febc35be8fc2d", "ref_doc_id": "37041e6b-f3af-4bbf-83e7-7a6b054dd476"}, "b43f319b-1c45-4a9d-909e-6822424e233d": {"doc_hash": "bc8066bac8c38e37e0303a3a37a65ece6dbc382151f52c664b3c29d8b1a75390", "ref_doc_id": "40b2b11c-e13b-4b31-a3eb-e28c95e4bed2"}, "d0c4dd9e-ead3-4bff-a81a-979589f1187b": {"doc_hash": "a124188a7e379ef9ccac00b482c368cde1f51fc9ce63579392400e7d632ec5de", "ref_doc_id": "bed6ce8a-dbe0-4ef7-8e1c-167947e586b9"}, "87a92049-411f-4a25-86cc-8e830b8f4df5": {"doc_hash": "45390fd830df222a45acd0d9597700df6b7bc42d8227dcd604a9c0037e735913", "ref_doc_id": "e442320d-4037-4bc7-8595-edfa35ae9456"}, "d76d8899-a808-4c5f-9856-5808f7399579": {"doc_hash": "1ff4226def0cb6bc8d7df6386cb995e877346f6c986bdc248d7f6815604b886c", "ref_doc_id": "b75d4795-94a5-4cf9-8df7-f7ed22423be6"}, "c1b5c9fb-611f-4e05-9141-b7e38bb2d5fa": {"doc_hash": "e179ae93b66fbc0eebd0257ba09f1fb7dec9e6d718b44ce43e0f975ce0d95d4e", "ref_doc_id": "410672fa-014f-4455-a82f-9e20ca8141a4"}, "cc07e5f1-915d-4abf-96c6-f77dad5b3ee1": {"doc_hash": "0a8e121bf1d19f4678767c5848395a9421d7d27e1336b2a1160944b3324463cf", "ref_doc_id": "6369e180-1485-4fc7-9815-1239aa7b1246"}, "538782fc-a847-4a95-a9f6-d597c1505930": {"doc_hash": "3f51d3d63e81160ab7738264b2d55838a8bf4044ebc63c558a1ba40f4b9f874a", "ref_doc_id": "4f8c9b27-f5c3-4269-ac33-c14d2e3186e5"}, "9eb084ea-b1c4-4dd6-be9e-0566427f9af7": {"doc_hash": "4e2495f51d87cf0cbe340125206a7fe558ffe51a21fd3f62724d0ecd7623deeb", "ref_doc_id": "653580ba-9804-4817-a658-1d654d434395"}, "11804f52-b7fd-4f5a-831e-9fa4b2626a83": {"doc_hash": "a60284e39f6e77c8cd53c8e1e73b5ef450c8f5bbb1d8cbab74690fc4a56f6e50", "ref_doc_id": "a7e3d947-82f2-4775-9941-b5643f9482b0"}, "a4d7367d-b46b-417a-9f1d-b7e0a97768cf": {"doc_hash": "1433696c8ddc556d90b971ee2eba9c1c742f3fbfd29e9714f0127c9f5bb2cc35", "ref_doc_id": "229093f3-3138-4816-9550-b7625c8dd1f9"}, "b1c21496-cf4f-4114-a988-f5ca08cd6a00": {"doc_hash": "74f69a9fded38f9214525ee93a27cc5b7367e8d6cce66924004ad6c1e60828ff", "ref_doc_id": "3384521b-7f77-4c93-b6fb-f3e87ba96198"}, "e6c59747-7be1-466f-a30d-b783577c59a6": {"doc_hash": "24e734a7bc93fae81aaeaad6b30ef431c7c0ade81b53528300570776e3158403", "ref_doc_id": "3384521b-7f77-4c93-b6fb-f3e87ba96198"}, "b8534770-760a-4fa3-a0b0-163b6825dfd0": {"doc_hash": "17d78bb60ff8c27d3413cbf8a1976ee2c87635558100d47290a67549aea4430d", "ref_doc_id": "8b1b3d92-6a39-44d3-a7d3-297ccdd96c5a"}, "723ed975-1add-4c2d-a413-14d22565705a": {"doc_hash": "7354f70a49653144acdd682f9db3f03491596bc6099eb8b40078a2e7bc63503f", "ref_doc_id": "7dc4c0d8-e421-4b38-a621-5b14d3ceddac"}, "8ea4a482-450e-4b9e-99eb-dd4453e2f14d": {"doc_hash": "0baf43c929de3256431c65af1287d8e33e8981bf4016479a10f75e0bde1de00e", "ref_doc_id": "d4782ab0-30f2-416f-9324-abf42622fcf8"}, "b547964c-2e71-40cf-9855-cbe8afc06e55": {"doc_hash": "9744c2d28a615e7453456b0797f183763cd39c51fa42e8d5467aa4d8a925a688", "ref_doc_id": "0f799979-c08b-4342-bc19-8dea7255a769"}, "527bfde7-d7c4-49a1-ab33-a428273daf2f": {"doc_hash": "b031f54f7e3e59ccc3e2490952da22a8d5fff93035b8e1e6a6bb7b5eccf58ced", "ref_doc_id": "ea472040-355b-407c-b0dd-021b5ccaae9a"}, "23737fb5-fee8-4ede-b4b8-59910f8bc373": {"doc_hash": "582638d66e42b2316d925a0aacf952c8f76906e67acb054ed4e7ca634593e3a9", "ref_doc_id": "21abde37-b7f3-4706-bd70-608bd96f42ad"}, "78abc551-2069-4142-9e8e-67ac0033ebdc": {"doc_hash": "d6617094e0d40eabcf3684e1657dfbf0d16d8c4e5382003d3fb87400eb9d4869", "ref_doc_id": "fc13cfb7-e4b7-48d7-a96e-fe1551a66467"}, "d34af291-fa2c-44c6-82eb-48f0b90a7f4e": {"doc_hash": "585e792eaae6f6c28342bd763fafaaed89364e1f4f3aac377fffeed3172f41cd", "ref_doc_id": "02145cbd-934f-4faf-93d3-383d83cc3fa4"}, "655a484a-ed22-4625-85f7-ba57c6accb8d": {"doc_hash": "06f3d2d842269c751c1146ad71030f649b55b79c0c104ad8740fbe6f36e45915", "ref_doc_id": "8664dcc4-e923-484f-bcee-cfda6f77f0a9"}, "ca92d59f-ac8e-478c-8b18-ef6ae4f9d309": {"doc_hash": "3cc5370e51c40de2a0b8d91223107576570bfdb22d81368b3b52296711c0694b", "ref_doc_id": "ce26248e-e970-4ba2-8d9b-c5a97084f5de"}, "576a3288-4706-49d4-8706-0ddc6a170fa0": {"doc_hash": "ffd38e9ffcf00dd50800ec1f95e3173ccc6817e2d25283c3dded88f9521176ad", "ref_doc_id": "ce26248e-e970-4ba2-8d9b-c5a97084f5de"}, "d04c16a7-c570-4492-98b5-d13820af0251": {"doc_hash": "f3964b892efd9a00f6b6131a6b0d693557ee696bbebfe6627bfa6beba0ce70f9", "ref_doc_id": "ce26248e-e970-4ba2-8d9b-c5a97084f5de"}, "656f8edd-bc45-4ad6-a095-1c80b30338e9": {"doc_hash": "debe80949f4cab3b410518713bf8037443c2ed58d8f373ec5e485b66ed33e2fd", "ref_doc_id": "ea3dfee1-4caf-4a42-a3ae-770e5e05bbc2"}, "5e87cf59-bf8b-4d35-9b9a-e785182844b5": {"doc_hash": "aee8a6597426543559e5c5df1035f6d112e36ca6267510c5e3577fa533212143", "ref_doc_id": "83e97aec-bd7b-4da0-acac-b3a27a37e031"}, "6fd501c7-b082-464c-8d24-49f44e45c9ee": {"doc_hash": "f7b6169a7f5c2764699c8a7dc9d984ed99e6bae0b95b1463974eb53b5a39fc3f", "ref_doc_id": "0c77cd49-6487-41fb-a280-74d979e8a698"}, "9112d35e-cd82-46e3-b2e4-1ba6c83d0727": {"doc_hash": "352f8be7edd7575772743b00fbb814298ca35c437bd2423f90c3fa7fac42e33f", "ref_doc_id": "0da429c1-bb29-42a2-b88c-7aed00f6d1de"}, "a6033f94-ced0-4d14-a8d2-ad05b4264bc3": {"doc_hash": "564857695d258cb7e4e870684d958b47d485599e8c6524c3351d5a97ab0f0d24", "ref_doc_id": "f9fdb3ae-ad98-4985-a026-e07b64492b74"}, "7e31d398-fdb0-440e-85ed-7f841f82c8f7": {"doc_hash": "2d947881f15dddd75d5e26cc23fd5cf3a743e157ff294339b1f394217865f659", "ref_doc_id": "884a6e5e-8476-4039-8546-7d19b636f3c5"}, "d1dd32eb-446a-4033-a773-ce6e50f810fe": {"doc_hash": "8518f40f644ea7af9489c24eb43df1fb49e5b8202788c21783f07ef15a7ddfab", "ref_doc_id": "b85444e2-1908-4b33-97c6-fffa69287381"}, "bdfb1451-9c27-42b5-87ea-2d0ab8eed262": {"doc_hash": "b3f9424b5f1155f8916c249433133af41011d83c9dc893316cc825fb053d52b2", "ref_doc_id": "14263c8f-90e0-464c-8555-610a44d14da1"}, "58ae4b66-3599-4009-a72c-6b466942acdf": {"doc_hash": "88f73840fbc13477d96e611aeb1b9ce1fe90efa268e90added46000b3c7041ee", "ref_doc_id": "c2bc1ebb-5c27-4528-ab24-5d371fd4d681"}, "f7cea8ce-08eb-4014-b482-3430ae73c40c": {"doc_hash": "b6e48c5940763bfdc09b48d3dc0ea2b5d2f36bfd1ec7a02d65c8a508b2b1889f", "ref_doc_id": "40c1e146-3641-495a-87fc-df10d4045174"}, "e7f89d06-8a09-4ca1-aeaf-e265e00c7bca": {"doc_hash": "250ea9ba17b8593a363eabf41d5a29cb44395c7885e68879105d7e71aa064de5", "ref_doc_id": "40c1e146-3641-495a-87fc-df10d4045174"}, "5166aef1-3aea-448a-9814-83a8f5493185": {"doc_hash": "65ffc0eb3fd8795ce47b449dc06b0434031f6225638815523220d9760c84ed76", "ref_doc_id": "67e1f39b-a8fc-4a1d-9564-c2704fc50f83"}, "3aca073e-96bf-435d-8e74-d173f9d5ca15": {"doc_hash": "8407732da6faf83f448b997c548c8f78a1c1537b57ecf8f72c16cbe56f0ca912", "ref_doc_id": "f8a7ae6a-91f7-4efb-8c9c-8922385a814b"}, "bb82a763-a146-44b4-9edb-210134feef54": {"doc_hash": "75a3434b12e35fd5981bffb36b83d10b4d447246f2ca3a04624543137eb8aabc", "ref_doc_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc"}, "377a4c7d-dbf6-4dc4-ae4b-11918c037062": {"doc_hash": "81c1c0b87c8b12c8d188d556793489ec7099fa79c717490c52878e1c5d4c5b03", "ref_doc_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc"}, "87f260b9-5a4b-4692-a875-606efce54f02": {"doc_hash": "9f042a424b339c1a27c24ec18185d66ed217a9e85d24749c6208f243387d0ff3", "ref_doc_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc"}, "7ae5ef6f-8505-4454-a4ab-d4ae4bb89511": {"doc_hash": "4541f511731e6dcc4b91537767778839928254b89381a9c4f47990f34fe964fb", "ref_doc_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc"}, "cee8624d-44b8-4b31-9025-1ed16e48d090": {"doc_hash": "92c9bf68eeece4394fb11b521142148a28ad19f3f95e6960a961b43dfa97aa3d", "ref_doc_id": "7f7267bc-a13a-43cf-9ef7-db2c9ff1c252"}, "a5073d26-4061-4667-87a2-954e6ec492bb": {"doc_hash": "cf65312fbabd1eee4d8c051f98a230f7f500cd5c0612dcd3c0eed3e75c5dd5e8", "ref_doc_id": "61e4a964-0740-48d8-8268-f714df1b37f6"}, "bb4f1aee-5c59-4b42-a2dd-c2e15e16eb0e": {"doc_hash": "f9e275fc6f9d7b7df51bd810ec9698254e3c890614db9bd9596036fe41ba15cd", "ref_doc_id": "dd75d9cc-0703-4376-9260-1172239c346c"}, "f29e0489-f5b6-40a1-80a0-75959ada2449": {"doc_hash": "7059ad47683fb6fa142279db8fd9f941aa6f5e7d49767821bd2ceed3b30330ef", "ref_doc_id": "498780bd-084c-436b-b101-114bd40fc00f"}, "123f2acf-5b38-41a4-bad2-0e38e247eb1a": {"doc_hash": "c188fb98fe97c55c53b24b890dd16504489fd31db17eab2fff1c64066a7a51ec", "ref_doc_id": "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a"}, "fa9b1d42-6863-4bea-92c3-459afb3e14c9": {"doc_hash": "e97d60bd39af665911f8080fcb0ce38f2425b4e6cea3cbb742124eb0b55b7e95", "ref_doc_id": "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a"}, "f92f839f-0a59-4333-baa4-c896467827f7": {"doc_hash": "325b7215340aa4b11eb6954080a2df2d7ea709187023e164b28e8a668b8cf201", "ref_doc_id": "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a"}, "26479a0b-868b-4989-aeb6-009f32b9a5c0": {"doc_hash": "29f85eb41503dff4b710b540629bde5f960e93adb070fe67caae39f5605c2292", "ref_doc_id": "63ad6d6e-f431-4dd2-834c-381c4698680c"}, "d77deb0a-cdbb-4438-bc81-2bba3f6396f7": {"doc_hash": "1fcdcfe83e0a729467924367a3f13558e2d59a906b372f64f7145093cc7b71e8", "ref_doc_id": "63ad6d6e-f431-4dd2-834c-381c4698680c"}, "bb507157-ddc1-43b6-b3c5-db02cc20adfd": {"doc_hash": "f8212963f2175e2be12326b5018d9864a69c6b04860ef61f2a1d4fd56ef929f7", "ref_doc_id": "63ad6d6e-f431-4dd2-834c-381c4698680c"}, "7c3f7ee5-af53-4668-9ff7-cf2cbbc7e7ed": {"doc_hash": "86548cb8ebc1add610095137f853432a15abad6f48419cc65a8d93e3b09e18e3", "ref_doc_id": "63ad6d6e-f431-4dd2-834c-381c4698680c"}, "9074351a-a1d1-41be-bb1a-579f59bf5e81": {"doc_hash": "49c3342c583c84f253d78054d958abaeccf72947e0eb36634c07674d7bc12140", "ref_doc_id": "63ad6d6e-f431-4dd2-834c-381c4698680c"}, "ad2adaaa-9363-4636-8d8e-3f32488e03b2": {"doc_hash": "b2ac39a61d413071a669e2647784acb977e4abcddcb85dfa410558533473ff7d", "ref_doc_id": "a7b811d0-431d-4178-aaf5-69971b5cef81"}, "0c4db018-701d-4b13-9c02-d4f26681f132": {"doc_hash": "8aa64aea3b1be53cadc8b554b451931b0ac194f3ec4f0c38b619cb9ea991646a", "ref_doc_id": "98b7a51e-d10e-4478-bcf1-11a2c20fb61b"}, "6d7522ab-208b-4313-af5c-7bb15e9d2242": {"doc_hash": "0daaf4048cd88dce96b1bf1c0d0a87c21b6f2bed808f0acab125b80f12038c5a", "ref_doc_id": "ad0d3a70-5a2a-4f0a-8714-24ea680cd55d"}, "861f1f8b-77a1-49f7-9af4-1fc2d4f8090e": {"doc_hash": "18e92bc31a255f506e82d22db0488b6fbc05c50c99d70e1797c89fcbed6a6674", "ref_doc_id": "228b53cf-a254-4aec-87a0-a7d8de7477a1"}, "babcb2dc-0fcb-4374-a043-58409b0ca870": {"doc_hash": "58e74cc95b665685e70b724289b2571732c635434052993e5f8e57ef1819e171", "ref_doc_id": "2461ef18-ddee-46ea-8a3c-09b960d4b6b4"}, "2fbd0573-143a-4e00-b824-8cb012b46335": {"doc_hash": "c3617bcd470b7c4fd39ce22348db65173669982f8e6ee923357b3b07183cf552", "ref_doc_id": "2461ef18-ddee-46ea-8a3c-09b960d4b6b4"}, "9b5660a1-c47f-4e39-9201-ca9c195b791a": {"doc_hash": "7f0caece8061dc219ce516e18f06521a191ae967851a810d182dd79f18cd7ccd", "ref_doc_id": "0075cbf1-8468-4518-b3fe-a27aeb62c3db"}, "00a6d016-18e1-48c5-8e20-df850088a030": {"doc_hash": "ab0c9df1a78964962e907f4e1ca6d04713cdd5187a812171d53f4e8a4d13e3c9", "ref_doc_id": "1bd5f3e2-0a5c-49f8-9a8b-67b5da7d4e2c"}, "fe6f6bac-9d08-4d81-9090-b7f63c989442": {"doc_hash": "e4bfe66baa5e64a16b9580427a13c5a9c8e8a73976ae119630179030f700f1c0", "ref_doc_id": "df301cba-68c1-432b-8dd8-bb0de0bd1872"}, "70bef62f-fc34-413d-9105-5f9204a08c25": {"doc_hash": "c012ffb0b7d2e8b6b3eaafb9b8328bf9a5e2fa08b24adb6a638084a95987b517", "ref_doc_id": "ff31062d-2f43-423a-bf02-d48212ee969b"}, "a8a9e263-e33e-4ccf-bf0a-f7df175e5b84": {"doc_hash": "f3eced5deac0e7d708f30cbd89c0bffeae3962d2808f4834b91ab168939f03cc", "ref_doc_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd"}, "bd84b1cd-4ce6-4bc5-b9f6-c8cbafddb0ac": {"doc_hash": "82d08b3efee9a20a2a976b921b1d8c7b51a0b6efe42ab61cf6edf6d40fde691b", "ref_doc_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd"}, "fbd78def-1a86-4c3b-b87f-8059aec7b3a9": {"doc_hash": "5c1b16f6209b4f2ce8b46a3378b82c7c43cf82fe84994ff0b5ead2852d36b678", "ref_doc_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd"}, "cbc1ff14-3215-4567-8d4c-53fd3947c25a": {"doc_hash": "4b553f4540328c7b122f31449c9edc06a8af89fa9cbe570ad37391a31af53195", "ref_doc_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd"}, "99291768-b043-47c2-ad4f-88a454c82fd2": {"doc_hash": "48cc196bcc4136f014f9515791315cee50b2ad7008a275b4ab2ba1709351786d", "ref_doc_id": "fd0d0fcc-2b2e-4b73-b503-deb56f81463e"}, "d88b889e-113b-46a5-b08c-30fb90880dcc": {"doc_hash": "8aaa4d91c7315b4106409572942136734c8f37f15daf92c11a3afb931b530105", "ref_doc_id": "fd0d0fcc-2b2e-4b73-b503-deb56f81463e"}, "ee148675-b135-4828-929e-bf31b2f79e5d": {"doc_hash": "385176b6cdcb7a202f1937b14dd7b7d4ecb93222d4d2527c52e37628d188d342", "ref_doc_id": "63428642-3509-467a-ba84-47ea8d8e92ad"}, "bb31603c-5f65-4ed5-aa47-f9e969bf9ef6": {"doc_hash": "f740fe9fcee19f54a4c7c1b3ebeafb44d04211d3d5973ff09cc844f475a887f3", "ref_doc_id": "6313a2c7-5be6-422e-a676-771d1ba19f96"}, "a600ce7d-addd-471c-b829-f99b0c11c8e2": {"doc_hash": "2859de60a00e691649bcfde4d7e4847b20af7c32afc72c0030bf8c386f94836d", "ref_doc_id": "6313a2c7-5be6-422e-a676-771d1ba19f96"}, "099098be-9ab2-45ce-938c-0f958a7ee92c": {"doc_hash": "0a8d0604558c5d810c5f110bff1c89d408b6acbba4efdc746ea5e9435d9c6eff", "ref_doc_id": "6313a2c7-5be6-422e-a676-771d1ba19f96"}, "740189d2-e803-49a7-b6d5-8b8d294da86b": {"doc_hash": "40e38d6ddf6b856a6c236752209a9b8835d3400ccba8f29e6a992c3b8859d1c8", "ref_doc_id": "6313a2c7-5be6-422e-a676-771d1ba19f96"}, "c138bc0f-ab83-4dc8-9696-71cd77552027": {"doc_hash": "1e8e55a5f119c50b95dacbf72977523c2a4fec40a771723df27cd42fc514a162", "ref_doc_id": "6313a2c7-5be6-422e-a676-771d1ba19f96"}, "9549336e-1d13-44b3-9425-b6211314e0ed": {"doc_hash": "53625fb3b0ce2cd1be9bb015af33c128ff1b867b9a6981ad30d4999f151e9c48", "ref_doc_id": "6313a2c7-5be6-422e-a676-771d1ba19f96"}, "bc912460-0b4d-4998-8ab3-8b241449f455": {"doc_hash": "9c0d6cb15471af98ad0f4031d1120ea6c3827f01e6dc2d92ddaa6b10f2019e95", "ref_doc_id": "228687d9-185d-42d4-bf1b-6a0ebf05ed48"}, "9269b9de-9c68-497d-ba6a-bb26a2269dfe": {"doc_hash": "c2f031d78a0453d07c4aab0d3edcd4111b8bc66434c704d0416a4567ea4bf720", "ref_doc_id": "4d41022e-a1c8-48c4-8925-cd58306a3947"}, "aa7af594-4220-4ce8-9c84-a5c46cb526d7": {"doc_hash": "7dc59e3807c7491158e11b26bb82c58dff5183a7f02ba6c8283276fbc9ef7387", "ref_doc_id": "9e6772e8-479e-497b-83ce-b5728aafc463"}, "1086f5af-2027-46dc-9976-a84b4b4c8f43": {"doc_hash": "84ffcfc55c8d8aa94c8692f42be2a9f18671e4fda4128c3097d5289306cefdda", "ref_doc_id": "d2ef52d7-8f67-44d8-be0b-1539687c0ef2"}, "36fa2c74-d7bd-4a0a-8b97-5d088a6c25c6": {"doc_hash": "4c63b9bf3a3398298fe5c927479d6b897bb7b350c8cfa44e7cc91d26c1612eeb", "ref_doc_id": "2f4609be-679b-454d-9662-bf69d41bf921"}, "786b47bf-75a5-4fc9-b93e-c51f6919985a": {"doc_hash": "4ed397ca53bdd49e0ad9baddd766769797269a4e0f02e1bd3c5522c6c7dd746f", "ref_doc_id": "e4597f15-99cf-435f-9a04-ed4c844c96a3"}, "de632f38-0ca3-4a6f-9b3b-b237ce14ed74": {"doc_hash": "3539f711eeb60d45652f383963be33a151f1adfc63f08c4d2221f8167676c340", "ref_doc_id": "fedf7284-8bd7-449d-8583-15c4741e9a40"}, "08478107-c0d1-437b-8c33-b8d84b1e647f": {"doc_hash": "8d03f292ded06b3768b780dd1a69419a5a4fd8885a5e8a7b49238c0199b98602", "ref_doc_id": "15ea1a97-9daf-40f6-b912-518052c443f5"}, "6b889bf1-fd55-491c-ad76-7c279aa4f7c6": {"doc_hash": "c3bb0be49603c3fd93cd78db75460134cd304b880c2f9d756b8a935c6411500c", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "c1b74f2b-e4e0-4e9e-a306-aad72168e436": {"doc_hash": "6527aae322d7acab8df74960e4529dc67b1a5801108cf8e143ed64d4b88332ac", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "5bc47470-bed2-4eb9-a850-352d5923ea7d": {"doc_hash": "236864ebeb144e4570b386d9a8a8f7f206a1173adfc24023023a13c95e321b8c", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "5fa2089b-4515-4f60-ad2e-9e409a4ad72e": {"doc_hash": "e64e435e0379f7b1a9d2532cee0a00901bc383492a3035f19f64a72ed8fc1c12", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "de6deba2-c3f5-41c7-8a27-9cdff1a5ac7b": {"doc_hash": "06dcaa2f2ee15453927e93cb3964cbb38dad3acc50c9481bb2a245a9893cf631", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "75b63164-962c-48de-8378-246464d714ab": {"doc_hash": "238819c9a79260d0e77d4f6e2d9471132edab730f4a27c141c0b54bded6c20e1", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "2907a7dc-a527-4850-bc63-db7a68f6abd9": {"doc_hash": "7f7cec0c7b55070075f14dbcb6114229c81b82ad8322c2d5345040bc7fc2dbac", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "37d3d69e-55cf-4f4e-b4db-238afc02b8b9": {"doc_hash": "b177e730882df76889d3f6ccbfb4dfedb191905bf9f1f4fbeb4d98c56dd890d5", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "2fbb8bb7-fe2a-4b31-85b4-1c544de51090": {"doc_hash": "c21c5e5f21c0f4e3686f7ab2bb6a28b1daf58a7f9207545c5d8d7f4aee32914c", "ref_doc_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd"}, "3039edf2-acef-4655-b867-bb6228b5042e": {"doc_hash": "29c37eb01b0d438a279d9ef00cdb2c14dbbf81d83b03c5bbd6fca6177d8df531", "ref_doc_id": "a49a291e-81cf-44f8-ab6a-29ed0d9e2a7e"}, "4f7eb92b-6b71-45b2-a50f-364f6993b98d": {"doc_hash": "78037930b800bf84c3d3ea4596d2a2a936521881b337ff4e566b2293eaf257ae", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "01649a08-dd5b-495f-aa60-8a461196d3a3": {"doc_hash": "7e0d284fe3aa6432225aa5dda2339551db8758bfb1783e7dbd1cb603c792db79", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "06344451-03dd-4161-8b01-059e32e1ad43": {"doc_hash": "f06718b913d66e2bc7b9fa051a5f3c9a07d85e0d08429022f0dffd4c02c032bc", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "c99e8179-bdca-4610-95d2-69e7ec922d5d": {"doc_hash": "f5eeb1eb57ab205d7b7ab27e790dc5182164e74bb81b63bc70e746b88ae76ed1", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "05d4e123-c41d-495f-b941-c0978c7d90be": {"doc_hash": "973ae8157009270d18ab3b074ded370a181a956fbcfded3262e593fceb7261c7", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "4ddad4dc-19f2-48c5-819b-34739d83325a": {"doc_hash": "54d5eb0e25b80e6dde8d392ab7dd0f9614252711a887d3bbefd332f96d08c5a1", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "16010fc3-becf-43c1-b25d-100b868973fb": {"doc_hash": "a0b0c220c8cede6d1b5ecc6f5c65cc1c649968c7edc3e93eb2cb70200702fdbd", "ref_doc_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc"}, "f16d9244-dcc6-412b-ae05-a0f8b79c18ff": {"doc_hash": "5ed841ae8f630d828930f8bfb7a8fce1d5f56067a8288edd94e7631c87da04be", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "ba3d462e-0434-43a1-959d-7b7bf1ee7491": {"doc_hash": "6000420b63b5ef9dff9bfcec6725b5920248c71cff0271c9211e9c516845bf49", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "be2af7a5-883e-40fb-830e-eb0635b0f400": {"doc_hash": "c9bf4880866f6f3d94464b6c628f9903230588e1e06a298f7711c24fbd3438b9", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "d2a6678a-3dd6-4775-aa08-c51ae83b4734": {"doc_hash": "243ea7888c1e781043e38e4afcb9e318b2a2e2872b0eb9aec50672ec92f4c440", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "6f46ba24-327d-413d-a6a0-fc13f01f61f4": {"doc_hash": "a8c297806f3555b0216bedd8679f5512dd2f21eba6dce61a26297cdca62a100c", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "ce70a585-6e18-4f30-b5be-ea101e738c13": {"doc_hash": "15a65b65e92e9f87fc5365addc06c3f4a6e73e5cc8b9a36b6605fa6d0852f475", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "b8ed2b9c-5b6b-48d7-b4e9-cee69dd75c61": {"doc_hash": "f566f64d8d54dd2a0537ec50412271f89c29f1ef3d448a98e78aae21921eca5f", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "e78a53d3-a097-42a3-9d11-8e7d6d4539a6": {"doc_hash": "629b07c553af9d3f8c0fea8b2369f801b3b8ac2a552d2c937355eb859ddc1f56", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "e1eafa26-9e1e-4623-a04b-ede8deee9484": {"doc_hash": "b76a8722f7715cf6afaf964a0baa4567c8cb0df403cd708a25a8d9c949b1366b", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "b8b5a20a-fa76-4984-b285-fe5fd39a4ed6": {"doc_hash": "487ae9fbd24a397f828ecff540d31d85fd5c7df9084193a112312a8a64de01e1", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "e75b966d-b1ee-49ae-abd0-1f58622f3e37": {"doc_hash": "ce11ecdb77314325d1d04be540f9dc9c8d0b614d7ffe1726a8264ad67cc5199f", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "fa391b96-75df-4892-9890-35d1308fdfe8": {"doc_hash": "24eab01c299fac1f4f1652940327c348e7b9623a65f7e2e0c53dfbb038849ad3", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "d351b4fa-bb02-43e3-a9a7-bed30990573f": {"doc_hash": "63a7f75851c727b3c6756b1acf6f1f19da87fe3f0721b4cde282d014a94f02e1", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "37fb48ae-48d8-4988-b14a-664f0745d96f": {"doc_hash": "726d1b1002ee51dcdb488148a3d309263bfb077c6f4715c95b50f0c72879e01e", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "75378eb4-acd8-4eeb-a183-65f23b9637cc": {"doc_hash": "ac6f471969060ef90fed9aceb209e6912eed45cef85c752fcd4123e489811985", "ref_doc_id": "60ade05e-ea05-4c4a-923e-714df5e6b162"}, "c42224a6-c112-4000-b057-66731c528d69": {"doc_hash": "ed36a18ac1001c375322c58738964b9ce52c26ebfc854aeae6efe6df0e0b4eea", "ref_doc_id": "af4db40e-101d-4953-9ef1-9bae0a723f27"}, "82fca539-d612-4664-b565-99467442cf4f": {"doc_hash": "7c91a55e59375039ba6102a416d7dfdea03ea47e74625fc79863d80355ee3b2c", "ref_doc_id": "40130f56-0714-4ad9-9a60-30154ed89569"}, "3eee76be-6875-4ed1-92a4-1ae0f8799f3c": {"doc_hash": "fd1d3252e19bbdcd71957371b8c266dad95935387b886c6a1ec8aa68f7c36cca", "ref_doc_id": "25a12674-4683-44b2-90a8-fc3ca4814280"}, "072c5eb8-d4c3-4cf3-9220-02b33f813271": {"doc_hash": "f7524df609de8abd10568bb53694eb12df54d72ff93297941dbaefa6835c19a5", "ref_doc_id": "3cab912f-730d-4daa-82b6-c812d016f809"}, "a47dd41e-f7f4-409e-96a0-1c74f339b9f9": {"doc_hash": "564a9cd67124219f4c1a22fb852f0f31bebfe324df3c2b5a4e68926c0ef0ec95", "ref_doc_id": "723f36c7-767e-4956-986c-7c26a50a3958"}, "d275ded3-2c1c-48de-813e-a9f529828db1": {"doc_hash": "adbc2c37b52a5515689deea9f18bbda80a23d3aacdd6d810245d1259df9118da", "ref_doc_id": "501117bb-59b3-4f0e-8c4b-d9fa547ef48e"}, "602a8439-a01d-48cc-ab4d-44216bade9d0": {"doc_hash": "e78181f144427aa2ab3f0afc25e051596a0a143d05d813638df8c22f972e9c99", "ref_doc_id": "9a20f4ac-f1fa-441a-8512-1f1fccb73a5d"}, "f2932a65-667a-4131-ac06-896b1377cda7": {"doc_hash": "380490b380a5a4564028b14fbcaa08fe891ef20099da9d3ca2c0d9dffaaa99c4", "ref_doc_id": "906298b4-363f-467d-bb6a-e30c9f7a047d"}, "95070a48-5cbc-4949-be75-e520eec6a29c": {"doc_hash": "f1cd957f7d21e88b75049862028c0d717b170ac936b915cb17c1e84576b27d8c", "ref_doc_id": "4fcb5c77-561a-4bf9-a03a-b9960b0a6e8f"}, "90f77b72-e08a-492e-9f57-54a84f7ae51e": {"doc_hash": "8a566fb85272ee03d7f84b76c3ce9d487753252885fcdfae965c86a9ae78b31a", "ref_doc_id": "8c122559-c8be-416d-a1c5-36181ac1ee65"}, "9705f8f0-5fbb-43b5-9cd6-1765d1a5fe80": {"doc_hash": "5b8a620594bcf783d998027efc907053e8fd52f69dabc2e68f75d7bf7dd75209", "ref_doc_id": "122baf94-a4e4-4a13-b77c-03ff283b931c"}, "760e0fe7-7e00-4f44-a628-453262461e91": {"doc_hash": "f75b3ba58e2f29bcdfc4d9e2ce9db5726e62740245507338733afd6ff961a244", "ref_doc_id": "94880efd-8fbb-469f-9dca-545dd3277949"}, "b167d900-2bfd-4dd4-a2d3-e7cf546af900": {"doc_hash": "d60e876710d524802741e2cf72a63312193df28658b0b66fe4b2383475dea23a", "ref_doc_id": "08354a60-7e6d-40d4-a8cb-37270b552ef1"}, "c0624af0-9600-4752-8d8f-82a1d8da62c1": {"doc_hash": "210448b6f0c0c8f32ec5e18b1fdea8bd35b084bdb25bd75936dbd3dcca6c2879", "ref_doc_id": "08354a60-7e6d-40d4-a8cb-37270b552ef1"}, "be6cf76b-2076-444f-9022-f723ef281940": {"doc_hash": "95eb7388ad82f14675da71b1792481f188a6809f3f044432af9b0627e823132a", "ref_doc_id": "d9c3b170-0bbd-4182-be76-ff3b15d7cc91"}, "eacfa080-5667-4bdb-8293-a56c66005fe0": {"doc_hash": "fcd3a7985c6902892f43520f0177b8a6cb7561e4b58d874e4070ea62714e291d", "ref_doc_id": "af821be8-f438-40d9-b136-b205dbf07270"}, "10b3f876-9d79-434d-8359-db4fbf1bea18": {"doc_hash": "655290440a75ff5db769d97b1dd42ee2e52d8168674ec2efdb31dd4c1a37c185", "ref_doc_id": "add56e7b-15f4-4e48-b9a3-e63af372e931"}, "21aebf4d-30f3-43f4-8800-e4adb871a059": {"doc_hash": "2ff311751c5c5c8aa59cba159637a2c39d117f1f7f48c077c8499179b4ac9d2e", "ref_doc_id": "25decb75-1742-45d0-b05f-f433d285fc43"}, "4543bc8a-81e6-4642-9d42-34357ca621b2": {"doc_hash": "00649f81871a99d27d27f9ffc34a0fa1c9df2109dce66a12a5e219cda585104e", "ref_doc_id": "a4e2410e-cb0f-48de-b8b4-0bb4a201f854"}, "cdfe54d9-4e53-4637-8812-d5c5457aa426": {"doc_hash": "63d992f3efb6d9f60765058ab9fd66dbf9ab25d5a621b4966111b63f5e85f649", "ref_doc_id": "e7b8cb34-e822-4487-aa4e-f0530099b8b4"}, "7974f9a4-5225-497e-b66f-9cb7668ef9e6": {"doc_hash": "9ea3729d4da871a8a8c0f13f4f1a4bebd416b5d3c11cd18bab6885cfd55f69b8", "ref_doc_id": "61dd2dff-57e9-42e1-b727-cadc5badbd9a"}, "3c2525a9-8b6e-40dc-85de-1d8e056c7a21": {"doc_hash": "43b822fddbf0406bf37c986b74f2ec388a29a75d60ba37448c143da10dd6c769", "ref_doc_id": "61dd2dff-57e9-42e1-b727-cadc5badbd9a"}, "8edde0a3-71da-4dd3-826d-021ad09369f6": {"doc_hash": "9d785fdf6b5ef0e54ab2cf7a3bea062a5d27d00a66c9a94f15bf9b587332ec58", "ref_doc_id": "48947c14-e046-4c7e-965b-ae53e9da43ef"}, "7a0a7bdd-02cd-4a7f-ba4f-e19f7b6ebdb1": {"doc_hash": "65c427ac9e97d3b778a30c45d2f4aa9568b10eedb65c6f8f90f78f3a6ce6878a", "ref_doc_id": "60f7fb2d-59b7-408d-b53b-2563e901b26a"}, "b7d1dce6-73de-473f-9321-ab6e56218c85": {"doc_hash": "f3cf356036f4a56c6999ed1c4543f1a1d4a5815d6e6a34091dfc1b149363f984", "ref_doc_id": "ca6985b7-73e4-4d36-abb0-0f7e317f1196"}, "30d4c0bd-28ae-4e72-b345-09e13a88fcef": {"doc_hash": "490a56d30ab61b597d167ba3d3a2848fbf0518890e674ff50b6efb050007b965", "ref_doc_id": "a5f58532-b563-42cb-97b8-180547eae720"}, "bdc5a975-4138-44dd-8968-f7cd77d8c145": {"doc_hash": "ebf54e3c4cbac4bdd591e26cd0f586cd72d030630060ae2c855a6880dc06732a", "ref_doc_id": "5047b9fb-a464-4744-ad60-6223e2b36fc0"}, "fe285b24-4ca3-4c1a-80c2-fd9fca7d6d02": {"doc_hash": "5510adb4ccae87b59d102366a1562e3bd3204c61063e84021e204773c437be62", "ref_doc_id": "4a8bfb97-6ec5-43db-b980-1dc2c1ac6544"}, "e8cc1d8e-ef60-4ce4-99cb-683bf811dd45": {"doc_hash": "2cb2ef374b8567c01f4509510765f314a35139525218b406f306817adad76f62", "ref_doc_id": "1bc7ae7c-fd0b-4120-a5a1-44880b1d2f43"}, "027932e1-9599-48cb-9a5c-35a493fe5cc1": {"doc_hash": "ac26338c03f8a0aa67d7f37d73ac326cabf694203872c5b1c27f76fe93fe07e9", "ref_doc_id": "2a310b6c-d302-4fda-ba62-15c8d623ed6e"}, "ebe20b30-9ae6-4253-adbc-027035bf395b": {"doc_hash": "64f8e43fc29f9b8360ab93b2ad3e0d381f4d62ccb207706adeb47c57f4b756e6", "ref_doc_id": "d872e017-c394-4b92-85c9-fed2a7d72483"}, "27452635-9ecc-4605-a4e8-e82428b5ff51": {"doc_hash": "f1b78d99f2c952d5d9d8784ec79e0d58323ac836c0d33022e83090c519711d79", "ref_doc_id": "f057991d-4465-4fcc-b9d5-4afbf680e892"}, "335aebce-6f88-4b1c-a55a-ac2b7c5bba62": {"doc_hash": "086c8d13bfd595e7e86ed189197705ca8c96683a7fde6b076b41771e90810867", "ref_doc_id": "f057991d-4465-4fcc-b9d5-4afbf680e892"}, "616a6886-8d9d-4792-a214-501ce218c32f": {"doc_hash": "e376f8191d33c21e3d59ef54bc1bda68e785461499c6cb72b723369ffae01319", "ref_doc_id": "f057991d-4465-4fcc-b9d5-4afbf680e892"}, "bd3ec216-1bd5-4277-8a3b-596ce5856833": {"doc_hash": "81de5c22e52ef600e8b2ad0f9ddcc9cbd99d5c66cf935cd08508bbac37396df9", "ref_doc_id": "b25d4a08-14cd-4e16-86c0-cb0ad3112787"}, "6ecd05ea-d5b6-4472-94f7-96b40c20afac": {"doc_hash": "7707dc6d42edecd0553aed7d9deb2c2a17d534215fcd67f0925d134f415b226d", "ref_doc_id": "0f5d66d8-7b45-4e0d-a988-3ef1de75b0db"}, "cbca2355-9e03-4578-86b4-d69eff1bd261": {"doc_hash": "178a28a4406533688b7206b831cd3e566545837428cd025e4aac044f4cbc1a45", "ref_doc_id": "3c564ca4-ed6a-4aa7-81e9-4d09d27db662"}, "dd7e5132-590f-454f-8a8e-0fa1c18fcf2b": {"doc_hash": "55dde9d4f8b816d5b85c637b6ab37df2d089ccc1ceec9f57fd9c9f08c6d1e15f", "ref_doc_id": "3ae318a9-5e4c-4570-ba7a-016dc0483cc9"}, "19bc7068-b965-4215-964d-47d2ccffd2a3": {"doc_hash": "169808f7546eb35363d8c2e50c663aeda8f2d434f572b2f64b1dec714e7f5aab", "ref_doc_id": "95c64dc9-e4d0-48a0-a35f-8cbd0295e49a"}, "3d7ce476-afb9-4530-8ec3-60b33d664879": {"doc_hash": "d1c578444d0b8b61443f9057b04274cad5b6db198740c4e4fca450d48a255536", "ref_doc_id": "95c64dc9-e4d0-48a0-a35f-8cbd0295e49a"}, "0fbcc06c-8e51-4b83-bfb1-1e97e23d06a4": {"doc_hash": "6fb77ca5597277228d90a09254c750ba8ad20c4dc8cc164bb1396e1d0f603918", "ref_doc_id": "c0b3bf09-dfcd-49f7-84b0-d4c2d938405c"}, "bf5a925b-fb04-4948-bc3b-e31c0cffb13b": {"doc_hash": "581820d72ecf238b33f62cd4a5c1ad5f4a8d58bfb040e91b2a5e12a9200be272", "ref_doc_id": "9100a7ef-bb38-4af8-b781-2a2da09b770e"}, "832789b9-32b8-4748-8740-71a63a7212e3": {"doc_hash": "9bc34a2d0149ac665a7bbc98807ddbb2a02eb34e5277d2d4d28c90b9e81b1da0", "ref_doc_id": "80e1e671-52b4-4881-8395-75c43e1f5873"}, "81b72515-dcd4-4464-bf2f-2684154095ee": {"doc_hash": "60d653548d921ab015eb339b4a982d6df64f50839e236a4b55ea31aa8640ac73", "ref_doc_id": "b18e20bb-2c35-4550-a71a-a0eeb94fb2f2"}, "7648935b-7a08-4c30-a23f-d542b50f87ab": {"doc_hash": "a8b936102500876188e26eccf4f07846965269912864f9757200139378ada630", "ref_doc_id": "f76d1e22-b47b-4e06-a0b2-7b42081c8e69"}, "71beb6e9-5333-428f-bea2-84a500615725": {"doc_hash": "626a97e285a99524d64289573d2b9bb7fcc75476c2790dd6580efae829ff1f0c", "ref_doc_id": "01a714d9-e060-4d29-908d-1e12c2ef9f19"}, "1591a5ac-8621-4e2f-a312-4f2beb74fa2f": {"doc_hash": "83dc35beb271d1aaf6e24f2db7033d6c67e798fdcdaec63f029df4fe667ebb41", "ref_doc_id": "2230f129-480d-4a25-8cbd-5f93a11c60c1"}, "ab46a04b-cfe3-4df6-bbd2-fa6288e9dee0": {"doc_hash": "9b6978f76458622284c1afb925101edcee8bf8dc5ff4ae4c53e2155ac8353fbd", "ref_doc_id": "2878648d-faea-4d33-984f-71bb6b8d2481"}, "c8b84100-1703-4246-828f-dc6f642bf091": {"doc_hash": "c5b8c719555ec4adc1c7cc6ea081b1e20ceb1a7c98de3ae149463ec936bfb910", "ref_doc_id": "7f7bb37f-c1b1-49d1-addc-c75f8024b6f3"}, "6bd1fadb-3633-4415-984f-a37f69b7b86b": {"doc_hash": "bb483df94ee3b589dfc97697b9eff033934eeda24246575bb729dd0824a253c3", "ref_doc_id": "d8e4cac6-b7dc-4a85-aef4-f4f2dcbc7251"}, "0847261c-16c9-4f84-9390-0b187f15d5b3": {"doc_hash": "c52335ce5241774298e2eb9af534afa7aa0ccc05c52d565a108bd8d8b2abcae1", "ref_doc_id": "9d19ab6d-0912-46f6-8a4d-310f45cc45cb"}, "3fe7a9ad-7333-4c02-b0a8-3f326e351a47": {"doc_hash": "c5264be063c2bb4a872567db87a6c6bb3d8de46414c7261b31503953ee70a322", "ref_doc_id": "9d19ab6d-0912-46f6-8a4d-310f45cc45cb"}, "8ec304d0-3da3-4f69-8c06-81a7c0e073bf": {"doc_hash": "840087bb7424fe692da431a77f5840b4880bf34f9ed047483516571a7cedb8c6", "ref_doc_id": "9d19ab6d-0912-46f6-8a4d-310f45cc45cb"}, "3acd92b5-55a2-47f0-b82f-937fa5d93cc2": {"doc_hash": "0e2d3590e8bc6713b8ae7144faab959f34a059a752815c27859109cc8ac8ac83", "ref_doc_id": "52dec9ab-f3e4-4763-9cd8-d383f66aedec"}, "82e7b984-c674-4519-b6f3-ec26b5ef15b0": {"doc_hash": "018c7dfabd6db9ee672bbeb9eba8532812ec960a2ace2b9a6afe3dc457798a04", "ref_doc_id": "1474d637-b80e-4c6c-8a72-79247cdb139c"}, "adb003e6-404a-42b7-acd9-8091e0915c89": {"doc_hash": "6a733bd00accf1d3d0cd371f8f96831c531bd453c2c52dbfdbc0705978ba5a85", "ref_doc_id": "c2b741a4-1417-44b2-a4cd-1e0b040c3b87"}, "2b8b6c74-f610-4a30-9510-b50dce0e6ad0": {"doc_hash": "d5f171abfb96ef4584b89e025f4a0cc922dd992fbb602765bdd1ed13ecfafa45", "ref_doc_id": "ba841022-58ec-44ca-a43b-39961be87b33"}, "005d0d86-2d03-48a7-b96d-b0a1e3ec0464": {"doc_hash": "25b22840f8355975e1ae044b309ce5d5128918321cd087b84484f505aa1e9e20", "ref_doc_id": "84ef711b-781e-425b-9aa4-c5587f9b95da"}, "2fac5f96-bcb2-49c7-8c8a-590981146891": {"doc_hash": "ba6cfb5eaf2272122c60d1bf09a1911e6bf65f218e9c2e31317e08f71c3ded73", "ref_doc_id": "7e6eac33-6916-41b4-8be7-1a725f838684"}, "c4b5b2e3-99fd-4527-a94c-a983ed5225e3": {"doc_hash": "dbc1e3f3767a789fa3549ce2de86ffab38105c120498855db6fcaa317cd32346", "ref_doc_id": "f3ee54c0-04da-45f1-9329-437b41468049"}, "24a13d41-d222-45da-8048-f5b2359dd043": {"doc_hash": "9baf7acdd9fd40e7b657933f7eb6452fe1f169d6da524840a2d501d0660e8b1d", "ref_doc_id": "4b1fd6b4-7d4e-41fa-ad42-a9fbe68f7bf2"}, "c78bb835-c426-49e4-86e1-2fde5db2e31f": {"doc_hash": "e03b633e006472c078b6a705322212b8f4a58dae1499942b240559df82dbdff4", "ref_doc_id": "30dcdd05-eeb3-4645-a18c-d36473030b1c"}, "9efbbbe5-3dbd-43aa-bf38-e07d6846062e": {"doc_hash": "4ee7188b450dc44307157ffe110aa5820925ce7ed253f7349950db3aab99a894", "ref_doc_id": "1cea5fff-9f8f-46de-b7d9-43f9d831eefd"}, "62895b8b-0db8-448e-bb80-6ba652d21441": {"doc_hash": "78c8ce5d08bc457aaa006e7980c63448f2464eebe3741f4d967ad037318f9cd3", "ref_doc_id": "b1951614-9d46-45d6-9977-1315fb47372e"}, "ac3585b6-c617-469b-8f97-918b88a25e0d": {"doc_hash": "ebb83b25b943a884f10df245a0bd0b6cfb0decc8f1fc90060044f24fcde4aac9", "ref_doc_id": "d9f7c5fa-4284-407e-b043-f602c5f22629"}, "49d3f981-e034-4fb0-aa02-7ea418171032": {"doc_hash": "01cbbe2aec327756a051b02ab49363a61e7fc70400a015d02995aaab13c16992", "ref_doc_id": "d9f7c5fa-4284-407e-b043-f602c5f22629"}, "7962e109-337f-40be-b319-4d3b447d5807": {"doc_hash": "6e3abf23ab58f3d2fa80bc6b8072ae41d9d08315d4e22ad8589d4403b70134d6", "ref_doc_id": "d9f7c5fa-4284-407e-b043-f602c5f22629"}, "3d790a79-cff2-4554-8f83-919bbb75db5a": {"doc_hash": "7caa9fd36fb0fb301d15e3e80619e57565d593467ac0ef0e9b77befef334b30e", "ref_doc_id": "e0d3428d-f1d4-4954-9770-b1acb9f38528"}, "ee1f883c-a7ed-404f-8177-d4360d4ac04c": {"doc_hash": "ef49e11b263ed4b8e803da839dd507e569278f8a7fe3db2c5d56c19d42dba5cb", "ref_doc_id": "92f96cc5-fb08-431c-8a82-2243eba56518"}, "ae0a7dd8-543a-4a38-97e9-ef9bf190fd8f": {"doc_hash": "a10705b90bb050941491631d506e6e3fa05fe6aa18b449784c4bd4bd91885bb4", "ref_doc_id": "e3f87ce7-f81a-4860-9272-74316bb0e0fb"}, "d22d01d8-0e3e-46d7-a880-0382d7624d37": {"doc_hash": "f8b4e27ae0e96ba4cac77ab1dcca436ba6b0236f068949cd5e94d30023c489d2", "ref_doc_id": "51a6cfaf-27f9-47e8-9134-bb649e724485"}, "d94545c6-1461-4d44-9ace-e676c17bef44": {"doc_hash": "540049e16659728ddcf8a42badd4866a2b10183e4e755495d209c128a63f0456", "ref_doc_id": "51a6cfaf-27f9-47e8-9134-bb649e724485"}, "5d9cb584-ffc7-4acc-ba26-4c50b4a0da62": {"doc_hash": "8df4a79237017956c7c033b7770dbc8ca22c3a4ffae0544aacc821d09bedd018", "ref_doc_id": "51a6cfaf-27f9-47e8-9134-bb649e724485"}, "e0b732ec-9a87-4e74-a73a-933ae89d0aed": {"doc_hash": "ebc8339a16ba6f32602808a74bbcad38408d59a74f66ad1fabb6b98577555d12", "ref_doc_id": "a703a98c-0cf4-4245-ba39-b360db3499c1"}, "03f1a526-d53f-4f12-8f1c-7806f0070334": {"doc_hash": "0c9fc6298c30148867ae28f917ffafe9c54212fa8eaa33029293e423d548a4a8", "ref_doc_id": "8469d484-e5c1-4c43-a63c-540e9f6ce250"}, "4bed57c7-f2a4-42ea-8fbe-c9ecc91a3189": {"doc_hash": "db8c30004dec67007988fb54d04fe48e3bf4f38d44190ae182b95d283fbaa55a", "ref_doc_id": "8469d484-e5c1-4c43-a63c-540e9f6ce250"}}, "docstore/data": {"ef36562f-401d-4d3f-8819-01bd317f38ac": {"__data__": {"id_": "ef36562f-401d-4d3f-8819-01bd317f38ac", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\0.txt", "file_name": "0.txt", "file_type": "text/plain", "file_size": 632, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5e292d2-a134-4a9e-86ad-69dc8cda1d3d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\0.txt", "file_name": "0.txt", "file_type": "text/plain", "file_size": 632, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5974a82fd70a4841a55dd2f2b6062157b4ec5b7364617b6d6d8cc4778b2f5d91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1c41615-7dba-48c5-b7bb-e0278326c20d", "node_type": "1", "metadata": {}, "hash": "c345491cb0565e8960c3c82c5bc0c66ce26b9e2ea93168d9a059080b3c6f76f9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: datasette\r\nlink: https://github.com/simonw/datasette\r\ndescription: Datasette is a tool for exploring and publishing data. It helps people take data of any shape or size and publish that as an interactive, explorable website and accompanying API. Datasette is aimed at data journalists, museum curators, archivists, local governments, scientists, researchers, and anyone else who has data that they wish to share with the world. To use it, you can install Datasette with Homebrew or pip, and run it on your local server or deploy it to the internet. Datasette Lite is also available for running entirely in your browser.", "start_char_idx": 0, "end_char_idx": 630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1c41615-7dba-48c5-b7bb-e0278326c20d": {"__data__": {"id_": "c1c41615-7dba-48c5-b7bb-e0278326c20d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\1.txt", "file_name": "1.txt", "file_type": "text/plain", "file_size": 751, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a33872ec-8199-4568-8cca-87ca03333326", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\1.txt", "file_name": "1.txt", "file_type": "text/plain", "file_size": 751, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4b57432aa26d691093f028c6f1835ac59299fa1901a1b92e2271261d70a41580", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef36562f-401d-4d3f-8819-01bd317f38ac", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\0.txt", "file_name": "0.txt", "file_type": "text/plain", "file_size": 632, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "399933a437acdb34290fd1f4951030558a04395b3d64cfcc7dcb2cbd701ba048", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "879aa659-c616-4665-848f-3b89ae161684", "node_type": "1", "metadata": {}, "hash": "ea54460e2838d1eec12e4274151e589fe2f3c707b950b0c6a11db04090d051c8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: nltk\r\nlink: https://github.com/nltk/nltk\r\ndescription: The Natural Language Toolkit (NLTK) is a suite of open source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing. NLTK requires Python version 3.7, 3.8, 3.9, 3.10 or 3.11. For documentation, please visit nltk.org. If you want to contribute to NLTK development, please read CONTRIBUTING.md for more details. AUTHORS.md contains a list of everyone who has contributed to NLTK. If you have found the toolkit helpful, please support NLTK development by donating to the project via PayPal, using the link on the NLTK homepage. If you publish work that uses NLTK, please cite the NLTK book. For license information, see LICENSE.txt.", "start_char_idx": 0, "end_char_idx": 749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "879aa659-c616-4665-848f-3b89ae161684": {"__data__": {"id_": "879aa659-c616-4665-848f-3b89ae161684", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\10.txt", "file_name": "10.txt", "file_type": "text/plain", "file_size": 599, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf6106e0-f8b6-4fd9-8110-83752bade113", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\10.txt", "file_name": "10.txt", "file_type": "text/plain", "file_size": 599, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eeb501010e2bf7c186da516dcfb523ecf9ffff8c2dc8ba76aea9eff65fa7bf15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1c41615-7dba-48c5-b7bb-e0278326c20d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\1.txt", "file_name": "1.txt", "file_type": "text/plain", "file_size": 751, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f2de9164b8ae056b2a441eeb842ada9fc450cce62f5d42b00c712764c140cd6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a845ed96-cdae-40a0-922a-1b83f8d38709", "node_type": "1", "metadata": {}, "hash": "ae590daed862600a2a521e508d327e8f7d830c5508f1aacbb52666138ed3ae73", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: dBoost\r\nlink: https://github.com/cpitclaudel/dBoost\r\ndescription: In this paper, we describe a tuple expansion procedure which reconstructs rich information from semantically poor SQL data types such as strings, integers, and floating point numbers. We then use this procedure as the foundation of a new user-guided outlier detection framework, dBoost, which relies on inference and statistical modeling of heterogeneous data to flag suspicious fields in database tuples. This repository contains our implementation, publicly available under version 3 of the GNU General Public License.", "start_char_idx": 0, "end_char_idx": 597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a845ed96-cdae-40a0-922a-1b83f8d38709": {"__data__": {"id_": "a845ed96-cdae-40a0-922a-1b83f8d38709", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\100.txt", "file_name": "100.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38b42cf6-0149-4170-a208-4b53d527d9d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\100.txt", "file_name": "100.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "532026303d170cfa98da189aa01cdef1e60fce06a587076169b97222df9fc63f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "879aa659-c616-4665-848f-3b89ae161684", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\10.txt", "file_name": "10.txt", "file_type": "text/plain", "file_size": 599, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8c84cf0281b160b4e5972ee172fd7c5a814d77d5a91a7c60494623a9af8248d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df31eaea-ed63-4a41-b411-842b54ea0ec9", "node_type": "1", "metadata": {}, "hash": "57f5e5d76b56610b712d64f513e7c91f2556dd38f9fa77f49a7e3d0b730f3f3b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: influxdb\r\nlink: https://github.com/influxdata/influxdb\r\ndescription: InfluxDB is an open source time series platform. This includes APIs for storing and querying data, processing it in the background for ETL or monitoring and alerting purposes, user dashboards, and visualizing and exploring the data and more. The master branch on this repo now represents the latest InfluxDB, which now includes functionality for Kapacitor (background processing) and Chronograf (the UI) all in a single binary. The list of InfluxDB Client Libraries that are compatible with the latest version can be found in our documentation. If you are looking for the 1.x line of releases, there are branches for each minor version as well as a `master-1.x` branch that will contain the code for the next 1.x release.", "start_char_idx": 0, "end_char_idx": 801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df31eaea-ed63-4a41-b411-842b54ea0ec9": {"__data__": {"id_": "df31eaea-ed63-4a41-b411-842b54ea0ec9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\101.txt", "file_name": "101.txt", "file_type": "text/plain", "file_size": 967, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "351b7993-f448-4ecd-8635-0288100f1c35", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\101.txt", "file_name": "101.txt", "file_type": "text/plain", "file_size": 967, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ade589436dbfc8eaa3b5966afb4c219b7afae9acc459c7d828157bb34881c676", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a845ed96-cdae-40a0-922a-1b83f8d38709", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\100.txt", "file_name": "100.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c58ee3ec0c6194bacd223151536223c51dba40fccf4a6ae54b3c77c0c30ba3af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a9e1df7-06b5-4099-b55c-b2b8917ab711", "node_type": "1", "metadata": {}, "hash": "527d14a502fe0f7f10e66d04c848d17a4b00fb39c023e5a78430e010576a002d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: edgedb\r\nlink: https://github.com/edgedb/edgedb\r\ndescription: EdgeDB is a full-fledged database with a powerful and elegant query language, a migrations system, a suite of client libraries in different languages, a command line tool, and\u2014coming soon\u2014a cloud hosting platform. It's more than just a mapper or ORM library, rethinking every aspect of how developers model, migrate, manage, and query their database. With a strict type system, indexes, constraints, computed properties, stored procedures, and shiny new features like link properties, schema mixins, and best-in-class JSON support, EdgeDB introduces types, not tables, and objects, not rows, connected by links. The super-powered EdgeQL query language produces rich, structured objects, deeply fetching related objects is painless, and EdgeQL queries are also composable. The goal is to give developers a next-level experience, with just three commands to open an interactive EdgeQL shell.", "start_char_idx": 0, "end_char_idx": 961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a9e1df7-06b5-4099-b55c-b2b8917ab711": {"__data__": {"id_": "2a9e1df7-06b5-4099-b55c-b2b8917ab711", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "139b9d1d-b8a9-4a68-8c54-2553083d7e07", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4fd17df5e4c8836a10ecc028c6cb1c60f928369ddd5b10971659d1e7a74fce50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df31eaea-ed63-4a41-b411-842b54ea0ec9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\101.txt", "file_name": "101.txt", "file_type": "text/plain", "file_size": 967, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "70447d0da781c7443549ea2d39ca25fd93dbd85d972ad5b96f4215f8a6e82605", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "babaae1a-c048-45cf-90fe-99d3a6940348", "node_type": "1", "metadata": {}, "hash": "5e6cc59f0c51b939db08d4788f46f57c7a372238d95869d3b8be984e076ae020", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pinot\r\nlink: https://github.com/apache/pinot\r\ndescription: \"What is Apache Pinot? Apache Pinot is a real-time distributed OLAP datastore, built to deliver scalable real-time analytics with low latency. It can ingest from batch data sources (such as Hadoop HDFS, Amazon S3, Azure ADLS, Google Cloud Storage) as well as stream data sources (such as Apache Kafka). Pinot was built by engineers at LinkedIn and Uber and is designed to scale up and out with no upper bound. Performance always remains constant based on the size of your cluster and an expected query per second (QPS) threshold. Pinot is designed to execute real-time OLAP queries with low latency on massive amounts of data and events. In addition to real-time stream ingestion, Pinot also supports batch use cases with the same low latency guarantees. It is suited in contexts where fast analytics, such as aggregations, are needed on immutable data, possibly, with real-time data ingestion.", "start_char_idx": 0, "end_char_idx": 964, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "babaae1a-c048-45cf-90fe-99d3a6940348": {"__data__": {"id_": "babaae1a-c048-45cf-90fe-99d3a6940348", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "139b9d1d-b8a9-4a68-8c54-2553083d7e07", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4fd17df5e4c8836a10ecc028c6cb1c60f928369ddd5b10971659d1e7a74fce50", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a9e1df7-06b5-4099-b55c-b2b8917ab711", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0942d2e9e87b391695e127ada359b11b788b4b765d0cde36fc10fe4d16d30a3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6f38bfa-f98f-4f3e-80a7-ef7298e0a554", "node_type": "1", "metadata": {}, "hash": "72fec7d29fe1f1b5e0195ade488a7d0c588e09954dc337e758ec59f528df31dd", "class_name": "RelatedNodeInfo"}}, "text": "Apache Pinot is a real-time distributed OLAP datastore, built to deliver scalable real-time analytics with low latency. It can ingest from batch data sources (such as Hadoop HDFS, Amazon S3, Azure ADLS, Google Cloud Storage) as well as stream data sources (such as Apache Kafka). Pinot was built by engineers at LinkedIn and Uber and is designed to scale up and out with no upper bound. Performance always remains constant based on the size of your cluster and an expected query per second (QPS) threshold. Pinot is designed to execute real-time OLAP queries with low latency on massive amounts of data and events. In addition to real-time stream ingestion, Pinot also supports batch use cases with the same low latency guarantees. It is suited in contexts where fast analytics, such as aggregations, are needed on immutable data, possibly, with real-time data ingestion. Pinot works very well for querying time series data with lots of dimensions and metrics.\"", "start_char_idx": 93, "end_char_idx": 1054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6f38bfa-f98f-4f3e-80a7-ef7298e0a554": {"__data__": {"id_": "b6f38bfa-f98f-4f3e-80a7-ef7298e0a554", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\103.txt", "file_name": "103.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aabfb308-4360-4680-b521-e061f877d376", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\103.txt", "file_name": "103.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7048bd5882c30a4bae7bd771b354f1376481c86221f6e822b94b2cfaf2833604", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "babaae1a-c048-45cf-90fe-99d3a6940348", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "98565490cc1cd992a281179289a0bc7dd814f7ab8edae12dee0e191fd25be527", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c675754f-af3e-49e2-85b1-a5f5e8c7577f", "node_type": "1", "metadata": {}, "hash": "b82d66d797835c9634e4208945c8b0db3eec389d47d031195d0883fbb8e25d8e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ignite\r\nlink: https://github.com/apache/ignite\r\ndescription: Apache Ignite is a distributed database for high-performance computing with in-memory speed. It is designed to work with memory, disk, and Intel Optane as active storage tiers, and supports Ignite Native Persistence - a distributed, ACID, and SQL-compliant disk-based store. The system also includes an ANSI-99 compliant, horizontally scalable, and fault-tolerant SQL engine, and offers machine learning tools for building predictive models. High-performance computing can be achieved by using Apache Ignite as a high-performance compute cluster, turning a group of commodity machines or a cloud environment into a distributed supercomputer.", "start_char_idx": 0, "end_char_idx": 713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c675754f-af3e-49e2-85b1-a5f5e8c7577f": {"__data__": {"id_": "c675754f-af3e-49e2-85b1-a5f5e8c7577f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\104.txt", "file_name": "104.txt", "file_type": "text/plain", "file_size": 942, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6560b13a-ac24-44e2-bafc-5ee91c0f8884", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\104.txt", "file_name": "104.txt", "file_type": "text/plain", "file_size": 942, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1d9f8af2429ddad803132bfebd4220e7946ac60296207b55c10314a279717fa8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6f38bfa-f98f-4f3e-80a7-ef7298e0a554", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\103.txt", "file_name": "103.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4c90da16c03bd49b3de1371b06a4118029697f47145f7464e68858748f799989", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cecd8ee7-58b9-4a0e-98ab-0ecdb5b81a83", "node_type": "1", "metadata": {}, "hash": "7c39585ee58a15f4687d62f5311cc73fb0d5be3b67f97730a7ddb98f54f4523b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: druid\r\nlink: https://github.com/apache/druid\r\ndescription: Druid is a high performance real-time analytics database designed for workflows where fast queries and ingest really matter. It excels at powering UIs, running operational (ad-hoc) queries, or handling high concurrency. Consider Druid as an open source alternative to data warehouses for a variety of use cases. The design documentation explains the key concepts. Druid provides a rich set of APIs (via HTTP and JDBC) for loading, managing, and querying your data. You can also interact with Druid via the built-in web console. Load streaming and batch data using a point-and-click wizard to guide you through ingestion setup. Manage your cluster with ease and prototype queries with the built-in query workbench. Visit the official project community page to read about getting involved in contributing to Apache Druid, and how we help one another use and operate Druid.", "start_char_idx": 0, "end_char_idx": 940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cecd8ee7-58b9-4a0e-98ab-0ecdb5b81a83": {"__data__": {"id_": "cecd8ee7-58b9-4a0e-98ab-0ecdb5b81a83", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\105.txt", "file_name": "105.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb57b4af-92f4-41d6-8bac-6e6967e6f8bc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\105.txt", "file_name": "105.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2b27be945e3e411c42bdb13fb05a0f7f5eea81fe9c56c6bb92bfd5c26ccb9735", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c675754f-af3e-49e2-85b1-a5f5e8c7577f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\104.txt", "file_name": "104.txt", "file_type": "text/plain", "file_size": 942, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "10d598ce69bac914c74684ad9ea4011a1c6d516a02805623d9fd8c8bd6eee230", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "847a89ad-85df-4391-a2ec-f9b6a4ec35f5", "node_type": "1", "metadata": {}, "hash": "860134a5157bfdbe3653c4e110031c3c5d55f959a1ef81157d4913491838e0df", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: alluxio\r\nlink: https://github.com/Alluxio/alluxio\r\ndescription: Alluxio (formerly known as Tachyon) is a virtual distributed storage system that bridges the gap between computation frameworks and storage systems, enabling computation applications to connect to numerous storage systems through a common interface. The Alluxio project originated from a research project called Tachyon at AMPLab, UC Berkeley, which was the data layer of the Berkeley Data Analytics Stack (BDAS). For more details, please refer to Haoyuan Li's PhD dissertation. Alluxio is used in production to manage Petabytes of data in many leading companies, with the largest deployment exceeding 3,000 nodes. You can find more use cases at Powered by Alluxio or visit our first community conference (Data Orchestration Summit) to learn from other community members!", "start_char_idx": 0, "end_char_idx": 846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "847a89ad-85df-4391-a2ec-f9b6a4ec35f5": {"__data__": {"id_": "847a89ad-85df-4391-a2ec-f9b6a4ec35f5", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\106.txt", "file_name": "106.txt", "file_type": "text/plain", "file_size": 983, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c97a0073-d1d9-4e6b-88e6-fceeb3e7e31e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\106.txt", "file_name": "106.txt", "file_type": "text/plain", "file_size": 983, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "28f6cc0de6fbed56d29566c1625b8b5f997f0c073120fc39acd5fd35d0f07a63", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cecd8ee7-58b9-4a0e-98ab-0ecdb5b81a83", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\105.txt", "file_name": "105.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "83e666a1948d00b8d732b8007a46ca1fb5b378c1e869ba3e8e15e78135f98ea4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2df32ffc-1586-4d61-a892-b62ba5932c5f", "node_type": "1", "metadata": {}, "hash": "b330c6b5a39a00c219afc12233c2f6a990fe237c2b7c44d10d19d0bc03a11059", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ml-metadata\r\nlink: https://github.com/google/ml-metadata\r\ndescription: _ML Metadata (MLMD)_ is a library for recording and retrieving metadata associated with ML developer and data scientist workflows. ML Metadata may be backwards incompatible before version 1.0. The recommended way to install ML Metadata is to use the PyPI package: `pip install ml-metadata`. ML Metadata (MLMD) also hosts nightly packages at https://pypi-nightly.tensorflow.org on Google Cloud. To install the latest nightly package, please use the following command: `pip install --extra-index-url https://pypi-nightly.tensorflow.org/simple ml-metadata`. ML Metadata can also be installed with Docker, which is the recommended way to build ML Metadata under Linux. Additionally, ML Metadata can be installed from source with a few prerequisites. MLMD is built and tested on the following 64-bit operating systems. For more information on installing ML Metadata, please see the getting started guide.", "start_char_idx": 0, "end_char_idx": 981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2df32ffc-1586-4d61-a892-b62ba5932c5f": {"__data__": {"id_": "2df32ffc-1586-4d61-a892-b62ba5932c5f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "baeb0f1a-cc92-46a2-886f-9b3186cd23b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "22c38b942f09300232e07abcb646fb8bd94a75e9e8399a163d3cc17dac241796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "847a89ad-85df-4391-a2ec-f9b6a4ec35f5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\106.txt", "file_name": "106.txt", "file_type": "text/plain", "file_size": 983, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ff502a77b03ffe3f569d9e14a4e3c90b85a5e0f0d30d44eb85015cdb33f264f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ac77a62-188b-4ae4-8a92-951eca0ca931", "node_type": "1", "metadata": {}, "hash": "cc9e738b1bb222d99421558e03e660d6a77548846411a163dfa0101845c29b6c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: model-card-toolkit\r\nlink: https://github.com/tensorflow/model-card-toolkit\r\ndescription: The Model Card Toolkit (MCT) streamlines and automates generation of Model Cards [1], machine learning documents that provide context and transparency into a model's development and performance. Integrating the MCT into your ML pipeline enables you to share model metadata and metrics with researchers, developers, reporters, and more. Some use cases of model cards include: Automate any workflow, Host and manage packages, Find and fix vulnerabilities, Instant dev environments, Write better code with AI, Collaborate outside of code, Fund open source developers, GitHub community articles. The Model Card Toolkit is hosted on PyPI, and requires Python 3.7 or later. See the installation guide for more installation options. If you are using TensorFlow Extended (TFX), you can incorporate model card generation into your TFX pipeline via the ModelCardGenerator component.", "start_char_idx": 0, "end_char_idx": 972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ac77a62-188b-4ae4-8a92-951eca0ca931": {"__data__": {"id_": "6ac77a62-188b-4ae4-8a92-951eca0ca931", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "baeb0f1a-cc92-46a2-886f-9b3186cd23b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "22c38b942f09300232e07abcb646fb8bd94a75e9e8399a163d3cc17dac241796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2df32ffc-1586-4d61-a892-b62ba5932c5f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5fa6cff2af1a461cc897ac3d738c04ade10189e33f71292facd5c1418f74026e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa795666-e029-48fd-8758-1d04d7f03014", "node_type": "1", "metadata": {}, "hash": "fe04bc0f0de3b87211e2ce4d086a10e0040fefe5c7829abbcf05907429071aa6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: model-card-toolkit\r\nlink: https://github.com/tensorflow/model-card-toolkit\r\ndescription: The Model Card Toolkit (MCT) streamlines and automates generation of Model Cards [1], machine learning documents that provide context and transparency into a model's development and performance. Integrating the MCT into your ML pipeline enables you to share model metadata and metrics with researchers, developers, reporters, and more. Some use cases of model cards include: Automate any workflow, Host and manage packages, Find and fix vulnerabilities, Instant dev environments, Write better code with AI, Collaborate outside of code, Fund open source developers, GitHub community articles. The Model Card Toolkit is hosted on PyPI, and requires Python 3.7 or later. See the installation guide for more installation options. If you are using TensorFlow Extended (TFX), you can incorporate model card generation into your TFX pipeline via the ModelCardGenerator component. The ModelCardGenerator component has moved to the TFX Addons library and is no longer packaged in Model Card Toolkit from version 2.0.0.", "start_char_idx": 0, "end_char_idx": 1109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa795666-e029-48fd-8758-1d04d7f03014": {"__data__": {"id_": "fa795666-e029-48fd-8758-1d04d7f03014", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "baeb0f1a-cc92-46a2-886f-9b3186cd23b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "22c38b942f09300232e07abcb646fb8bd94a75e9e8399a163d3cc17dac241796", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ac77a62-188b-4ae4-8a92-951eca0ca931", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f69f8c146a6e63ada4a8c39e569d86b7f54f54d0ed62180a0c7627f1c0c3accd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "539dc552-ce4f-4f08-92e2-2ef8b6a339e4", "node_type": "1", "metadata": {}, "hash": "03b5016820a0ccb3ffd3bd5d0020111b439387fcd9dfc936522459c79f0044c5", "class_name": "RelatedNodeInfo"}}, "text": "Integrating the MCT into your ML pipeline enables you to share model metadata and metrics with researchers, developers, reporters, and more. Some use cases of model cards include: Automate any workflow, Host and manage packages, Find and fix vulnerabilities, Instant dev environments, Write better code with AI, Collaborate outside of code, Fund open source developers, GitHub community articles. The Model Card Toolkit is hosted on PyPI, and requires Python 3.7 or later. See the installation guide for more installation options. If you are using TensorFlow Extended (TFX), you can incorporate model card generation into your TFX pipeline via the ModelCardGenerator component. The ModelCardGenerator component has moved to the TFX Addons library and is no longer packaged in Model Card Toolkit from version 2.0.0. Before you can use the component, you will need to install the tfx-addons package. Model cards are stored in proto as an intermediate format. You can see the model card JSON schema in the schema directory.", "start_char_idx": 295, "end_char_idx": 1315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "539dc552-ce4f-4f08-92e2-2ef8b6a339e4": {"__data__": {"id_": "539dc552-ce4f-4f08-92e2-2ef8b6a339e4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\108.txt", "file_name": "108.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c2b9635-b7c3-4992-bcb0-b2b41675bcf4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\108.txt", "file_name": "108.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "930af367995762aff6437e04c6f67d85249a63689c0435b1c1ab9fdb0377dfed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa795666-e029-48fd-8758-1d04d7f03014", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "57f70b6c3c0bd82d705d5c6c92fd2ea210ad7b6bc5d8355e500a63cdef81c770", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0948931-3583-440b-ae33-cd6eded834ec", "node_type": "1", "metadata": {}, "hash": "8501661deaad98ce17658139c3fbd422158fe6e19bce4a907c7645f9e1fe3ac1", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: metacat\r\nlink: https://github.com/Netflix/metacat\r\ndescription: Metacat is a unified metadata exploration API service. You can explore Hive, RDS, Teradata, Redshift, S3 and Cassandra. Metacat provides you information about what data you have, where it resides and how to process it. Metadata in the end is really data about the data. So the primary purpose of Metacat is to give a place to describe the data so that we could do more useful things with it. Metacat focuses on solving these three problems.", "start_char_idx": 0, "end_char_idx": 515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0948931-3583-440b-ae33-cd6eded834ec": {"__data__": {"id_": "c0948931-3583-440b-ae33-cd6eded834ec", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\109.txt", "file_name": "109.txt", "file_type": "text/plain", "file_size": 973, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8751e01c-546f-45c9-a18f-9072f84ece02", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\109.txt", "file_name": "109.txt", "file_type": "text/plain", "file_size": 973, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "775e03c1e68954d6db99aae547166bd712da6ca81603e2752dd168cd7ac8309f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "539dc552-ce4f-4f08-92e2-2ef8b6a339e4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\108.txt", "file_name": "108.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9acd08f1b006fa8234dd64fca0161a127f10599ff0dd989e095e65bd78bf3bab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f7cf6b2-255f-47cc-af52-7266f17db243", "node_type": "1", "metadata": {}, "hash": "ac370a2474c9f016df955094cd3d0b69b054a1b56bca5f27740ac8fafa0215ce", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: datahub\r\nlink: https://github.com/datahub-project/datahub\r\ndescription: DataHub is an open-source metadata platform for the modern data stack that can automate any workflow, host and manage packages, find and fix vulnerabilities, provide instant dev environments, help write better code with AI, collaborate outside of code and fund open source developers. It is built with love by Acryl Data and LinkedIn and hosted on Acryl Data's DataHub Docs platform. It offers various features and a roadmap to follow, a demo environment to explore without installing it locally, and a quickstart guide to get started with Docker. DataHub also provides source code, repositories, and releases following the SemVer Specification and Keep a Changelog convention. It welcomes contributions from the community and has a contrib directory for incubating experimental features. DataHub is adopted by several companies, and there are select articles and talks available as well.", "start_char_idx": 0, "end_char_idx": 971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f7cf6b2-255f-47cc-af52-7266f17db243": {"__data__": {"id_": "3f7cf6b2-255f-47cc-af52-7266f17db243", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\11.txt", "file_name": "11.txt", "file_type": "text/plain", "file_size": 903, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96495664-a204-47b7-bbea-f03ce0079376", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\11.txt", "file_name": "11.txt", "file_type": "text/plain", "file_size": 903, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4efc834447c42fe7b613b0aa0bd0a438d9e1397003dce3d5dd7b763b36861ccd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0948931-3583-440b-ae33-cd6eded834ec", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\109.txt", "file_name": "109.txt", "file_type": "text/plain", "file_size": 973, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "39e7524fd313d2e3c1525fdd5dfd8b22149508517f44a2080be4cc1b9834ed7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6361f438-1636-431c-a9e3-6451497ade6f", "node_type": "1", "metadata": {}, "hash": "1a9291290e67ba84366b5cb02aaec14201ffff5248e08cb878f2c6cb4bd02486", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: kafka\r\nlink: https://github.com/apache/kafka\r\ndescription: Apache Kafka is a platform that allows users to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and access GitHub community articles. Kafka can be built and tested with Java 8, 11 and 17 and Scala 2.12 and 2.13. The following are various tasks and commands that can be used with Kafka, such as building a jar and running it, running unit/integration tests, and generating test coverage reports. Additionally, Kafka provides information on running a Kafka broker in KRaft or ZooKeeper mode, how to install jars to the local Maven repository, and running code quality checks using Checkstyle and Spotbugs. Kafka also invites community contributions through their Apache mailing lists.", "start_char_idx": 0, "end_char_idx": 901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6361f438-1636-431c-a9e3-6451497ade6f": {"__data__": {"id_": "6361f438-1636-431c-a9e3-6451497ade6f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\110.txt", "file_name": "110.txt", "file_type": "text/plain", "file_size": 694, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b29e417-ccc3-4b22-8dfd-39da9db03f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\110.txt", "file_name": "110.txt", "file_type": "text/plain", "file_size": 694, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "819c75d105441d8040f480a9ccb6bfa6374b6a7df68403c3d3a7b857e8ae475c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f7cf6b2-255f-47cc-af52-7266f17db243", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\11.txt", "file_name": "11.txt", "file_type": "text/plain", "file_size": 903, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f889ab631ee4719f6df58abe87853379fc864ba438846ab5787ae204ad698601", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d6aeb60-b634-4d07-a04a-172a35461673", "node_type": "1", "metadata": {}, "hash": "25d805f257f8847ea6c0447cfc8964d0d395af2635ca2e7cec7fc811f55dbcaa", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: marquez\r\nlink: https://github.com/MarquezProject/marquez\r\ndescription: Marquez is an open source **metadata service** for the **collection**, **aggregation**, and **visualization** of a data ecosystem's metadata. It maintains the provenance of how datasets are consumed and produced, provides global visibility into job runtime and frequency of dataset access, centralization of dataset lifecycle management, and much more. Marquez was released and open sourced by WeWork. It is an LF AI & Data Foundation incubation project under active development, and we'd love your help! Marquez provides a simple way to collect and view _dataset_, _job_, and _run_ metadata using OpenLineage.", "start_char_idx": 0, "end_char_idx": 692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d6aeb60-b634-4d07-a04a-172a35461673": {"__data__": {"id_": "4d6aeb60-b634-4d07-a04a-172a35461673", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\111.txt", "file_name": "111.txt", "file_type": "text/plain", "file_size": 1041, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "731197b0-1ce1-4ff6-8452-a3c6ba3e9bc8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\111.txt", "file_name": "111.txt", "file_type": "text/plain", "file_size": 1041, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b63fa40761ded24ab353897757c6fdae5aeab294aa36f0d9117e594c8f41f301", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6361f438-1636-431c-a9e3-6451497ade6f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\110.txt", "file_name": "110.txt", "file_type": "text/plain", "file_size": 694, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c45497fc34c38e1be42601222bea25a2dc9e771c852789d66ba523a0b398cd9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c42ca9d-ce06-4c1e-9a5d-6747b1734d40", "node_type": "1", "metadata": {}, "hash": "e94b16336cc2249485c01e8a7fdadf9f7123a29da978ef4030fd0eaebf125da2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: amundsen\r\nlink: https://github.com/amundsen-io/amundsen\r\ndescription: Amundsen is a data discovery and metadata engine that improves the productivity of data analysts, data scientists, and engineers when interacting with data. It indexes data resources such as tables, dashboards, and streams, and powers a page-rank style search based on usage patterns. The project is named after Norwegian explorer Roald Amundsen, the first person to discover the South Pole. Amundsen is hosted by the LF AI & Data Foundation and includes three microservices, one data ingestion library, and one common library. \r\n\r\nAside from the installation, the Amundsen platform offers various features to help automate workflows, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, and fund open-source developers. The platform also has various community resources and documentation that users can utilize to get started and engage with the Amundsen community.", "start_char_idx": 0, "end_char_idx": 1039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c42ca9d-ce06-4c1e-9a5d-6747b1734d40": {"__data__": {"id_": "2c42ca9d-ce06-4c1e-9a5d-6747b1734d40", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\112.txt", "file_name": "112.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a272fd28-cfe0-494c-8745-5c6fdbf79897", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\112.txt", "file_name": "112.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e7123c42e911cf68b3b264818ada612418fd0050b283e638deccbd444037c122", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d6aeb60-b634-4d07-a04a-172a35461673", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\111.txt", "file_name": "111.txt", "file_type": "text/plain", "file_size": 1041, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "927b150c7526174f9ea74dac3166d2f84bd2aee007d0fb74a991b247a6af3997", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eff87b85-cdc5-4e04-8ec2-0fc9418e8053", "node_type": "1", "metadata": {}, "hash": "4ad3945a6a9f0194318e1ff721cf70ba8c5914840f99de1639ee0d547dd32202", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: atlas\r\nlink: https://github.com/apache/atlas\r\ndescription:", "start_char_idx": 0, "end_char_idx": 69, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eff87b85-cdc5-4e04-8ec2-0fc9418e8053": {"__data__": {"id_": "eff87b85-cdc5-4e04-8ec2-0fc9418e8053", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40ed341b-1507-4cae-9d07-e6e9baf669b7", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7b32b980bcedcb497445e1ca4d1d2cd5d5e8c63905648c1dd66184d231ade623", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c42ca9d-ce06-4c1e-9a5d-6747b1734d40", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\112.txt", "file_name": "112.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "121404acebd9de50ff497de2d37410c5a4075612906eee167cdf34475d850b59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53bd7ae5-5941-477a-ba1a-db0deb515cd9", "node_type": "1", "metadata": {}, "hash": "6fb8e52ccc9b95507a6de8a0649765796e4fea607638258ed396560f0a4f0f24", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: VoTT\r\nlink: https://github.com/microsoft/VoTT\r\ndescription: VoTT (Visual Object Tagging Tool) is an open source annotation and labeling tool for image and video assets that facilitates an end-to-end machine learning pipeline. Written in TypeScript and built with React and Redux, VoTT can be installed as a native application or run from source, and is also available as a stand-alone Web application that can be used in any modern Web browser. The tool helps automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and more. With VoTT, users can create connections, projects, and label images and videos, as well as export labels into various formats. VoTT is no longer being maintained, but users can still download and install a release package for their platform, build and run from source, or run as a Web application.", "start_char_idx": 0, "end_char_idx": 975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53bd7ae5-5941-477a-ba1a-db0deb515cd9": {"__data__": {"id_": "53bd7ae5-5941-477a-ba1a-db0deb515cd9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40ed341b-1507-4cae-9d07-e6e9baf669b7", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7b32b980bcedcb497445e1ca4d1d2cd5d5e8c63905648c1dd66184d231ade623", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eff87b85-cdc5-4e04-8ec2-0fc9418e8053", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9db447504d6bfdeb39e6c89db83d88d2e119614db905a97b620777dc12cb5490", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d51298aa-aa0d-47ec-a224-ef5bdf6d4949", "node_type": "1", "metadata": {}, "hash": "fbf38fc22c3feaa5e7e3e4ab474136941a0aa9ea0532a821c249dd02861dd5c0", "class_name": "RelatedNodeInfo"}}, "text": "Written in TypeScript and built with React and Redux, VoTT can be installed as a native application or run from source, and is also available as a stand-alone Web application that can be used in any modern Web browser. The tool helps automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and more. With VoTT, users can create connections, projects, and label images and videos, as well as export labels into various formats. VoTT is no longer being maintained, but users can still download and install a release package for their platform, build and run from source, or run as a Web application. To learn more about using VoTT, creating connections and projects, labeling assets, and contributing to the project, users can consult the project repository on GitHub.", "start_char_idx": 237, "end_char_idx": 1144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d51298aa-aa0d-47ec-a224-ef5bdf6d4949": {"__data__": {"id_": "d51298aa-aa0d-47ec-a224-ef5bdf6d4949", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\114.txt", "file_name": "114.txt", "file_type": "text/plain", "file_size": 984, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "301cd385-0111-44ca-9961-a276a29c7e02", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\114.txt", "file_name": "114.txt", "file_type": "text/plain", "file_size": 984, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4d814d9f9cc09947292ae418ca7839ac293f0f046877d4339770c87f1728886c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53bd7ae5-5941-477a-ba1a-db0deb515cd9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f44b651fa4a6c7f2002ec14f9d137f355fcb7c004e21c593cdc683b2733998a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44c55cae-f536-4a38-9177-afe958025933", "node_type": "1", "metadata": {}, "hash": "5a5782247ab3d80cd20b2fced15ca41e955535f7559254f4e13e80abea4ed746", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: semantic-segmentation-editor\r\nlink: https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor\r\ndescription: The Semantic Segmentation Editor is a web-based labeling tool for creating AI training data sets for 2D and 3D images. It is Meteor app developed with React, Paper.js, and three.js. The tool supports images (.jpg or .png) and point clouds (.pcd), and it includes two editing modes: the Bitmap Image Editor, which is dedicated to the labeling of jpg and png files by drawing polygons, and the PCD Point Cloud Editor, which is dedicated to the labeling of point clouds by creating objects made of subsets of 3D points. The tool also has a file navigator that lets you browse available files to select an image or a point cloud for labeling. It is possible to customize the tool by modifying the settings.json file, which is explained in detail in the documentation. The tool can be run using Docker Compose or it can be installed manually.", "start_char_idx": 0, "end_char_idx": 982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44c55cae-f536-4a38-9177-afe958025933": {"__data__": {"id_": "44c55cae-f536-4a38-9177-afe958025933", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\115.txt", "file_name": "115.txt", "file_type": "text/plain", "file_size": 677, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82d4e3c2-b6cc-418b-941c-4e5692901e27", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\115.txt", "file_name": "115.txt", "file_type": "text/plain", "file_size": 677, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6a462bb2844a1dea718719b9bf1d25eced5aae2a3c77b732b1df3b95154633d8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d51298aa-aa0d-47ec-a224-ef5bdf6d4949", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\114.txt", "file_name": "114.txt", "file_type": "text/plain", "file_size": 984, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2e31c9ba342329553663ec06344691f30514da220e081e8561393bdd387a4664", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81263731-90e8-47fd-9da3-9ef79436b245", "node_type": "1", "metadata": {}, "hash": "f5291fff38f270a343b9931fc81b1909e3385b06bfd5f19df9141d95d1067cbc", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: superintendent\r\nlink: https://github.com/janfreyberg/superintendent\r\ndescription: The Superintendent is an `ipywidget`-based interactive labelling tool for your data that allows flexible labelling of all kinds of data, as well as combining data-labelling tasks with statistical or machine learning models to enable quick and practical active learning. To get started with Superintendent, you can take a look at the documentation and examples provided at http://www.janfreyberg.com/superintendent/ before installing it using pip and enabling the required jupyter extension. If you want to contribute to the library, test dependencies will also need to be installed.", "start_char_idx": 0, "end_char_idx": 675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81263731-90e8-47fd-9da3-9ef79436b245": {"__data__": {"id_": "81263731-90e8-47fd-9da3-9ef79436b245", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\116.txt", "file_name": "116.txt", "file_type": "text/plain", "file_size": 578, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9069c6f4-c561-45c6-8347-d92d766bf645", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\116.txt", "file_name": "116.txt", "file_type": "text/plain", "file_size": 578, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "25457e62a2a6ccedf7154f2389a0c20fa3c95865e8309369ad041be90b73ad21", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44c55cae-f536-4a38-9177-afe958025933", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\115.txt", "file_name": "115.txt", "file_type": "text/plain", "file_size": 677, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "28c6e3e868ff659c280567ef986c77db1fbced43e6eae3966347b3062b782835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b23d7c4c-e038-4a00-abeb-2de0501ff9d2", "node_type": "1", "metadata": {}, "hash": "08ce40c5db62beb3b25ddfd8b593c345f6c1ca2b0203d6a2cfe8dacaac04ad0c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: PixelAnnotationTool\r\nlink: https://github.com/abreheret/PixelAnnotationTool\r\ndescription: PixelAnnotationTool is a tool that can assist in pixel annotation tasks. It comes with features such as automated workflows, hosting and managing packages, finding and fixing vulnerabilities, and instant dev environment setup. It also utilizes AI to help users write better code, and provides collaboration features to work outside of code. The tool is licensed under the GNU Lesser General Public License v3.0 and donations are accepted to help maintain and update the tool.", "start_char_idx": 0, "end_char_idx": 576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b23d7c4c-e038-4a00-abeb-2de0501ff9d2": {"__data__": {"id_": "b23d7c4c-e038-4a00-abeb-2de0501ff9d2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\117.txt", "file_name": "117.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb254119-f0ab-428e-a4d9-804706c1269c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\117.txt", "file_name": "117.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "93114a11e0a18cf8ad5ce79193789adb0df6037acd515d7eac0c76e8218b1254", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81263731-90e8-47fd-9da3-9ef79436b245", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\116.txt", "file_name": "116.txt", "file_type": "text/plain", "file_size": 578, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "746b34767345e0899a53ecd483dbe8cba37150d0f5f10027551d3c175736b695", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ef4627c-fc8d-4e15-8ffd-278b12d8f36d", "node_type": "1", "metadata": {}, "hash": "4cbb3a64f0f01a821d794a1698dab105b5d2c8f3f14657fe5711cf0ae2cecb35", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: MedTagger\r\nlink: https://github.com/medtagger/MedTagger\r\ndescription: **MedTagger** is a collaborative framework for annotating medical datasets. Its main goal is to design and develop a software environment that helps in aggregation and labeling huge datasets of medical scans, powered by the idea of crowdsourcing. The platform also provides a mechanism for label validation, thus making produced datasets of labels more reliable for future use. MedTagger is still under heavy development, so please keep in mind that many things may change or new versions may not be fully backward compatible. To set up MedTagger locally, you can use a Vagrant virtual machine or Docker-Compose. More information about setting up the local Jupyter Notebook session and MedTagger's technology stack can be found in its documentation.", "start_char_idx": 0, "end_char_idx": 830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ef4627c-fc8d-4e15-8ffd-278b12d8f36d": {"__data__": {"id_": "6ef4627c-fc8d-4e15-8ffd-278b12d8f36d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\118.txt", "file_name": "118.txt", "file_type": "text/plain", "file_size": 506, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d847c661-f669-4bae-b731-b9cdfe9cfdbf", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\118.txt", "file_name": "118.txt", "file_type": "text/plain", "file_size": 506, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eaf3bba0171a9de432f3555e43dd4feef4203e226c09f049bb4a602d87a74204", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b23d7c4c-e038-4a00-abeb-2de0501ff9d2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\117.txt", "file_name": "117.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "33d9a299c4c1301ce196f3448deba39a1212b9d4151145ba67beff2a668c320d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cd11473-6533-4e0d-aff6-bc811e79378f", "node_type": "1", "metadata": {}, "hash": "901f7a198eec0abd7d04bc1140fc7bd2add6be0053426313081c11542e44c8fa", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: OpenLabeling\r\nlink: https://github.com/Cartucho/OpenLabeling\r\ndescription: OpenLabeling is an open-source image and video labeler that supports multiple annotation formats. It can be used to label data manually or through automatic labeling with the help of deep learning. To use the tool, users can download the latest release or clone the repository, and follow the instructions provided in the documentation. The project is hosted on GitHub and contributions from the community are welcome.", "start_char_idx": 0, "end_char_idx": 504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cd11473-6533-4e0d-aff6-bc811e79378f": {"__data__": {"id_": "4cd11473-6533-4e0d-aff6-bc811e79378f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\119.txt", "file_name": "119.txt", "file_type": "text/plain", "file_size": 1047, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e5297a92-0aab-4af3-baa7-56b2bfe617bf", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\119.txt", "file_name": "119.txt", "file_type": "text/plain", "file_size": 1047, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ce9a09045cd36a3349fd8a7abf8406107b904175ec20c295fe049d72a9dff1bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ef4627c-fc8d-4e15-8ffd-278b12d8f36d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\118.txt", "file_name": "118.txt", "file_type": "text/plain", "file_size": 506, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1034d6da2dc10708c224a86738d514a71c6741b30f167c11f256181436e2da83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "655f1b90-c6e0-4099-b28a-aeafa6225868", "node_type": "1", "metadata": {}, "hash": "0ee4b53472ee4f7dac8f7ffaa71294403757d18ec7b70fc883ccb580be7c82df", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: make-sense\r\nlink: https://github.com/SkalskiP/make-sense\r\ndescription: MakeSense.ai is a free-to-use online tool for labeling photos. Thanks to the use of a browser it does not require any complicated installation - just visit the website and you are ready to go. It also doesn't matter which operating system you're running on - we do our best to be truly cross-platform. It is perfect for small computer vision deep learning projects, making the process of preparing a dataset much easier and faster. Prepared labels can be downloaded in one of the multiple supported formats. The application was written in TypeScript and is based on React/Redux duo. If you are just starting your adventure with deep learning and would like to learn and create something cool along the way, makesense.ai can help you with that. Leverage our bounding box labeling functionality to prepare a data set and use it to train your first state-of-the-art object detection model. Follow instructions and examples but most importantly, free your creativity.", "start_char_idx": 0, "end_char_idx": 1045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "655f1b90-c6e0-4099-b28a-aeafa6225868": {"__data__": {"id_": "655f1b90-c6e0-4099-b28a-aeafa6225868", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7a0d092e5d99618edc4fe58a7095a9e793f7f3ad1845bea9377d5a3fd696dabc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cd11473-6533-4e0d-aff6-bc811e79378f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\119.txt", "file_name": "119.txt", "file_type": "text/plain", "file_size": 1047, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "32c38b83841a836b29fd61c3919dd70104406f9f348653b6a28af15ade772684", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81cdda33-ed53-4777-b414-95ef6943af02", "node_type": "1", "metadata": {}, "hash": "9f96295121669f71747718a1a0e8f833bcaf7e14efbafe8783ccc054608382cb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: samza\r\nlink: https://github.com/apache/samza\r\ndescription: Apache Samza is a distributed stream processing framework. It uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management. Samza's key features include: automating any workflow, hosting and managing packages, finding and fixing vulnerabilities, providing instant dev environments, and writing better code with AI. To try Samza, check out Hello Samza and read the Background page to learn more about it. Samza can be built from a git checkout or a source release, and it requires Gradle to be installed on the source machine. Samza is built with Java 8 and can run in a Java 8 runtime environment, but it also supports running in a Java 11 runtime environment.", "start_char_idx": 0, "end_char_idx": 808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81cdda33-ed53-4777-b414-95ef6943af02": {"__data__": {"id_": "81cdda33-ed53-4777-b414-95ef6943af02", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7a0d092e5d99618edc4fe58a7095a9e793f7f3ad1845bea9377d5a3fd696dabc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "655f1b90-c6e0-4099-b28a-aeafa6225868", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "84b0698214b8d146d7937c066f5839cf0e8b796d6314ff9dca75a5c0bb4b0909", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "150f858c-ebb7-4b34-875f-26f5a4af09a2", "node_type": "1", "metadata": {}, "hash": "273575fa876226374c9809b0b3f4ec3ed8bc6ae05c29e625ef7486a94893feeb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: samza\r\nlink: https://github.com/apache/samza\r\ndescription: Apache Samza is a distributed stream processing framework. It uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management. Samza's key features include: automating any workflow, hosting and managing packages, finding and fixing vulnerabilities, providing instant dev environments, and writing better code with AI. To try Samza, check out Hello Samza and read the Background page to learn more about it. Samza can be built from a git checkout or a source release, and it requires Gradle to be installed on the source machine. Samza is built with Java 8 and can run in a Java 8 runtime environment, but it also supports running in a Java 11 runtime environment. To run Samza, use the provided instructions and tools, such as `./gradlew checkstyleMain checkstyleTest` for checking the Java code style, `./gradlew samza-shell:runJob` for running a job defined in a properties file, and `./gradlew samza-shell:checkpointTool` for inspecting a job's latest checkpoint.", "start_char_idx": 0, "end_char_idx": 1111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "150f858c-ebb7-4b34-875f-26f5a4af09a2": {"__data__": {"id_": "150f858c-ebb7-4b34-875f-26f5a4af09a2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7a0d092e5d99618edc4fe58a7095a9e793f7f3ad1845bea9377d5a3fd696dabc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81cdda33-ed53-4777-b414-95ef6943af02", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1d9280f9d454c799351f48ac1bf2f12600b51574e4bc6967a6b84cd6805d5cba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18473db6-4bcb-4daf-a165-600f8551e081", "node_type": "1", "metadata": {}, "hash": "c85fb6c579348558b8509f4e7e5f5aaa437ff9ca49364e60c858c330aac50e42", "class_name": "RelatedNodeInfo"}}, "text": "Samza's key features include: automating any workflow, hosting and managing packages, finding and fixing vulnerabilities, providing instant dev environments, and writing better code with AI. To try Samza, check out Hello Samza and read the Background page to learn more about it. Samza can be built from a git checkout or a source release, and it requires Gradle to be installed on the source machine. Samza is built with Java 8 and can run in a Java 8 runtime environment, but it also supports running in a Java 11 runtime environment. To run Samza, use the provided instructions and tools, such as `./gradlew checkstyleMain checkstyleTest` for checking the Java code style, `./gradlew samza-shell:runJob` for running a job defined in a properties file, and `./gradlew samza-shell:checkpointTool` for inspecting a job's latest checkpoint. As a top-level project of the Apache Software Foundation, Samza follows ASF rules and has its contributor corner.", "start_char_idx": 272, "end_char_idx": 1225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18473db6-4bcb-4daf-a165-600f8551e081": {"__data__": {"id_": "18473db6-4bcb-4daf-a165-600f8551e081", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7a0d092e5d99618edc4fe58a7095a9e793f7f3ad1845bea9377d5a3fd696dabc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "150f858c-ebb7-4b34-875f-26f5a4af09a2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a88f74514db5211eb77950429a6b5b7b9cc8b0fb910a12209579851cff999d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70c8a4bc-f01e-4542-8f6a-dfd0ca11a234", "node_type": "1", "metadata": {}, "hash": "e2c1bd68f2059d28279e0242413abca85f974c4aefad704c83083d3afda667bf", "class_name": "RelatedNodeInfo"}}, "text": "To try Samza, check out Hello Samza and read the Background page to learn more about it. Samza can be built from a git checkout or a source release, and it requires Gradle to be installed on the source machine. Samza is built with Java 8 and can run in a Java 8 runtime environment, but it also supports running in a Java 11 runtime environment. To run Samza, use the provided instructions and tools, such as `./gradlew checkstyleMain checkstyleTest` for checking the Java code style, `./gradlew samza-shell:runJob` for running a job defined in a properties file, and `./gradlew samza-shell:checkpointTool` for inspecting a job's latest checkpoint. As a top-level project of the Apache Software Foundation, Samza follows ASF rules and has its contributor corner. Note that Samza's git repository does not support git pull request.", "start_char_idx": 463, "end_char_idx": 1293, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70c8a4bc-f01e-4542-8f6a-dfd0ca11a234": {"__data__": {"id_": "70c8a4bc-f01e-4542-8f6a-dfd0ca11a234", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\120.txt", "file_name": "120.txt", "file_type": "text/plain", "file_size": 660, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df3de19e-b8da-4d9d-998b-4f2e16fc0998", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\120.txt", "file_name": "120.txt", "file_type": "text/plain", "file_size": 660, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "71d2d36e297dd7184242fa79231bcdadd0d57fd828b59fdeb439f6ace37a2744", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18473db6-4bcb-4daf-a165-600f8551e081", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f01de903a86ef032ed4c2381b3b476839729b9f2bfdcdc4755094ceecb4e2745", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77669bd8-7407-4c75-8c1d-de4ed1cfb681", "node_type": "1", "metadata": {}, "hash": "e7126a753f3f3fdcdba818ed925df35325f975bf7c60eb16ccd10224c0a2ecea", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: label-studio\r\nlink: https://github.com/heartexlabs/label-studio\r\ndescription: Label Studio is an open source data labeling tool. It lets you label data types like audio, text, images, videos, and time series with a simple and straightforward UI and export to various model formats. It can be used to prepare raw data or improve existing training data to get more accurate ML models. You can customize Label Studio to fit your needs and have a custom dataset. The software can be installed locally with Docker or pip, or deployed in a cloud instance. Label Studio includes templates for labeling data and can be integrated with your existing tools.", "start_char_idx": 0, "end_char_idx": 658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77669bd8-7407-4c75-8c1d-de4ed1cfb681": {"__data__": {"id_": "77669bd8-7407-4c75-8c1d-de4ed1cfb681", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\121.txt", "file_name": "121.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e201afc0-6bc7-446c-b9e6-e0e132d9b77c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\121.txt", "file_name": "121.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "af8bd90d8cc74508e2bfeec888a22be71849779f997acdfd99d5bf959e52e6f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70c8a4bc-f01e-4542-8f6a-dfd0ca11a234", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\120.txt", "file_name": "120.txt", "file_type": "text/plain", "file_size": 660, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "68be39ebcf4c355976104ebd43e9825c0a178973f5336bdd1105a5c954037c46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8edd86e4-b1ed-4738-8bd9-4e03b664490d", "node_type": "1", "metadata": {}, "hash": "78f353074e32a11fce296b34d6a49c3bf1449de96285166d1c28fee3db9106f7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: labelImg\r\nlink: https://github.com/heartexlabs/labelImg\r\ndescription:", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8edd86e4-b1ed-4738-8bd9-4e03b664490d": {"__data__": {"id_": "8edd86e4-b1ed-4738-8bd9-4e03b664490d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\122.txt", "file_name": "122.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3fb87eb-71b8-4f8d-b9b1-2d37466816bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\122.txt", "file_name": "122.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fe66e98a6e0cd22c8752ef2e881e96859fb83f27af178c92386249d8aeb88dcc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77669bd8-7407-4c75-8c1d-de4ed1cfb681", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\121.txt", "file_name": "121.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7b77cbea74cea184c34638284708a5686117c07bf3f4dab4ca3e151c2975422f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8999210f-60e5-4433-8832-0e20f0ce5eae", "node_type": "1", "metadata": {}, "hash": "f22b07dd02d46c3eaf3bc8bc3935a5c56ea350bea12e1fca10ba53a7c85b88bd", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: imagetagger\r\nlink: https://github.com/bit-bots/imagetagger\r\ndescription: ImageTagger is a collaborative online tool for labeling image data. This tool can be installed and run locally (best for development), in a docker container or in Kubernetes (used in our deployment). In some of the following code snippets, the `DJANGO_CONFIGURATION` environment variable is exported. This defines the type of deployment by selecting one of our predefined configuration presets. If ImageTagger is running in a development environment, no export is necessary. Please refer to the 'Installing and running ImageTagger' section for detailed installation instructions.", "start_char_idx": 0, "end_char_idx": 663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8999210f-60e5-4433-8832-0e20f0ce5eae": {"__data__": {"id_": "8999210f-60e5-4433-8832-0e20f0ce5eae", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\123.txt", "file_name": "123.txt", "file_type": "text/plain", "file_size": 685, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e85ca8c-b81e-4143-a7e2-5e66679a2913", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\123.txt", "file_name": "123.txt", "file_type": "text/plain", "file_size": 685, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "01cc189abbccb92f15243199ab2fbf09f9b5917bdba46e2ce7df0e27598230b6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8edd86e4-b1ed-4738-8bd9-4e03b664490d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\122.txt", "file_name": "122.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "adc21a872cbf9b14667a750dd28f93036b5129926ea8577ff6a9afedbaa5eaa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0d71673-7bf9-4f7e-9655-3980e80ae62a", "node_type": "1", "metadata": {}, "hash": "b34120c19aa7548e1d9dc2e24c9a7fe16d27dbadb3084e593c68e6f14c6245c3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: imglab\r\nlink: https://github.com/NaturalIntelligence/imglab\r\ndescription: ImgLab is a web-based tool designed to label images for objects that can be used to train dlib or other object detectors. It is platform independent and runs directly from the browser, requiring minimal CPU and memory. ImgLab features auto suggestion, plugins, different shapes, and keyboard shortcuts, among others. To learn more about how to use ImgLab's features, please check the demo video tutorial/demonstration or the user guide. If you are interested in contributing to ImgLab, please read CONTRIBUTING.md for the details on our code of conduct and the process for submitting pull requests.", "start_char_idx": 0, "end_char_idx": 683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0d71673-7bf9-4f7e-9655-3980e80ae62a": {"__data__": {"id_": "c0d71673-7bf9-4f7e-9655-3980e80ae62a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "367ec997-53eb-4fd9-85e0-5edfc8935240", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "688c67506d094488bfbf4c497c845b7d9c89328d5730160d4f1da4a58d3a5d3b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8999210f-60e5-4433-8832-0e20f0ce5eae", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\123.txt", "file_name": "123.txt", "file_type": "text/plain", "file_size": 685, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3ff1d572a2f46f49ee665331948954c22042388c7cee1ac48971de7d316549b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "894cc91f-110d-4104-a004-129253335b25", "node_type": "1", "metadata": {}, "hash": "7aba27e5f8be7791012f340a94ea16bc54cbc13863ed1a4499a4b9b10660b181", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cvat\r\nlink: https://github.com/opencv/cvat\r\ndescription: CVAT (Computer Vision Annotation Tool) is an interactive video and image annotation tool used by tens of thousands of users and companies around the world. Its mission is to help developers, companies, and organizations solve real problems using the Data-centric AI approach. Apart from hosting and managing packages and finding and fixing vulnerabilities, CVAT offers a range of services that includes automating any workflow, instant dev environments, writing better code with AI, collaborating outside of code, funding open-source developers, and GitHub community articles. CVAT is available online at cvat.ai and as a self-hosted solution with premium features. It offers Enterprise support for self-hosted installations, including SSO, LDAP, Roboflow and HuggingFace integrations, and advanced analytics. If you use CVAT, drop them a line at contact@cvat.ai.", "start_char_idx": 0, "end_char_idx": 931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "894cc91f-110d-4104-a004-129253335b25": {"__data__": {"id_": "894cc91f-110d-4104-a004-129253335b25", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "367ec997-53eb-4fd9-85e0-5edfc8935240", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "688c67506d094488bfbf4c497c845b7d9c89328d5730160d4f1da4a58d3a5d3b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0d71673-7bf9-4f7e-9655-3980e80ae62a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eaa96a89ad8a8b18f1f25761cd3efdb0966de11cb2d641086614947f33d9adb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f092cc31-f987-4770-8a56-cf8f8d8c1584", "node_type": "1", "metadata": {}, "hash": "dbef9a5cd2acc1c273b5399607c8a35a19779a28b264a5aebc37a160b2922af7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cvat\r\nlink: https://github.com/opencv/cvat\r\ndescription: CVAT (Computer Vision Annotation Tool) is an interactive video and image annotation tool used by tens of thousands of users and companies around the world. Its mission is to help developers, companies, and organizations solve real problems using the Data-centric AI approach. Apart from hosting and managing packages and finding and fixing vulnerabilities, CVAT offers a range of services that includes automating any workflow, instant dev environments, writing better code with AI, collaborating outside of code, funding open-source developers, and GitHub community articles. CVAT is available online at cvat.ai and as a self-hosted solution with premium features. It offers Enterprise support for self-hosted installations, including SSO, LDAP, Roboflow and HuggingFace integrations, and advanced analytics. If you use CVAT, drop them a line at contact@cvat.ai. CVAT is used by teams all over the world, including ATLANTIS, an open-source dataset for semantic segmentation of waterbody images, developed by iWERS group in the Department of Civil and Environmental Engineering at the University of South Carolina.", "start_char_idx": 0, "end_char_idx": 1182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f092cc31-f987-4770-8a56-cf8f8d8c1584": {"__data__": {"id_": "f092cc31-f987-4770-8a56-cf8f8d8c1584", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\125.txt", "file_name": "125.txt", "file_type": "text/plain", "file_size": 1055, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73f46aef-770e-4d59-818c-7d15d8e6b2b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\125.txt", "file_name": "125.txt", "file_type": "text/plain", "file_size": 1055, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "87048b1423280e3f3ab691fe28014681c58fb56e3dae1ca6bd22ed0f93c994b9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "894cc91f-110d-4104-a004-129253335b25", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4cb112be1fdd61e19d3565d5f987b98a34068676eaa83a101b580292abacaeff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02e60a50-9761-4ac2-a284-8a7d11f05729", "node_type": "1", "metadata": {}, "hash": "ab0ab96532da87fc342faf0e6b350e7ec3703e19ec6d243835ed55a649f64753", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: doccano\r\nlink: https://github.com/doccano/doccano\r\ndescription: doccano is an open source text annotation tool for humans. It provides.annotation features for text classification, sequence labeling and sequence to.sequence tasks. So, you can create labeled data for sentiment analysis, named.entity recognition, text summarization and so on. Just create a project,.upload data and start annotating. You can build a dataset in hours. You can try the annotation demo and read the documentation at https://doccano.github.io/doccano/. There are three options to run doccano and to install it via pip. By default, SQLite 3 is used for the default database but you can install additional dependencies to use PostgreSQL. After installation, run the necessary commands and create a Docker container. Finally, start doccano by running the container or use the one-click deployment method. Doccano is under continuous development so feel free to file an issue or contribute towards adding new features. For help and feedback, please contact the author.", "start_char_idx": 0, "end_char_idx": 1053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02e60a50-9761-4ac2-a284-8a7d11f05729": {"__data__": {"id_": "02e60a50-9761-4ac2-a284-8a7d11f05729", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\126.txt", "file_name": "126.txt", "file_type": "text/plain", "file_size": 1023, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9ad1924-220e-40f2-bda0-6d0e6eae7aa0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\126.txt", "file_name": "126.txt", "file_type": "text/plain", "file_size": 1023, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7d25eb54d8dd17f22272d3ea009867d76acf07b66e457a3ffec747c37dbe5649", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f092cc31-f987-4770-8a56-cf8f8d8c1584", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\125.txt", "file_name": "125.txt", "file_type": "text/plain", "file_size": 1055, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4017cde8bda419f4ce598836da87bf251e2188e5d27f61740fa305de73d194e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08130c11-d1bb-4f01-a4f4-45675abe7b9d", "node_type": "1", "metadata": {}, "hash": "b5a9371b62ddd635507b0ac98768951855dbfd4a8f5508b0cfe6587e6f6c331f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: setl\r\nlink: https://github.com/SETL-Framework/setl\r\ndescription: If you're a data scientist or data engineer, you might be familiar with working on an ETL project. SETL (pronounced \"settle\") is a Scala ETL framework powered by Apache Spark that helps you structure your Spark ETL projects, modularize your data transformation logic and speed up your development. Automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and read more about the GitHub community articles. In this guide, we will introduce SETL except for its installation. We will cover how to use SETL in a new project, in an existing project, show code examples for context initialization, implementation of factory, and generating a pipeline diagram. We will also cover how to configure the app and dependencies. Finally, you can check the contributing guide and footer navigation for more information.", "start_char_idx": 0, "end_char_idx": 1021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08130c11-d1bb-4f01-a4f4-45675abe7b9d": {"__data__": {"id_": "08130c11-d1bb-4f01-a4f4-45675abe7b9d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\127.txt", "file_name": "127.txt", "file_type": "text/plain", "file_size": 978, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9ae78a1-ffdf-4399-843d-6ce5b5247d7c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\127.txt", "file_name": "127.txt", "file_type": "text/plain", "file_size": 978, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6c44b64fa58438265613dfcd0ade31dba8d524be7ed58790a48993bfb3e6dbef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02e60a50-9761-4ac2-a284-8a7d11f05729", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\126.txt", "file_name": "126.txt", "file_type": "text/plain", "file_size": 1023, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bbc21462cbdbbecc05b39c36f1e8267acfbdb9044906e0b1c250ce46a3c4b7e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23d2654f-051e-479c-a882-9f2c8ccb80f8", "node_type": "1", "metadata": {}, "hash": "dd43e6ea7dab783d0640d3e07ea1d9c3f5b51bdb33d3a0ece236db516bef4cf6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: coco-annotator\r\nlink: https://github.com/jsbroks/coco-annotator\r\ndescription: 'COCO Annotator is a web-based image annotation tool designed for versatility and efficiently label images to create training data for image localization and object detection. It provides many distinct features including the ability to label an image segment (or part of a segment), track object instances, labeling objects with disconnected visible parts, efficiently storing and export annotations in the well-known COCO format. The annotation process is delivered through an intuitive and customizable interface and provides many tools for creating accurate datasets. Several annotation tools are currently available, with most applications as a desktop installation. _COCO Annotator_ allows users to annotate images using free-form curves or polygons and provides many additional features were other annotations tool fall short. For examples and more information check out the wiki.'", "start_char_idx": 0, "end_char_idx": 976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23d2654f-051e-479c-a882-9f2c8ccb80f8": {"__data__": {"id_": "23d2654f-051e-479c-a882-9f2c8ccb80f8", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\128.txt", "file_name": "128.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3990527f-3cec-47ae-82a2-741edf3041bb", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\128.txt", "file_name": "128.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a4e5553c14a31a282b7d1ce6dd4d01c521b11c11850d9b66130f2f9c42cf1321", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08130c11-d1bb-4f01-a4f4-45675abe7b9d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\127.txt", "file_name": "127.txt", "file_type": "text/plain", "file_size": 978, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d5ff62fc488b1936b410aa768217f28590eb7572f3bcfe6681ce9d54b4ca6721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "089cb881-ea18-4d61-9e5f-f5d43c0de8f0", "node_type": "1", "metadata": {}, "hash": "a8a9716b62e4efe152802611840a17ca0a9b2cc995c343ef6519c33143f4d0f3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pipelinex\r\nlink: https://github.com/Minyus/pipelinex\r\ndescription:", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "089cb881-ea18-4d61-9e5f-f5d43c0de8f0": {"__data__": {"id_": "089cb881-ea18-4d61-9e5f-f5d43c0de8f0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\129.txt", "file_name": "129.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "baafa8fc-3198-4410-8c79-7c0c121b2935", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\129.txt", "file_name": "129.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9859ef5eb1805d44c7e90cc1f160576b1092f6bec9e6d7474d754b4c54f72ca9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23d2654f-051e-479c-a882-9f2c8ccb80f8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\128.txt", "file_name": "128.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c84aafa88d56fe101e2f018652bbbb2f191ecb3b2658ffd1b8a3aaa94cf17366", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fbf122a-b3c9-48a1-8a27-4a7e977a1a03", "node_type": "1", "metadata": {}, "hash": "cfcaa85ce07ea54499f6d6adf93586ac3c3012a2a70b832e88619afd8bd49100", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: prefect\r\nlink: https://github.com/PrefectHQ/prefect\r\ndescription: From the Latin _praefectus_, meaning \"one who is in charge\", a prefect is an official who oversees a domain and makes sure that the rules are followed. Similarly, Prefect is responsible for making sure that workflows execute properly. It also happens to be the name of a roving researcher for that wholly remarkable book, _The Hitchhiker's Guide to the Galaxy_. Thanks to Prefect's growing task library and deep ecosystem integrations, building data applications is easier than ever.", "start_char_idx": 0, "end_char_idx": 560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fbf122a-b3c9-48a1-8a27-4a7e977a1a03": {"__data__": {"id_": "5fbf122a-b3c9-48a1-8a27-4a7e977a1a03", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\13.txt", "file_name": "13.txt", "file_type": "text/plain", "file_size": 793, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7212107f-33c2-4ab2-9949-7116da9c38a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\13.txt", "file_name": "13.txt", "file_type": "text/plain", "file_size": 793, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b3b61727a02ce33b0e9d4f1e82748eaa39a76bb5a6b9335ac84367b865a13c6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "089cb881-ea18-4d61-9e5f-f5d43c0de8f0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\129.txt", "file_name": "129.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "18097e6b28c402b88aa61d5644e3fdbe37738d6c989eeb743e0048e02eac757d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ca150dc-dd0f-4f9f-8cba-1de45bf3294b", "node_type": "1", "metadata": {}, "hash": "5cfee481e03df2bc0f1c4380f187034595bb7b763c550aaae0681be2c1182adf", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: NNEF-Tools\r\nlink: https://github.com/KhronosGroup/NNEF-Tools\r\ndescription: NNEF-Tools is a repository containing various tools, such as a parser in C++ and Python, for generating and consuming NNEF documents. This enables a diverse range of devices and platforms to use a rich mix of neural network training tools and inference engines, thus reducing machine learning deployment fragmentation. Among the tools in the repository are converters for deep learning frameworks and pre-trained models in TensorFlow, Caffe, Caffe2, and ONNX to NNEF format, with a Model Zoo containing NNEF models converted from various sources. The NNEF-Tools has recently undergone spec version updates, various bug fixes, and general tool reworking, and provides a thorough readme for using the tools.", "start_char_idx": 0, "end_char_idx": 791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ca150dc-dd0f-4f9f-8cba-1de45bf3294b": {"__data__": {"id_": "3ca150dc-dd0f-4f9f-8cba-1de45bf3294b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\130.txt", "file_name": "130.txt", "file_type": "text/plain", "file_size": 1053, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59f980eb-bd73-4503-8be4-33e3ebd5e985", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\130.txt", "file_name": "130.txt", "file_type": "text/plain", "file_size": 1053, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b6227d349dc635ff620389e3ea4899dd3b01936648a2e445338ba1b8318d88a6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fbf122a-b3c9-48a1-8a27-4a7e977a1a03", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\13.txt", "file_name": "13.txt", "file_type": "text/plain", "file_size": 793, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0fadd85f64e582cce39c4cd97665280a028fe64d124ab7c9c924ffb8779d28f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3691e635-ea60-4d2c-9ed7-35137dd55fed", "node_type": "1", "metadata": {}, "hash": "c953cb88c680db2f9aec155609acd91060d1ab8ac4cca19adba88720901361a5", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: metaflow\r\nlink: https://github.com/Netflix/metaflow\r\ndescription: Metaflow is a human-friendly library that helps scientists and engineers build and manage real-life data science projects. Metaflow provides a simple, friendly API that covers foundational needs of ML, AI, and data science projects. It was originally developed at Netflix to boost productivity of data scientists who work on a wide variety of projects from classical statistics to state-of-the-art deep learning. What sets Metaflow apart is its ability to help users go from prototype to production and back seamlessly. While you can get started with Metaflow easily on your laptop, the main benefits of Metaflow lie in its ability to scale out to external compute clusters and to deploy to production-grade workflow orchestrators. Additionally, there is an active community of thousands of data scientists and ML engineers discussing the ins-and-outs of applied machine learning. For more information and installation instructions, see Metaflow's website and documentation.", "start_char_idx": 0, "end_char_idx": 1051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3691e635-ea60-4d2c-9ed7-35137dd55fed": {"__data__": {"id_": "3691e635-ea60-4d2c-9ed7-35137dd55fed", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\131.txt", "file_name": "131.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb7d2daa-0898-48dd-bfb3-a034cf33a7b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\131.txt", "file_name": "131.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9bf2f0b4e4dcaff01c6a5c901b99ae352e7f26335b08e3101a1727a850c71019", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ca150dc-dd0f-4f9f-8cba-1de45bf3294b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\130.txt", "file_name": "130.txt", "file_type": "text/plain", "file_size": 1053, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "122dec58184204f28744c6095bd075e9bd9aee397b7483f1fd38d069e810551a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbb7aa70-540a-4167-bc7e-685b18e878ab", "node_type": "1", "metadata": {}, "hash": "eb02089426bc727d6871dc29c57a3b02ea2a6e3e1f85473fd3598fa144e3bebb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: Neuraxle\r\nlink: https://github.com/Neuraxio/Neuraxle\r\ndescription:", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbb7aa70-540a-4167-bc7e-685b18e878ab": {"__data__": {"id_": "bbb7aa70-540a-4167-bc7e-685b18e878ab", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\132.txt", "file_name": "132.txt", "file_type": "text/plain", "file_size": 875, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e765345f-1a30-4e05-8cb5-af9b246e3f8b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\132.txt", "file_name": "132.txt", "file_type": "text/plain", "file_size": 875, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "855f8f0be0403ad7ab18ec80f517d8af31f277bc49a297bd9a69d26b0d780486", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3691e635-ea60-4d2c-9ed7-35137dd55fed", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\131.txt", "file_name": "131.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a47f1c883945b485531208727cf34d45e3a8b6be7a652c222289ec5e6c580b5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cacedebd-a22f-4f68-a04d-33daba44a92d", "node_type": "1", "metadata": {}, "hash": "6dbe8b94abb552ff5a50617d514df3ccfcbef7abf197d97c9edfa76e53d4b27c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: kedro\r\nlink: https://github.com/kedro-org/kedro\r\ndescription: Kedro is an open-source Python framework to create reproducible, maintainable, and modular data science code. It uses software engineering best practices to help you build production-ready data engineering and data science pipelines. Kedro is hosted by the LF AI & Data Foundation. The Kedro documentation first explains how to install Kedro and then introduces key Kedro concepts. For new and intermediate Kedro users, there's a comprehensive section on how to visualise Kedro projects using Kedro-Viz and how to work with Kedro and Jupyter notebooks. The Kedro product team and a number of open-source contributors from across the world maintain Kedro. There is a growing community around Kedro. Have a look at the Kedro FAQs to find projects using Kedro and links to articles, podcasts, and talks.", "start_char_idx": 0, "end_char_idx": 873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cacedebd-a22f-4f68-a04d-33daba44a92d": {"__data__": {"id_": "cacedebd-a22f-4f68-a04d-33daba44a92d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\133.txt", "file_name": "133.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db4fc2c8-86d1-4f48-b0f1-02a702b3aa3c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\133.txt", "file_name": "133.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7f6f11b96e6b1d78a7cdce0365f4e159f068dcd79fe155b85113459296403252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbb7aa70-540a-4167-bc7e-685b18e878ab", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\132.txt", "file_name": "132.txt", "file_type": "text/plain", "file_size": 875, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "052937263bd87de40a9b2c917f3d0cda86d140e9031bf64737192fca92ec0c56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb098947-0625-4970-8a89-eccdc3f10e27", "node_type": "1", "metadata": {}, "hash": "080ff6baceb5357e7d08d7fbd4c48bdcf375721620c5768a713aca1e6dc69553", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: luigi\r\nlink: https://github.com/spotify/luigi\r\ndescription:", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb098947-0625-4970-8a89-eccdc3f10e27": {"__data__": {"id_": "fb098947-0625-4970-8a89-eccdc3f10e27", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\134.txt", "file_name": "134.txt", "file_type": "text/plain", "file_size": 661, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b43f6d2c-aa71-4cc2-bef0-0c9f7a0b9cdf", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\134.txt", "file_name": "134.txt", "file_type": "text/plain", "file_size": 661, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a97ceed5aaee739cf54da85f84c499617e750bebcfdb2e75a74cd5380049e0e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cacedebd-a22f-4f68-a04d-33daba44a92d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\133.txt", "file_name": "133.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7c4153be7af765b510929bc14cdeb86c83fddc3b91d70a9681583192f4dc2639", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "728b8825-0bf0-4a95-a1f9-0cbccee398db", "node_type": "1", "metadata": {}, "hash": "2a87433b5de418d6221b804ba5bf2c1cd8358b13d3fc2ab637eac57e26336590", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: genie\r\nlink: https://github.com/Netflix/genie\r\ndescription: \"Genie is a federated Big Data orchestration and execution engine developed by Netflix. Genie is designed to sit at the boundary of these two worlds, and simplify the lives of people on either side. A data scientist can \u201crub the magic lamp\u201d and just say \u201cGenie, run query \u2018Q\u2019. using engine SparkSQL against production data\u201d. Genie takes care of all the nitty-gritty details. It dynamically assembles the necessary binaries and configurations, execute the job, monitors it, notifies the user of its completion, and makes the output data available for immediate and future use.\"", "start_char_idx": 0, "end_char_idx": 647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "728b8825-0bf0-4a95-a1f9-0cbccee398db": {"__data__": {"id_": "728b8825-0bf0-4a95-a1f9-0cbccee398db", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\135.txt", "file_name": "135.txt", "file_type": "text/plain", "file_size": 356, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "122052c7-8a0c-4c26-ae97-7309864955b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\135.txt", "file_name": "135.txt", "file_type": "text/plain", "file_size": 356, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "86f947381e7f7df98d275ece0cf05c38e1d50c35a50b864a078041fa85e15d23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb098947-0625-4970-8a89-eccdc3f10e27", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\134.txt", "file_name": "134.txt", "file_type": "text/plain", "file_size": 661, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e6fe4c2cfc6d776132e7c35a4c41a1742dc2314d9d185f20232b21d9b615ba8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a216bc2-0426-4aa7-bcea-5856638efc00", "node_type": "1", "metadata": {}, "hash": "718a5df509e8396d94ae7251f6906b60129d01ae599598b2ccd75828f8770b49", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: gokart\r\nlink: https://github.com/m3dev/gokart\r\ndescription: \"Gokart solves reproducibility, task dependencies, constraints of good code, and ease of use for Machine Learning Pipeline. **All the functions above are created for constructing Machine Learning batches. Provides an excellent environment for reproducibility and team development.**\"", "start_char_idx": 0, "end_char_idx": 354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a216bc2-0426-4aa7-bcea-5856638efc00": {"__data__": {"id_": "9a216bc2-0426-4aa7-bcea-5856638efc00", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\136.txt", "file_name": "136.txt", "file_type": "text/plain", "file_size": 332, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7de105af-319a-4cee-96d2-dfa5b88cf140", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\136.txt", "file_name": "136.txt", "file_type": "text/plain", "file_size": 332, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3c5936378685e43785e50f36dbf28d39cccd7615cd3bf88b56d81eae027bedb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "728b8825-0bf0-4a95-a1f9-0cbccee398db", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\135.txt", "file_name": "135.txt", "file_type": "text/plain", "file_size": 356, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "021de772c329b96182b288ea6d1d8f3e3ede776aa10b8c0150bdc30f00703e14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e289446f-61ad-44f1-8b85-f3855bb569a9", "node_type": "1", "metadata": {}, "hash": "b59c2a351d754a90506916d31e958e98826955f2b5488cfdb34b592f9ff970f8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: dagster\r\nlink: https://github.com/dagster-io/dagster\r\ndescription: Dagster is a powerful tool that allows you to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and more.", "start_char_idx": 0, "end_char_idx": 330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e289446f-61ad-44f1-8b85-f3855bb569a9": {"__data__": {"id_": "e289446f-61ad-44f1-8b85-f3855bb569a9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7f361e-3568-45ec-b415-6135472503f8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "14e4731e27e7fb69eac1574a6238cf900413b23f30f3deee1982e098b5dec223", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a216bc2-0426-4aa7-bcea-5856638efc00", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\136.txt", "file_name": "136.txt", "file_type": "text/plain", "file_size": 332, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "56fc5e1b21eb80c4c6317ee42c1048039308ec5cd9b828c0383937fcbb7c13cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5feffe8c-e4f6-433d-bc4b-128f9da33b58", "node_type": "1", "metadata": {}, "hash": "ff46601a8309e9fd53d383644a80b5a06e9b5e4d29e8b013818e98e21658837b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: chronos\r\nlink: https://github.com/mesos/chronos\r\ndescription: Chronos is a replacement for `cron`. It is a distributed and fault-tolerant.scheduler that runs on top of Apache Mesos that can be used for job.orchestration. It supports custom Mesos executors as well as the default.command executor. Thus by default, Chronos executes `sh` (on most systems.bash) scripts. Chronos can be used to interact with systems such as Hadoop (incl. EMR), even.if the Mesos agents on which execution happens do not have Hadoop installed..Included wrapper scripts allow transfering files and executing them on a.remote machine in the background and using asynchronous callbacks to notify.Chronos of job completion or failures. Chronos is also natively able to.schedule jobs that run inside Docker containers. Chronos has a number of advantages over regular cron.", "start_char_idx": 0, "end_char_idx": 857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5feffe8c-e4f6-433d-bc4b-128f9da33b58": {"__data__": {"id_": "5feffe8c-e4f6-433d-bc4b-128f9da33b58", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7f361e-3568-45ec-b415-6135472503f8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "14e4731e27e7fb69eac1574a6238cf900413b23f30f3deee1982e098b5dec223", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e289446f-61ad-44f1-8b85-f3855bb569a9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7007d34b1454f1a73c461468fa4ebaf63ed3085b9199860ca02423c387b5ae93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "670b3b83-d006-468d-99c5-d0fb1f101eda", "node_type": "1", "metadata": {}, "hash": "5fa2d69175cfcdf1a3667a3d31ea7420e46864803ab9f1511766b5b840b573d3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: chronos\r\nlink: https://github.com/mesos/chronos\r\ndescription: Chronos is a replacement for `cron`. It is a distributed and fault-tolerant.scheduler that runs on top of Apache Mesos that can be used for job.orchestration. It supports custom Mesos executors as well as the default.command executor. Thus by default, Chronos executes `sh` (on most systems.bash) scripts. Chronos can be used to interact with systems such as Hadoop (incl. EMR), even.if the Mesos agents on which execution happens do not have Hadoop installed..Included wrapper scripts allow transfering files and executing them on a.remote machine in the background and using asynchronous callbacks to notify.Chronos of job completion or failures. Chronos is also natively able to.schedule jobs that run inside Docker containers. Chronos has a number of advantages over regular cron. It allows you to.schedule your jobs using ISO8601 repeating interval notation, which enables.more flexibility in job scheduling.", "start_char_idx": 0, "end_char_idx": 986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "670b3b83-d006-468d-99c5-d0fb1f101eda": {"__data__": {"id_": "670b3b83-d006-468d-99c5-d0fb1f101eda", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7f361e-3568-45ec-b415-6135472503f8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "14e4731e27e7fb69eac1574a6238cf900413b23f30f3deee1982e098b5dec223", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5feffe8c-e4f6-433d-bc4b-128f9da33b58", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cf31c50799ec01b9f2ae8cccd24ed54bd5cdc594e7d9690ab8e57fadca70e31b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adf58f5c-3667-4a70-a67d-eafd6ebc7c38", "node_type": "1", "metadata": {}, "hash": "0c83d2d86aeec4b260757c3cdf025c60e75857c01cfeb8683386fa79a8a4ebb7", "class_name": "RelatedNodeInfo"}}, "text": "It is a distributed and fault-tolerant.scheduler that runs on top of Apache Mesos that can be used for job.orchestration. It supports custom Mesos executors as well as the default.command executor. Thus by default, Chronos executes `sh` (on most systems.bash) scripts. Chronos can be used to interact with systems such as Hadoop (incl. EMR), even.if the Mesos agents on which execution happens do not have Hadoop installed..Included wrapper scripts allow transfering files and executing them on a.remote machine in the background and using asynchronous callbacks to notify.Chronos of job completion or failures. Chronos is also natively able to.schedule jobs that run inside Docker containers. Chronos has a number of advantages over regular cron. It allows you to.schedule your jobs using ISO8601 repeating interval notation, which enables.more flexibility in job scheduling. Chronos also supports the definition of.jobs triggered by the completion of other jobs. It supports arbitrarily long.dependency chains.", "start_char_idx": 110, "end_char_idx": 1122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adf58f5c-3667-4a70-a67d-eafd6ebc7c38": {"__data__": {"id_": "adf58f5c-3667-4a70-a67d-eafd6ebc7c38", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b7f361e-3568-45ec-b415-6135472503f8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "14e4731e27e7fb69eac1574a6238cf900413b23f30f3deee1982e098b5dec223", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "670b3b83-d006-468d-99c5-d0fb1f101eda", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "179a5ed4e629f48c603d03097bf54bc5a976a679f8de77c74caa24264f31066f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e8c09c3-c8f2-4b79-b176-943d00c715ec", "node_type": "1", "metadata": {}, "hash": "2a86f4c4202ff841a7195c3c93c16e4cf4f71d46f99d611f5b724d5eefd97a43", "class_name": "RelatedNodeInfo"}}, "text": "It supports custom Mesos executors as well as the default.command executor. Thus by default, Chronos executes `sh` (on most systems.bash) scripts. Chronos can be used to interact with systems such as Hadoop (incl. EMR), even.if the Mesos agents on which execution happens do not have Hadoop installed..Included wrapper scripts allow transfering files and executing them on a.remote machine in the background and using asynchronous callbacks to notify.Chronos of job completion or failures. Chronos is also natively able to.schedule jobs that run inside Docker containers. Chronos has a number of advantages over regular cron. It allows you to.schedule your jobs using ISO8601 repeating interval notation, which enables.more flexibility in job scheduling. Chronos also supports the definition of.jobs triggered by the completion of other jobs. It supports arbitrarily long.dependency chains. The easiest way to use Chronos is to useDC/OS and install chronos via the.universe.", "start_char_idx": 232, "end_char_idx": 1206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e8c09c3-c8f2-4b79-b176-943d00c715ec": {"__data__": {"id_": "2e8c09c3-c8f2-4b79-b176-943d00c715ec", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\138.txt", "file_name": "138.txt", "file_type": "text/plain", "file_size": 784, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f692231-45d4-4f4b-a63b-b06028d512e9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\138.txt", "file_name": "138.txt", "file_type": "text/plain", "file_size": 784, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "17dd3189273ec415b3331755c5a0eb720b650c62a0f60ed3cc3ea2c7a9d80637", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adf58f5c-3667-4a70-a67d-eafd6ebc7c38", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "653be441eeb4b2133cd942576fc967bcb0aa0512cd3910974e19e107299a11ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58e0b3b0-7348-4cef-8884-86ed8ff27651", "node_type": "1", "metadata": {}, "hash": "a34787f65da3d4fd8bd523318508edcc1c1e3cfa217e539124599017190199eb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: couler\r\nlink: https://github.com/couler-proj/couler\r\ndescription: Couler is a platform that aims to provide a unified interface for constructing and managing workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow. Couler is included in CNCF Cloud Native Landscape and LF AI Landscape. It currently only supports Argo Workflows as the workflow orchestration backend. Couler provides Python SDK that provides access to all the available features from Argo Workflows. To install Couler, use \"python3 -m pip install git+https://github.com/couler-proj/couler --ignore-installed\" or clone the repository and run \"python setup.py install\". Couler also provides examples and tutorials on how to use the platform with Argo Workflows.", "start_char_idx": 0, "end_char_idx": 782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58e0b3b0-7348-4cef-8884-86ed8ff27651": {"__data__": {"id_": "58e0b3b0-7348-4cef-8884-86ed8ff27651", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\139.txt", "file_name": "139.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3e56570-1c06-4ee2-8547-00212530db25", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\139.txt", "file_name": "139.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "db0762a87badcdecdf50b9b23c1434ff49ce538dd232a59d1485a746c52205ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e8c09c3-c8f2-4b79-b176-943d00c715ec", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\138.txt", "file_name": "138.txt", "file_type": "text/plain", "file_size": 784, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2374b1498ab30d917cf01e2f0ca44c9762f4693a42e421579dbc0f6b69ee2342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45f9c967-3361-4118-9a2d-9141eaacc236", "node_type": "1", "metadata": {}, "hash": "9552828be1c3c2dc463e0d838d66cf557ee661acf51b84e94dd29da0acad01b4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: bonobo\r\nlink: https://github.com/python-bonobo/bonobo\r\ndescription:", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45f9c967-3361-4118-9a2d-9141eaacc236": {"__data__": {"id_": "45f9c967-3361-4118-9a2d-9141eaacc236", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\14.txt", "file_name": "14.txt", "file_type": "text/plain", "file_size": 288, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9f35f6d-bcd8-4023-b9d2-e0398bf942f8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\14.txt", "file_name": "14.txt", "file_type": "text/plain", "file_size": 288, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "927aa909392d704a263198484f9c44726cfc3efdac4df8671d5951359445ac25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58e0b3b0-7348-4cef-8884-86ed8ff27651", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\139.txt", "file_name": "139.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "14379fc514bba60b10a03e250bb9a220d418e1b860e1da757c6834dbcbcc1433", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0784aad8-4ec9-4654-b590-73023b0eadf6", "node_type": "1", "metadata": {}, "hash": "0ccb5f7462836ac270437f7cc0cbaa94da6502fe168692b092a5c40f67edb788", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pywren\r\nlink: https://github.com/pywren/pywren\r\ndescription: PyWren is a tool that provides a mini-condor like cloud-based solution for complex calls, capable of up to 40 TFLOPS peak from AWS Lambda. This is the development site, and you can learn more about it at pywren.io.", "start_char_idx": 0, "end_char_idx": 286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0784aad8-4ec9-4654-b590-73023b0eadf6": {"__data__": {"id_": "0784aad8-4ec9-4654-b590-73023b0eadf6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\140.txt", "file_name": "140.txt", "file_type": "text/plain", "file_size": 936, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cb2a6f8-ba4d-4f32-bc2b-43fcb62583cd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\140.txt", "file_name": "140.txt", "file_type": "text/plain", "file_size": 936, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2d99df50af2eb9e38ca60f2d07d66d7e813a3b62105674ec573bc3469f2b5cca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45f9c967-3361-4118-9a2d-9141eaacc236", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\14.txt", "file_name": "14.txt", "file_type": "text/plain", "file_size": 288, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a223014f6d919db5dc2ba04feb82c9af814ebb2bc7d72272ab601e061edbb711", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "060a5051-5217-4504-b8a3-82a61fe369d6", "node_type": "1", "metadata": {}, "hash": "f1c80afc8586a17795d8c5967aea5f41494a2e93e85fa3efb6a48c6d13299bb6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: basin\r\nlink: https://github.com/basin-etl/basin\r\ndescription: Basin is a visual programming tool that allows users to extract, transform, and load data using a drag-and-drop interface. It can run Spark jobs on any environment and can be used to create complex pipelines and flows. Users can debug and preview their work step by step with an integrated dataview grid viewer and can export their work to pure Python code. Basin can be installed from dockerhub, and a data folder must be created to hold all input and output files. One can run the image by mapping the data directory to a local environment and pointing a browser to http://localhost:3000. Basin can also be installed from source by setting up two containers, \"basin-client\" and \"basin-server\". To create new block types, users can define a code template using the C code library template, and the program is free software under the Server Side Public License.", "start_char_idx": 0, "end_char_idx": 934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "060a5051-5217-4504-b8a3-82a61fe369d6": {"__data__": {"id_": "060a5051-5217-4504-b8a3-82a61fe369d6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\141.txt", "file_name": "141.txt", "file_type": "text/plain", "file_size": 700, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b34569e-b2e0-49ba-b688-14f3ab9c1a9f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\141.txt", "file_name": "141.txt", "file_type": "text/plain", "file_size": 700, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3e78d9699634920b811956fa4d9af06f28c305f7a7f05ec1546c4ea4e687b743", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0784aad8-4ec9-4654-b590-73023b0eadf6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\140.txt", "file_name": "140.txt", "file_type": "text/plain", "file_size": 936, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d4a8a57466a11b1aeabeaa251d7ce1bee8ca3ef560d827f61d5064ff5381a829", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb13c929-acb8-4349-beca-024cea5a60fb", "node_type": "1", "metadata": {}, "hash": "5fca415deeadafb74360a6cef4beb317088fbf64657b135f966547173377cc26", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: azkaban\r\nlink: https://github.com/azkaban/azkaban\r\ndescription: Azkaban is a platform that allows users to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. Users can access GitHub community articles to get help, but should note that the documentation is in development. Azkaban builds use Gradle and requires Java 8 or higher. The platform can be installed using a set of commands that run on *nix platforms like Linux, OS X. It is important to note that creating certain branches may cause unexpected behavior, and that contributors cannot currently be retrieved.", "start_char_idx": 0, "end_char_idx": 698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb13c929-acb8-4349-beca-024cea5a60fb": {"__data__": {"id_": "eb13c929-acb8-4349-beca-024cea5a60fb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\142.txt", "file_name": "142.txt", "file_type": "text/plain", "file_size": 1008, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89aa0982-d92b-435c-b00b-131dde63e9b6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\142.txt", "file_name": "142.txt", "file_type": "text/plain", "file_size": 1008, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9b2583104bdb35a43ace73d4e8670c99c2a465974da43cae3b7cbaa17f614629", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "060a5051-5217-4504-b8a3-82a61fe369d6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\141.txt", "file_name": "141.txt", "file_type": "text/plain", "file_size": 700, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8a6108795437cc062b496f50d63707225a6726fa552f1a2dca17747076a427e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e652061-42f8-49b0-92ca-ffa4cd3bf00c", "node_type": "1", "metadata": {}, "hash": "90940d2c811c04785e75c8a63005b622a3e714c06e7e4c27574e7fde09940c22", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: nifi\r\nlink: https://github.com/apache/nifi\r\ndescription: Apache NiFi is an easy to use, powerful, and reliable system to process and distribute data. It was made for dataflow and supports highly configurable directed graphs of data routing, transformation, and system mediation logic. Some of its key features include automating workflows, hosting and managing packages, finding and fixing vulnerabilities, providing instant dev environments, enabling AI to optimize code writing, and facilitating collaboration outside of code. For getting started with NiFi, one can execute 'mvn clean install -DskipTests' to compile tests but skip running them, and direct the browser to http://localhost:8080/nifi/ to see the screen. NiFi User Guide provides help to build the first data flow. For any help, questions can be asked on Apache NiFi Slack Workspace and IRC. To submit a bug report, one can file a Jira. NiFi is licensed under the Apache License, Version 2.0, and includes cryptographic software.", "start_char_idx": 0, "end_char_idx": 1006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e652061-42f8-49b0-92ca-ffa4cd3bf00c": {"__data__": {"id_": "0e652061-42f8-49b0-92ca-ffa4cd3bf00c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\143.txt", "file_name": "143.txt", "file_type": "text/plain", "file_size": 668, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "892183e9-fea9-4b7d-ad31-6ea876f5a171", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\143.txt", "file_name": "143.txt", "file_type": "text/plain", "file_size": 668, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f4af595d1e366c42b7261ce3ed31d8208e0c34108338dc1f70085f4be24d9480", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb13c929-acb8-4349-beca-024cea5a60fb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\142.txt", "file_name": "142.txt", "file_type": "text/plain", "file_size": 1008, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "28460262d5cefba3ecc3a623dbcef7bb8b48dcd2448d73bd42b434dc55640ed5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5d8630e-70a9-489f-99f8-27a214bfebd4", "node_type": "1", "metadata": {}, "hash": "efd32c4b1e4300b5f5445c43e6b1968f59e91de87c894770718878516612d888", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: argo-workflows\r\nlink: https://github.com/argoproj/argo-workflows\r\ndescription: Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition). It has many features such as the ability to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. Argo Workflows is used by about 200+ organizations and many projects. The project is governed by the CNCF Code of Conduct and has monthly community meetings and blogs.", "start_char_idx": 0, "end_char_idx": 666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5d8630e-70a9-489f-99f8-27a214bfebd4": {"__data__": {"id_": "b5d8630e-70a9-489f-99f8-27a214bfebd4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\144.txt", "file_name": "144.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5659ed1c-d7a7-42bf-b884-e9e048361786", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\144.txt", "file_name": "144.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fe07364b3138029b552f3b3bda3476ff9e6c83c714f836e12635f616c0031922", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e652061-42f8-49b0-92ca-ffa4cd3bf00c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\143.txt", "file_name": "143.txt", "file_type": "text/plain", "file_size": 668, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e6f6f2cee9762a94d381d91b3c5c0b885358b5d27b146e80d9dd71089b0290", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d07b641f-711d-48e3-9d1a-69b93eca2b4a", "node_type": "1", "metadata": {}, "hash": "2800ff4e37173dcfa0dbc07e3e744276dcadc29834a0a302d38d69a1540138a6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: transformers\r\nlink: https://github.com/huggingface/transformers\r\ndescription: \ud83e\udd17 Transformers provides state-of-the-art machine learning models for performing tasks on text, vision, and audio modalities. These transformer models can perform tasks on several combined modalities such as table question answering, optical character recognition, video classification, and visual question answering. The APIs provided by \ud83e\udd17 Transformers allow for the quick download and use of pre-trained models on a given text, for fine-tuning them on your own datasets, and for sharing them with the community on their model hub. This library is backed by the three most popular deep learning libraries \u2014Jax, PyTorch and TensorFlow, with a seamless integration between them. They also provide custom support for their APIs and models.", "start_char_idx": 0, "end_char_idx": 825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d07b641f-711d-48e3-9d1a-69b93eca2b4a": {"__data__": {"id_": "d07b641f-711d-48e3-9d1a-69b93eca2b4a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\145.txt", "file_name": "145.txt", "file_type": "text/plain", "file_size": 997, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "09380548-18aa-4184-ab0c-2454dd0927dd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\145.txt", "file_name": "145.txt", "file_type": "text/plain", "file_size": 997, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "37e01919dcc2302cd975f0b7c0db8b0e4eeb1f25b1f4854eb8f1d7e6f993e647", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5d8630e-70a9-489f-99f8-27a214bfebd4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\144.txt", "file_name": "144.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b232f77c55f5484ae4a8fe37a1673f8ec72ca54836d7cd2b4cb0930924378924", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ae764a0-a39b-41e6-b96f-f0deb4dec986", "node_type": "1", "metadata": {}, "hash": "a742fb5e4ef3b92adf3a509db1dcefbd84f5a241a13f551dc2c8dade1809eede", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: YouTokenToMe\r\nlink: https://github.com/VKCOM/YouTokenToMe\r\ndescription: YouTokenToMe is an unsupervised text tokenizer that focuses on computational efficiency. It currently implements fast Byte Pair Encoding (BPE) and is much faster in training and tokenization than Hugging Face, fastBPE, and SentencePiece. YouTokenToMe's implementation is also much faster in some test cases - up to 60 times faster. It offers a Python interface for installation alongside many other features such as hosting and managing packages, finding and fixing vulnerabilities, instant dev environments, and collaborating outside of code. Users can also fund open source developers and access GitHub community articles. YouTokenToMe's BPE model allows users to handle tokenization of subwords as well as word boundaries. It also provides a command line interface with features such as training BPE models based on text files, applying BPE encoding for a corpus of sentences, and converting IDs back to text.", "start_char_idx": 0, "end_char_idx": 995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ae764a0-a39b-41e6-b96f-f0deb4dec986": {"__data__": {"id_": "6ae764a0-a39b-41e6-b96f-f0deb4dec986", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\146.txt", "file_name": "146.txt", "file_type": "text/plain", "file_size": 1054, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36a12fe1-c86e-4bc5-996a-f7d09ee9fee9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\146.txt", "file_name": "146.txt", "file_type": "text/plain", "file_size": 1054, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "90a2dd1938e00da877e47dd38eac259a2a54fe086af2445b6e38e719415824ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d07b641f-711d-48e3-9d1a-69b93eca2b4a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\145.txt", "file_name": "145.txt", "file_type": "text/plain", "file_size": 997, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "963a84e941e48487f39bd1fa63b8a680ac17c9c5fb74ec949bce187b4c6fbe3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14cfd544-b38e-4c64-89ad-2edebe155068", "node_type": "1", "metadata": {}, "hash": "5d149f2825343ec7610b6d41ad40bf02465342eff742f543c93f94c49e8f1cfb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lingvo\r\nlink: https://github.com/tensorflow/lingvo\r\ndescription: Lingvo is a framework for building neural networks in Tensorflow, particularly sequence models. It offers a variety of features, such as automating workflows, hosting and managing packages, finding and fixing vulnerabilities, providing instant dev environments, using AI to write better code, collaborative tools, and funding open source developers. Lingvo has seen multiple releases, but note that there have been major breaking changes, and older release details are unavailable. To set up Lingvo, you can either install a fixed version through pip or clone the repository and build it with bazel - docker configurations are provided for both cases. Once set up, you can run various models, such as the MNIST image model, machine translation model, GShard transformer based giant language model, or the 3D object detection model. To develop the framework further and potentially contribute pull requests, it is recommended that you clone the repository instead of using pip.", "start_char_idx": 0, "end_char_idx": 1052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14cfd544-b38e-4c64-89ad-2edebe155068": {"__data__": {"id_": "14cfd544-b38e-4c64-89ad-2edebe155068", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\147.txt", "file_name": "147.txt", "file_type": "text/plain", "file_size": 782, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "affb6e22-ed28-418a-9f50-05c93afd0763", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\147.txt", "file_name": "147.txt", "file_type": "text/plain", "file_size": 782, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f55c543b43536b9027053ec9643041b95ed62538ed0247eb5387f6f07162f007", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ae764a0-a39b-41e6-b96f-f0deb4dec986", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\146.txt", "file_name": "146.txt", "file_type": "text/plain", "file_size": 1054, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cdedc752900c788df338141f5188e8f15c34096ceda957ccc6307d3dfe5d9e07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "02fb04fb-69dd-4584-8f9e-584843f8f772", "node_type": "1", "metadata": {}, "hash": "dc97d14c7a185d04ceed58fcdfc8e3caaeb1c4bca39183254874c5f6bdf3a03a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: text\r\nlink: https://github.com/tensorflow/text\r\ndescription: TensorFlow Text provides a collection of text related classes and ops ready to use with TensorFlow 2.0. The library can perform the preprocessing regularly required by text-based models, and includes other features useful for sequence modeling not provided by core TensorFlow. The benefit of using these ops in your text preprocessing is that they are done in the TensorFlow graph. You do not need to worry about tokenization in training being different than the tokenization at inference, or managing preprocessing scripts. Please visit http://tensorflow.org/text for all documentation. This site includes API docs, guides for working with TensorFlow Text, as well as tutorials for building specific models.", "start_char_idx": 0, "end_char_idx": 780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02fb04fb-69dd-4584-8f9e-584843f8f772": {"__data__": {"id_": "02fb04fb-69dd-4584-8f9e-584843f8f772", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\148.txt", "file_name": "148.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30ce481d-8967-4d3f-832a-c6ade99b46e0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\148.txt", "file_name": "148.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c68e16af802d71c308f21944c291cacecb8a3526de6f2f4c3a364cf314721694", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14cfd544-b38e-4c64-89ad-2edebe155068", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\147.txt", "file_name": "147.txt", "file_type": "text/plain", "file_size": 782, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ebe0d1ac7f9f4a24703d9639937e22b832c4388720973dc8e3217a84457612f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca1bfd86-7c1c-436b-9d01-64d9c9b66a86", "node_type": "1", "metadata": {}, "hash": "a84d744d7358a45f2917345de74db384bd9d8f94c5abf53db21a1c46bafdeb06", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: stable-baselines\r\nlink: https://github.com/hill-a/stable-baselines\r\ndescription: Stable Baselines is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines. These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas and will create good baselines to build projects on top of. It is a fork of OpenAI Baselines with a major structural refactoring and code cleanups. Stable-Baselines also provides RL Baselines Zoo, a collection of pre-trained Reinforcement Learning agents that includes basic scripts for training, evaluating agents, tuning hyperparameters, and recording videos. The package is available for installation using pip and supports Tensorflow versions from 1.8.0 to 1.14.0.", "start_char_idx": 0, "end_char_idx": 800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca1bfd86-7c1c-436b-9d01-64d9c9b66a86": {"__data__": {"id_": "ca1bfd86-7c1c-436b-9d01-64d9c9b66a86", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\149.txt", "file_name": "149.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc9c5841-73b5-4f72-b615-be4b1ba51faa", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\149.txt", "file_name": "149.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a2f9c93052f0ce48b52debcb12da6b21b5aa2d17f8df3a361fcf1b077c358915", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "02fb04fb-69dd-4584-8f9e-584843f8f772", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\148.txt", "file_name": "148.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "45cbcb45fc121ad6096d81b850c3c4f6cd7bdefcc8e3b2a9df604fabf2adc08e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7be92c7b-ea10-4014-9e09-9530b76b92e4", "node_type": "1", "metadata": {}, "hash": "03defaae8f797a345d50e7bf16c5584c25ee00e193a176d2c0983001183d1fe8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: snorkel\r\nlink: https://github.com/snorkel-team/snorkel\r\ndescription: **Programmatically Build and Manage Training Data**\r\n\r\nThe Snorkel project started at Stanford in 2015 with a simple technical bet: that it would increasingly be the training data, not the models, algorithms, or infrastructure, that decided whether a machine learning project succeeded or failed. Given this premise, we set out to explore the radical idea that you could bring mathematical and systems structure to the messy and often entirely manual process of training data creation and management, starting by empowering users to programmatically label, build, and manage training data.", "start_char_idx": 0, "end_char_idx": 669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7be92c7b-ea10-4014-9e09-9530b76b92e4": {"__data__": {"id_": "7be92c7b-ea10-4014-9e09-9530b76b92e4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\15.txt", "file_name": "15.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d13ae74a-8073-453b-8099-5558a4293888", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\15.txt", "file_name": "15.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cce7c16c770945969e93174395dc31272b6ce398c8b70ea4412e3420685900aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca1bfd86-7c1c-436b-9d01-64d9c9b66a86", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\149.txt", "file_name": "149.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "88833ced720e33cfa5797dc1ec23a4411035c343b19e36016d2720a55940705e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55f77059-cd17-4b44-ac14-81f5483eb85c", "node_type": "1", "metadata": {}, "hash": "5f86ed7e2371e62c0492d4cec401c927e610557e476e6cfebe311e07d9b5d6b4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: dask-ml\r\nlink: https://github.com/dask/dask-ml\r\ndescription:", "start_char_idx": 0, "end_char_idx": 71, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55f77059-cd17-4b44-ac14-81f5483eb85c": {"__data__": {"id_": "55f77059-cd17-4b44-ac14-81f5483eb85c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\150.txt", "file_name": "150.txt", "file_type": "text/plain", "file_size": 649, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6bbd1c2-97f4-4eff-bb6b-014db6b718d3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\150.txt", "file_name": "150.txt", "file_type": "text/plain", "file_size": 649, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "36749d2da2deb699241728129ee03c6014cf5be90b7173f0cafd4617341e8885", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7be92c7b-ea10-4014-9e09-9530b76b92e4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\15.txt", "file_name": "15.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e4a420400b8abf245687b043dfddfd798b59a6ce25863dc77aa9dcc87959bdb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88c13260-64a0-45b7-86e2-8b91f958b4bb", "node_type": "1", "metadata": {}, "hash": "28e83781108c03a7a48928ac987f0e3b7322cc00ecd306aa1dbb6803bdeb7332", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: spaCy\r\nlink: https://github.com/explosion/spaCy\r\ndescription: spaCy is a library for advanced Natural Language Processing in Python and Cython. It features neural network models for tagging, parsing, named entity recognition, text classification and more, and currently supports tokenization and training for 70+ languages. Additionally, it includes pretrained pipelines, a production-ready training system, and easy model packaging, deployment, and workflow management. Lastly, spaCy is commercial open-source software, released under the MIT license. For more information, see the documentation and the facts, figures, and benchmarks.", "start_char_idx": 0, "end_char_idx": 647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88c13260-64a0-45b7-86e2-8b91f958b4bb": {"__data__": {"id_": "88c13260-64a0-45b7-86e2-8b91f958b4bb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\151.txt", "file_name": "151.txt", "file_type": "text/plain", "file_size": 724, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61dcf961-7eca-4fef-b222-a6c86e9310b8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\151.txt", "file_name": "151.txt", "file_type": "text/plain", "file_size": 724, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6e122809f0e4bbc8e02e05c704f12af5ebc4c91d44fd48adfde0f0610a4aa779", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55f77059-cd17-4b44-ac14-81f5483eb85c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\150.txt", "file_name": "150.txt", "file_type": "text/plain", "file_size": 649, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "07afb0b84566f66e530480634081ec12c7805bcae1a72d1b6b02b7cd0d639342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87be98ee-aa47-4d7f-ade8-f2257ccf98f9", "node_type": "1", "metadata": {}, "hash": "2996a1bd2fe44e94de358a6be79d9eb06067f86aff49289a8e718ffb454ad42f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: gpt-2\r\nlink: https://github.com/openai/gpt-2\r\ndescription: The GPT-2 is a language model that can be used by researchers and engineers to experiment with language generation and various natural language processing tasks. It is an archive status repository that comes with code and models from \"Language Models are Unsupervised Multitask Learners\" paper. The starting point here should be the model card. The GPT-2 has a dataset for researchers, though it comes with some caveats. The GitHub community articles give contributors and users an opportunity to collaborate outside of code. While the model no longer receives updates, GitHub continues to fund open-source developers and foster creative collaboration.", "start_char_idx": 0, "end_char_idx": 722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87be98ee-aa47-4d7f-ade8-f2257ccf98f9": {"__data__": {"id_": "87be98ee-aa47-4d7f-ade8-f2257ccf98f9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\152.txt", "file_name": "152.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0000001c-2244-46db-8588-916c3be6d671", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\152.txt", "file_name": "152.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e1f185585f98a5d5d5341102fccf0e2539d2c3f16091d74a6840fcc0cee8a390", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88c13260-64a0-45b7-86e2-8b91f958b4bb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\151.txt", "file_name": "151.txt", "file_type": "text/plain", "file_size": 724, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "532b577b8a25356abd2018d5fd6f038cf31c55f1944fb46df7192e4a1b7e5744", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c9df2ee-beb6-40b7-ac9b-1e76f7023ec2", "node_type": "1", "metadata": {}, "hash": "691ba31d1e7f937526cf5e75cf8f01c29f0e8103fad1bc38f9021175d5eedf9e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: sense2vec\r\nlink: https://github.com/explosion/sense2vec\r\ndescription: Sense2vec is a Python implementation for loading, querying, and training sense2vec models, which is a variation of word2vec. It lets you learn more interesting and detailed word vectors. The library provides a variety of features such as a quick start, installation, and setup, usage with spaCy v3, standalone usage, API class, and component, training your sense2vec vectors, and a registry function to customize key and phrase generation. The library also includes pretrained vectors trained on Reddit comments and an interactive demo to explore the vectors' semantic similarities.", "start_char_idx": 0, "end_char_idx": 663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c9df2ee-beb6-40b7-ac9b-1e76f7023ec2": {"__data__": {"id_": "7c9df2ee-beb6-40b7-ac9b-1e76f7023ec2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\153.txt", "file_name": "153.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e18191f7-ad15-4f14-8c45-a7f6879d485e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\153.txt", "file_name": "153.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1b607938eac9cca6de7daea25f2441c94ed10c0868dd42a27672b3d97a25ee4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87be98ee-aa47-4d7f-ade8-f2257ccf98f9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\152.txt", "file_name": "152.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e2c45df93fad5da03be62a3104855990aaec5584301d0ce8f0c327ca417741a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbd1ff79-0952-4c42-828a-e475a0632f01", "node_type": "1", "metadata": {}, "hash": "a16dc959fa6984d287846cc7141d0d02010009340bcbbe2a2bba2fc63b61294d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: grover\r\nlink: https://github.com/rowanz/grover\r\ndescription: Grover is a model for Neural Fake News, designed for both generation and detection. It can also be used for other generation tasks. You can find more information about Grover on the project page at rowanzellers.com/grover or in the full paper at arxiv.org/abs/1905.12616. The grover can be set up using the provided instructions, which are easy to follow. Grover-Mega is now publicly available, and you can download it using download_model.py.", "start_char_idx": 0, "end_char_idx": 515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbd1ff79-0952-4c42-828a-e475a0632f01": {"__data__": {"id_": "dbd1ff79-0952-4c42-828a-e475a0632f01", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\154.txt", "file_name": "154.txt", "file_type": "text/plain", "file_size": 528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d89ad8cd-a5b3-4c66-8626-63e7e25878f6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\154.txt", "file_name": "154.txt", "file_type": "text/plain", "file_size": 528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f08791547bbea4c489b9702fccfbfe8c5fda0832c9e4030c4106fdd1d7a78ef8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c9df2ee-beb6-40b7-ac9b-1e76f7023ec2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\153.txt", "file_name": "153.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "53b368b95fe50de3f2801831ef0564e4c93ec5890f3532ff0bf950c2aa775e13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd08f12e-b42a-485c-b313-5e98f5139222", "node_type": "1", "metadata": {}, "hash": "6fffbafa554af840c3e3631578520092e55e7657eb8c3a786263cf5082d3257d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: Kashgari\r\nlink: https://github.com/BrikerMan/Kashgari\r\ndescription: Kashgari is a simple and powerful NLP Transfer learning framework that allows you to build a state-of-art model in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS), and text classification tasks. It is based on Python 3.6+ and offers a range of features including performance monitoring, documentation, and contributing guidelines. There are also tutorials and articles available to help you get started with the library.", "start_char_idx": 0, "end_char_idx": 526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd08f12e-b42a-485c-b313-5e98f5139222": {"__data__": {"id_": "bd08f12e-b42a-485c-b313-5e98f5139222", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\155.txt", "file_name": "155.txt", "file_type": "text/plain", "file_size": 759, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a3cb979b-edef-4ad3-9457-0246e60c8f8e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\155.txt", "file_name": "155.txt", "file_type": "text/plain", "file_size": 759, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "20c436ebcdaa93421db6db6d1f6736ae6c9925e7bf9e43da9718fd36ea9bc393", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbd1ff79-0952-4c42-828a-e475a0632f01", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\154.txt", "file_name": "154.txt", "file_type": "text/plain", "file_size": 528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3cd3f58b3c793576ab74cc2ed0e5b606e730e5440cbcbd05de038d07fb0cb217", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67fd19c3-6488-4156-ad13-7c82780cb3bf", "node_type": "1", "metadata": {}, "hash": "a3b6f4bf08176a074514fcb79d2e58b8ac480c7fde36c3ce75a9ba2981262ee3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: gnes\r\nlink: https://github.com/gnes-ai/gnes\r\ndescription: GNES is a cloud-native semantic search system based on deep neural network that enables large-scale index and semantic search for text-to-text, image-to-image, video-to-video and any-to-any content form. To know more about the key tenets of GNES, you can read their blog post. GNES Hub ships AI/ML models as Docker containers, which offers a clean and sustainable way to port external algorithms into the GNES framework. There are two ways to get GNES, either as a Docker image or as a PyPi package. GNES can be ran as a Docker container using \"docker run gnes/gnes:latest-alpine\" command, and it can also be installed via pip. The official documentation of GNES is hosted on doc.gnes.ai.", "start_char_idx": 0, "end_char_idx": 757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67fd19c3-6488-4156-ad13-7c82780cb3bf": {"__data__": {"id_": "67fd19c3-6488-4156-ad13-7c82780cb3bf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\156.txt", "file_name": "156.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c03a63ed-4b7b-4f31-99b7-6b0475267417", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\156.txt", "file_name": "156.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4f66c4b93aa21db11bfb7f8cb4d49ceb40ac4d2d5afdfe78cd336eec98ff0a5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd08f12e-b42a-485c-b313-5e98f5139222", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\155.txt", "file_name": "155.txt", "file_type": "text/plain", "file_size": 759, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ef18bbe3bb2b49dcda01250b40dd862d3bc8e8eb9d7cf55cb22dae5e48f71e92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d05eef7f-8a9e-4439-b57b-1ee13b19c2f2", "node_type": "1", "metadata": {}, "hash": "8f84a3f224d1e0a626c4353023159e8a7a084f250f9bad49403505c75874da59", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: gluon-nlp\r\nlink: https://github.com/dmlc/gluon-nlp\r\ndescription: GluonNLP is a toolkit that helps you solve NLP problems. It provides easy-to-use tools that helps you load the text data, process the text data, and train models. See our documents at https://nlp.gluon.ai/master/index.html. To facilitate both the engineers and researchers, we provide command-line toolkits for downloading and processing the NLP datasets. For more details, you may refer to GluonNLP Datasets and GluonNLP Data Processing Tools. You may go to tests to see how to run the unittests. You can use Docker to launch a JupyterLab development environment with GluonNLP installed. For more details, you can refer to the guidance in tools/docker.", "start_char_idx": 0, "end_char_idx": 729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d05eef7f-8a9e-4439-b57b-1ee13b19c2f2": {"__data__": {"id_": "d05eef7f-8a9e-4439-b57b-1ee13b19c2f2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\157.txt", "file_name": "157.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27f9f4d0-045c-4dec-bc9d-5f2b33bc10c1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\157.txt", "file_name": "157.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b37547096e70c0f147de8e6c27840d3d743d2c063db8728c17e6a08125766800", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67fd19c3-6488-4156-ad13-7c82780cb3bf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\156.txt", "file_name": "156.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "96b2ed3329d439e576ea4975788ce04dd20a0092d2af9112d51071d58da3bcb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b27b84e-2eea-4b04-b54f-ac5e36da422a", "node_type": "1", "metadata": {}, "hash": "f071818afe2fbf8c790d4b5f7b5182c045dca561d7982e309278be2fa2f563c9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: flair\r\nlink: https://github.com/flairNLP/flair\r\ndescription: Flair is a powerful NLP library developed by Humboldt University of Berlin and friends, providing state-of-the-art natural language processing models such as named entity recognition (NER), sentiment analysis, part-of-speech tagging (PoS), biomedical data support, sense disambiguation and classification, with support for a growing number of languages. It also includes interfaces to use and combine different word and document embeddings, and is built on top of PyTorch framework, making it easy to train and experiment. Flair ships with pre-trained models and has tutorials to get started quickly. The documentation page has many tutorials, including biomedical NER and datasets with installation instructions. Flair is licensed under the MIT License and welcomes contributions.", "start_char_idx": 0, "end_char_idx": 853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b27b84e-2eea-4b04-b54f-ac5e36da422a": {"__data__": {"id_": "4b27b84e-2eea-4b04-b54f-ac5e36da422a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\158.txt", "file_name": "158.txt", "file_type": "text/plain", "file_size": 745, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a264463d-7e21-4ae7-8366-e236e310daa8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\158.txt", "file_name": "158.txt", "file_type": "text/plain", "file_size": 745, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "67fc915f5f065f8b04242a0c849643ea8a11b5ec009b6daa687cad38867e109d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d05eef7f-8a9e-4439-b57b-1ee13b19c2f2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\157.txt", "file_name": "157.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0a79c38ede49e3c36cfe4ecdfdc1b53871c80557ef475fac06d582b8bb6d4fd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed5d40ed-0b1c-476f-b7d6-08e5ab8e71b7", "node_type": "1", "metadata": {}, "hash": "3563a6026368e4e3dc07c8a95a1b474337035f103dd0fda02707e769b346a636", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: semantic\r\nlink: https://github.com/github/semantic\r\ndescription: `semantic` is a Haskell library and command line tool for parsing, analyzing, and comparing source code. It offers features such as automating workflows, hosting and managing packages, finding and fixing vulnerabilities, and instant dev environments. Users can write better code with AI, as well as collaborate outside of code. There are also options to fund open source developers and access GitHub community articles. The tool requires at least GHC 8.10.1 and Cabal 3.0 and builds only on Unix systems. Architecturally, `semantic` leverages a number of interesting algorithms and techniques. Contributions are welcome and Semantic is licensed under the MIT license.", "start_char_idx": 0, "end_char_idx": 743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed5d40ed-0b1c-476f-b7d6-08e5ab8e71b7": {"__data__": {"id_": "ed5d40ed-0b1c-476f-b7d6-08e5ab8e71b7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\159.txt", "file_name": "159.txt", "file_type": "text/plain", "file_size": 78, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f0bd10-1ce2-4015-bb6a-6f6ff085216d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\159.txt", "file_name": "159.txt", "file_type": "text/plain", "file_size": 78, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d63e9a1782b3b33a4b74ba104115e89475abeab965dbe6414753186f52dd2bfd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b27b84e-2eea-4b04-b54f-ac5e36da422a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\158.txt", "file_name": "158.txt", "file_type": "text/plain", "file_size": 745, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f53fd8bc6d4aaefca92a7fc5f9297a9162e7ce7c17f7cacbb48f5e54481226b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4e0ddf3-f606-4dde-afc0-c74e995aa2c4", "node_type": "1", "metadata": {}, "hash": "e22be1a9b9a31329364b843207d9477b9631a438ddcb29363ed33d4faab5a58d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: XLM\r\nlink: https://github.com/facebookresearch/XLM\r\ndescription:", "start_char_idx": 0, "end_char_idx": 75, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4e0ddf3-f606-4dde-afc0-c74e995aa2c4": {"__data__": {"id_": "c4e0ddf3-f606-4dde-afc0-c74e995aa2c4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\16.txt", "file_name": "16.txt", "file_type": "text/plain", "file_size": 682, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "462d6170-8c76-444e-b308-e4900cdf71fe", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\16.txt", "file_name": "16.txt", "file_type": "text/plain", "file_size": 682, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ebee45f4d1b9133a8f3cf5e4c7a139f8fd4ccb36befe583348ccdfc491d86169", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed5d40ed-0b1c-476f-b7d6-08e5ab8e71b7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\159.txt", "file_name": "159.txt", "file_type": "text/plain", "file_size": 78, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a05bdb1189e89baa7ad12beb76f6baacd18803e3a373c5365359b58ee5869b32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6e3162c8-e330-4c45-83ad-29d986a4d643", "node_type": "1", "metadata": {}, "hash": "63df279a1310c6a784a9cf2f4aa30e5e69ac3bb25e9648b2ec684a0cfc33fd28", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: spark\r\nlink: https://github.com/apache/spark\r\ndescription: Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing. You can find the latest Spark documentation, including a programming guide, on the project web page. This README file only contains basic setup instructions.", "start_char_idx": 0, "end_char_idx": 680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e3162c8-e330-4c45-83ad-29d986a4d643": {"__data__": {"id_": "6e3162c8-e330-4c45-83ad-29d986a4d643", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\160.txt", "file_name": "160.txt", "file_type": "text/plain", "file_size": 289, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc1181aa-9fc5-4538-986d-617ea6ee6245", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\160.txt", "file_name": "160.txt", "file_type": "text/plain", "file_size": 289, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a3bcf498ef33b43430a399613c357ae8d4653be57b67a1c48d3a591182b9c052", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4e0ddf3-f606-4dde-afc0-c74e995aa2c4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\16.txt", "file_name": "16.txt", "file_type": "text/plain", "file_size": 682, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4c3600a07a145a27745001d62daf038d0605b7160d66e70c265067908e891648", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60e86002-b3e8-4d07-a76d-de68a9610980", "node_type": "1", "metadata": {}, "hash": "b093916616e0a9e4abb3c842744b8d9b34175590e1d8a9886500fcb5ed042f00", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ctrl\r\nlink: https://github.com/salesforce/ctrl\r\ndescription: CTRL - A Conditional Transformer Language Model for Controllable Generation.Updates Introduction Table of Contents Citation License Questions for.Deliberation Usage Generations Source Attributions FAQs Get Involved.", "start_char_idx": 0, "end_char_idx": 287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60e86002-b3e8-4d07-a76d-de68a9610980": {"__data__": {"id_": "60e86002-b3e8-4d07-a76d-de68a9610980", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\161.txt", "file_name": "161.txt", "file_type": "text/plain", "file_size": 355, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "716ce4a5-53ce-4e42-a5de-c0c5dfd9f60e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\161.txt", "file_name": "161.txt", "file_type": "text/plain", "file_size": 355, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7edb8f249e266a27fc367a2e5f7597d5296d40309dae30533d6c4d1af56e956c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6e3162c8-e330-4c45-83ad-29d986a4d643", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\160.txt", "file_name": "160.txt", "file_type": "text/plain", "file_size": 289, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d97719d4c49663b742cbf7a5565f7424a5493923dfd63bccb34505a3d8ef8c28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "947af18e-4ae9-4e47-9f6f-d5603aea7fbb", "node_type": "1", "metadata": {}, "hash": "78c7bd7ff4c8dad7fefa09d576db66666fa3a20cd1bd5da296baa0e3a0f7a466", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: Blackstone\r\nlink: https://github.com/ICLRandD/Blackstone\r\ndescription: Blackstone is a spaCy model and library for processing long-form, unstructured legal text. Blackstone is an experimental research project from the Incorporated Council of Law Reporting for England and Wales' research lab, ICLR&D. Blackstone was written by Daniel Hoadley.", "start_char_idx": 0, "end_char_idx": 353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "947af18e-4ae9-4e47-9f6f-d5603aea7fbb": {"__data__": {"id_": "947af18e-4ae9-4e47-9f6f-d5603aea7fbb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\162.txt", "file_name": "162.txt", "file_type": "text/plain", "file_size": 469, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e83a0796-4953-4f1e-bf3d-3333d778ef97", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\162.txt", "file_name": "162.txt", "file_type": "text/plain", "file_size": 469, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cc30aa75acdb9dd13f4bc430b0ccf41acec9fb64a8f00404fb801ce129cb1d71", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60e86002-b3e8-4d07-a76d-de68a9610980", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\161.txt", "file_name": "161.txt", "file_type": "text/plain", "file_size": 355, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "800ec983609fc42abce7b72386cc3132c3678752b4939637699e0c9a2cddc055", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab44ceb6-b2c5-447a-9a53-312e4b59fcfd", "node_type": "1", "metadata": {}, "hash": "494c34e736e4a7cbf2c17f6d05f603b2b849b6fe5e4671127e814dab83a1314d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: yellowbrick\r\nlink: https://github.com/DistrictDataLabs/yellowbrick\r\ndescription: Yellowbrick is a suite of visual diagnostic tools called \"Visualizers\" that extend the scikit-learn API to allow human steering of the model selection process. In a nutshell, Yellowbrick combines scikit-learn with matplotlib to produce visualizations for your machine learning workflow. **Visual analysis and diagnostic tools to facilitate machine learning model selection.**", "start_char_idx": 0, "end_char_idx": 467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab44ceb6-b2c5-447a-9a53-312e4b59fcfd": {"__data__": {"id_": "ab44ceb6-b2c5-447a-9a53-312e4b59fcfd", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\163.txt", "file_name": "163.txt", "file_type": "text/plain", "file_size": 704, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63c60423-ec78-4506-bce9-0bd23fa1c367", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\163.txt", "file_name": "163.txt", "file_type": "text/plain", "file_size": 704, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2f875ae99ab49492e53584c228187b2e77053b5c1a037417ec879cd2a9468cd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "947af18e-4ae9-4e47-9f6f-d5603aea7fbb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\162.txt", "file_name": "162.txt", "file_type": "text/plain", "file_size": 469, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bd396fd11063e69f8c6c6f9075ab5cd8a1ba9f8687a44d00654d05c3bd16bb4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1bd299c-5eb4-4f0b-bd7d-c312becf0764", "node_type": "1", "metadata": {}, "hash": "fc3392a2e166aa56f623ea434ffdb784947b6657aad63a1738c1cd10e9591e79", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: streamlit\r\nlink: https://github.com/streamlit/streamlit\r\ndescription: Welcome to Streamlit \ud83d\udc4b!\r\n\r\nStreamlit is the fastest way to build and share data apps, letting you turn data scripts into shareable web apps in minutes, not weeks. It\u2019s all Python, open-source, and free! With Streamlit, you can also automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and read GitHub community articles. Streamlit makes it incredibly easy to build interactive apps with its simple and focused API, allowing you to build incredibly rich and powerful tools.", "start_char_idx": 0, "end_char_idx": 697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1bd299c-5eb4-4f0b-bd7d-c312becf0764": {"__data__": {"id_": "d1bd299c-5eb4-4f0b-bd7d-c312becf0764", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\164.txt", "file_name": "164.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49dccfd9-335c-480d-870c-8943fae4b87d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\164.txt", "file_name": "164.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e446d3fb39c430a37eca3e41a0630537516facdc4dacebd046ca60c19c09bbae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab44ceb6-b2c5-447a-9a53-312e4b59fcfd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\163.txt", "file_name": "163.txt", "file_type": "text/plain", "file_size": 704, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "65115fefcceb89d53fe544aba834b8a52d55ed42c68a9139d1f82de2af63b79a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "894a342f-1b66-4722-8b0b-30922c727dc6", "node_type": "1", "metadata": {}, "hash": "7d9d61377777f9a146ca7afa8a3c3d5180ebcc1b58588a4b709d3429c168d7ca", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: seaborn\r\nlink: https://github.com/mwaskom/seaborn\r\ndescription: \"Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Online documentation is available at seaborn.pydata.org. The docs include a tutorial, example gallery, API reference, FAQ, and other useful information. Seaborn supports Python 3.8+. Some advanced statistical functionality requires scipy and/or statsmodels. The latest stable release (and required dependencies) can be installed from PyPI. Seaborn can also be installed with conda. A paper describing seaborn has been published in the Journal of Open Source Software. Testing seaborn requires installing additional dependencies; they can be installed with the dev extra. Seaborn development takes place on Github.\"", "start_char_idx": 0, "end_char_idx": 833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "894a342f-1b66-4722-8b0b-30922c727dc6": {"__data__": {"id_": "894a342f-1b66-4722-8b0b-30922c727dc6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\165.txt", "file_name": "165.txt", "file_type": "text/plain", "file_size": 391, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "010a51e2-c61f-42e3-bad5-64c74c7c09aa", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\165.txt", "file_name": "165.txt", "file_type": "text/plain", "file_size": 391, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a9830671a54cecfdf71d1d4d6cd04597882173de4158a778d66a15fcd06d89cb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1bd299c-5eb4-4f0b-bd7d-c312becf0764", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\164.txt", "file_name": "164.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3b0d40935f1935e1781d74364cb1519f7d174b9be304b8491ad7e822ade29e7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ecba323-fa85-43fb-aae2-15a011e9629f", "node_type": "1", "metadata": {}, "hash": "d37b5ab2446c4df88335cd5e5876797afa98178ac457c276c453ab4ba799eebd", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pygal\r\nlink: https://github.com/Kozea/pygal\r\ndescription: Pygal is a dynamic SVG charting library written in python. All the documentation is on www.pygal.org. Pygal is tested with py.test. You are welcomed to fork the project and make pull requests. Be sure to create a branch for each feature, write tests if needed and run the current tests! You can also support the project.", "start_char_idx": 0, "end_char_idx": 389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ecba323-fa85-43fb-aae2-15a011e9629f": {"__data__": {"id_": "4ecba323-fa85-43fb-aae2-15a011e9629f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\166.txt", "file_name": "166.txt", "file_type": "text/plain", "file_size": 672, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "78bf74e2-f33f-4af2-a01c-f9caa88df9fd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\166.txt", "file_name": "166.txt", "file_type": "text/plain", "file_size": 672, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a09ddd413ca8c523d8ed7bd90f70c76bc3415a9d41ab78901e4cdf9478526409", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "894a342f-1b66-4722-8b0b-30922c727dc6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\165.txt", "file_name": "165.txt", "file_type": "text/plain", "file_size": 391, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "570b34f36c7090151d2a97247365adb0b596010fa21b5808e674a6fc222f576a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1c1a2a3-9ce7-4897-a5ca-cbb7e0e9f35e", "node_type": "1", "metadata": {}, "hash": "1d60ab37923875c5f754a31af0e496214a67dd8a2c38cd6b8ef71cc7376d5281", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: redash\r\nlink: https://github.com/getredash/redash\r\ndescription: Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions. Redash features: ![](https://raw.githubusercontent.com/getredash/website/8e820cd02c73a8ddf4f946a9d293c54fd3fb08b9/website/_assets/images/redash-anim.gif).", "start_char_idx": 0, "end_char_idx": 670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1c1a2a3-9ce7-4897-a5ca-cbb7e0e9f35e": {"__data__": {"id_": "a1c1a2a3-9ce7-4897-a5ca-cbb7e0e9f35e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\167.txt", "file_name": "167.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87ee2a70-6212-4e20-ba00-4de444782e49", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\167.txt", "file_name": "167.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3b967bbf9ad5390862a4a8cb9d91001af6024341f12a8d8029cacb59febddc11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ecba323-fa85-43fb-aae2-15a011e9629f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\166.txt", "file_name": "166.txt", "file_type": "text/plain", "file_size": 672, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "af10791904e47db93a65675fe0ee8b4618fe23dd1da296852a198599d3aaf498", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6139311c-5093-4c2b-9987-653262cedf51", "node_type": "1", "metadata": {}, "hash": "379b8156e09611a52507f2fa2f5d04bd5cd5c701b2e791885125f4f8c868a658", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: PyCEbox\r\nlink: https://github.com/AustinRochford/PyCEbox\r\ndescription: PyCEbox is a Python implementation of individual conditional expecation plots inspired by.R's ICEbox, which is available on PyPI and can be installed with pip install.pycebox. For easy development and prototyping using IPython notebooks, a Docker.environment is included. To run an IPython notebook with access to your.development version of pycebox, run PORT=8889 sh ./start_container.sh. A.Jupyter notebook server with access to your development version of `pycebox`.should be available at `http://localhost:8889/tree`. For details of `pycebox`'s API, consult the documentation. This library is distributed under the MIT License.", "start_char_idx": 0, "end_char_idx": 713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6139311c-5093-4c2b-9987-653262cedf51": {"__data__": {"id_": "6139311c-5093-4c2b-9987-653262cedf51", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\168.txt", "file_name": "168.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4871d211-2c6b-42a2-b9ce-0d377c287e1e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\168.txt", "file_name": "168.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fd94ddf5016ecf269eca9a46734a2d64b589ef655bb12fe2c38d1924387ac1a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1c1a2a3-9ce7-4897-a5ca-cbb7e0e9f35e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\167.txt", "file_name": "167.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3d6950eb5c398ec8405ee1abda3a11480d3e9e490ce4fae6033725fa46dda95f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c3e10b1-7452-42ef-813a-9be497e7f577", "node_type": "1", "metadata": {}, "hash": "14ed375ed21300183ddf21fc503633c0422e2f08172196a279dee4f7dddd6a57", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: plotly.py\r\nlink: https://github.com/plotly/plotly.py\r\ndescription: `plotly.py` is an interactive, open-source, and browser-based graphing library for Python. It is built on top of plotly.js and is a high-level, declarative charting library that ships with over 30 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps, financial charts, and more. `plotly.py` is MIT Licensed and graphs can be viewed in Jupyter notebooks, standalone HTML files, or integrated into Dash applications. It also supports static image export using either the `kaleido` package or the Orca command line utility. For use in JupyterLab or Jupyter Notebook, install the required packages. Some `plotly.py` features rely on fairly large geographic shape files, which are distributed as a separate `plotly-geo` package that can be installed using pip or conda.", "start_char_idx": 0, "end_char_idx": 871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c3e10b1-7452-42ef-813a-9be497e7f577": {"__data__": {"id_": "4c3e10b1-7452-42ef-813a-9be497e7f577", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\169.txt", "file_name": "169.txt", "file_type": "text/plain", "file_size": 938, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37b7993b-8c2e-40f9-963e-9b6f99317064", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\169.txt", "file_name": "169.txt", "file_type": "text/plain", "file_size": 938, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a0f61223019f4ba686b3a2de67416dcf0abf0ee34ede6706b952dfe23605a910", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6139311c-5093-4c2b-9987-653262cedf51", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\168.txt", "file_name": "168.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2a258875fa6defcea4e939a0e9cb85655d111c2cb0ce22900f153210acc2f1d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "662b426c-ac08-4b38-bf62-d893ae49a335", "node_type": "1", "metadata": {}, "hash": "712c19befb4e9fa1dd9285a78309ef7aa1ad5a5c366fc0b548b57e8cdeafba75", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pixiedust\r\nlink: https://github.com/pixiedust/pixiedust\r\ndescription: PixieDust is an open source helper library that works as an add-on to Jupyter notebooks to improve the user experience of working with data. It also fills a gap for users who have no access to configuration files when a notebook is hosted on the cloud. PixieDust is a productivity tool for Python or Scala notebooks, which lets a developer encapsulate business logic into something easy for your customers to consume. PixieDust greatly simplifies working with Python display libraries like matplotlib, but works just as effectively in Scala notebooks too. You no longer have compromise your love of Scala to generate great charts. PixieDust lets you bring robust Python visualization options to your Scala notebooks. PixieDust's current capabilities include: packageManager, visualizations, embedded apps, export, Scala bridge, and Spark progress monitor.", "start_char_idx": 0, "end_char_idx": 936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "662b426c-ac08-4b38-bf62-d893ae49a335": {"__data__": {"id_": "662b426c-ac08-4b38-bf62-d893ae49a335", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\17.txt", "file_name": "17.txt", "file_type": "text/plain", "file_size": 412, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "934d0538-759a-44c8-80f7-eadfde381392", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\17.txt", "file_name": "17.txt", "file_type": "text/plain", "file_size": 412, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "37bce3afff06238f758f1d860a364df9d7a2cadec259c19099a31d082596666e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c3e10b1-7452-42ef-813a-9be497e7f577", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\169.txt", "file_name": "169.txt", "file_type": "text/plain", "file_size": 938, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6fc30408d88c7b3deeec4238827039eb1dbff6f45cf2553fc19a49d851f26667", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "750e9770-2ebb-497e-8e74-4870bab1bb88", "node_type": "1", "metadata": {}, "hash": "6e3ceb2bb8d0a5b466b5dd66ddad87ce8a8b248e84b8ef42944f34a420d48491", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: zarr-python\r\nlink: https://github.com/zarr-developers/zarr-python\r\ndescription: Zarr is a Python package providing an implementation of compressed, chunked, N-dimensional arrays, designed for use in parallel computing. See the documentation for more information. Zarr can be installed from PyPI using `pip`. For more details, including how to install from source, see the installation documentation.", "start_char_idx": 0, "end_char_idx": 410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "750e9770-2ebb-497e-8e74-4870bab1bb88": {"__data__": {"id_": "750e9770-2ebb-497e-8e74-4870bab1bb88", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\170.txt", "file_name": "170.txt", "file_type": "text/plain", "file_size": 380, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d114870-ea8a-4678-b740-30cc2f4dd978", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\170.txt", "file_name": "170.txt", "file_type": "text/plain", "file_size": 380, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81aaa4ab347eb589c498214d087de7bd53d29dcf4b1cd58ac933716cae7313ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "662b426c-ac08-4b38-bf62-d893ae49a335", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\17.txt", "file_name": "17.txt", "file_type": "text/plain", "file_size": 412, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6369b462dc982790fc41c611998cb873aee1e3ef71c5eab116408cdba4d618a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44343dfa-b0ef-496e-97c4-68cb93913d54", "node_type": "1", "metadata": {}, "hash": "014468d3dc0929c715574635c70e8270ec5feee8b61f6cff8fd6d9aaad2f47f1", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: dash\r\nlink: https://github.com/plotly/dash\r\ndescription: Dash Dash is the most downloaded, trusted Python framework for building ML & data science web apps. Built on top of Plotly.js, React and Flask, Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read our tutorial (proudly crafted \u2764\ufe0f with Dash itself).", "start_char_idx": 0, "end_char_idx": 374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44343dfa-b0ef-496e-97c4-68cb93913d54": {"__data__": {"id_": "44343dfa-b0ef-496e-97c4-68cb93913d54", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\171.txt", "file_name": "171.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4acd7e19-c74d-4352-a754-6c0abbe6974d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\171.txt", "file_name": "171.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "89bc94b5043dd46835781b2a106f5d651b22f311d5d385af69b74c5dcd66ef62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "750e9770-2ebb-497e-8e74-4870bab1bb88", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\170.txt", "file_name": "170.txt", "file_type": "text/plain", "file_size": 380, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "04da761fd65f849f77776750eaab9c9a86873a2929d2271ee2758262c6d04b67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70398f22-bcf9-4975-a14c-19f0e8731b8d", "node_type": "1", "metadata": {}, "hash": "d8cea9f3d13e07a7780321f8f8bc56b2788112e67d7e810a1060e5be7acedd84", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: PDPbox\r\nlink: https://github.com/SauceCat/PDPbox\r\ndescription: PDPbox is a powerful Python toolbox to understand the impact of individual features towards model prediction for any supervised learning algorithm, inspired by ICEbox. When using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome. PDPbox offers various tools like PDP for a single feature, PDP for a multi-class, PDP Interact for two features with contour plot, PDP Interact for two features with grid plot, PDP Interact for multi-class, target plot for a single feature, target interact plot for two features, and actual prediction plot for a single feature.", "start_char_idx": 0, "end_char_idx": 729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70398f22-bcf9-4975-a14c-19f0e8731b8d": {"__data__": {"id_": "70398f22-bcf9-4975-a14c-19f0e8731b8d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\172.txt", "file_name": "172.txt", "file_type": "text/plain", "file_size": 394, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "789ceda0-8b18-4d86-8e7b-dc5a1e73ce97", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\172.txt", "file_name": "172.txt", "file_type": "text/plain", "file_size": 394, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d51ca57af46d7b61896586287dcd3f42b73c96819be8d88db015fba6d1799a81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44343dfa-b0ef-496e-97c4-68cb93913d54", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\171.txt", "file_name": "171.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4fc601b91628542eff6a385032c48e34e06e04128af4e991e0c2af476d3df8ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "788994aa-9da5-4e99-814a-42a7249566cf", "node_type": "1", "metadata": {}, "hash": "cb121bdd10f6e8d70ce990dad733a0dcdd3da82ff598b3f88d6d5d866280fca7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: perspective\r\nlink: https://github.com/finos/perspective\r\ndescription: Perspective is an interactive analytics and data visualization component, which is especially well-suited for large and/or streaming datasets. Use it to create user-configurable reports, dashboards, notebooks and applications, then deploy stand-alone in the browser, or in concert with Python and/or Jupyterlab.", "start_char_idx": 0, "end_char_idx": 392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "788994aa-9da5-4e99-814a-42a7249566cf": {"__data__": {"id_": "788994aa-9da5-4e99-814a-42a7249566cf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\173.txt", "file_name": "173.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7b4fc38-010b-4fad-be2e-cb1516718134", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\173.txt", "file_name": "173.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f4053cec75f993bd028510dbd7bf23b3e3d454a0dcaa8247a48fadeba44c78fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70398f22-bcf9-4975-a14c-19f0e8731b8d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\172.txt", "file_name": "172.txt", "file_type": "text/plain", "file_size": 394, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b9c6605a55b40c818e58c6fded2ac45bfb65a466f97ad89878978e860880d368", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92ac68fb-6eed-4e8a-a2f9-6e04d7a0f589", "node_type": "1", "metadata": {}, "hash": "de1453891db3f3ab5b48856daa95afe1b8c070f2a9622bb165dcea7d3a3496fa", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: matplotlib\r\nlink: https://github.com/matplotlib/matplotlib\r\ndescription: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Matplotlib produces publication-quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, Python/IPython shells, web application servers, and various graphical user interface toolkits. Check out our home page for more information.", "start_char_idx": 0, "end_char_idx": 505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92ac68fb-6eed-4e8a-a2f9-6e04d7a0f589": {"__data__": {"id_": "92ac68fb-6eed-4e8a-a2f9-6e04d7a0f589", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\174.txt", "file_name": "174.txt", "file_type": "text/plain", "file_size": 764, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd80c7a4-e1a6-49b9-923c-7b9d7312f54e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\174.txt", "file_name": "174.txt", "file_type": "text/plain", "file_size": 764, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4b887eb0951caf67980d58f29541899cb682044b8c44c57c2519eef289a809b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "788994aa-9da5-4e99-814a-42a7249566cf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\173.txt", "file_name": "173.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ef2764d5d326fbdb277caf5b19848f7173f28e78f81e9214f8b4d70d90c7e0e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1699c17-785f-4806-8489-40e367456d8f", "node_type": "1", "metadata": {}, "hash": "112e040635090fe8a3782ecb3e449de84c96caf3cb5a75ec9cb2a88e3214237b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: missingno\r\nlink: https://github.com/ResidentMario/missingno\r\ndescription: Introducing \"missingno,\" a toolset for visualizing and managing missing data in datasets. With \"missingno,\" users can get quick visual summaries of their dataset's completeness or lack thereof. Simply install the tool by running \"pip install missingno,\" and begin using its various visualization options, including the \"nullity matrix,\" \"bar plot,\" and \"correlation heatmap.\" The tool also includes a dendrogram for more in-depth analysis of variable completion trends. For configuration details and more, refer to the \"CONFIGURATION.md\" file in the repository. For bug reports, feature suggestions, and contributions, see the respective sections in the \"CONTRIBUTING.md\" file.", "start_char_idx": 0, "end_char_idx": 762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1699c17-785f-4806-8489-40e367456d8f": {"__data__": {"id_": "e1699c17-785f-4806-8489-40e367456d8f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\175.txt", "file_name": "175.txt", "file_type": "text/plain", "file_size": 851, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "afb3ead4-8175-4f44-985f-641cbabbf28c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\175.txt", "file_name": "175.txt", "file_type": "text/plain", "file_size": 851, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b3a78489e5c62886bcb83b4a8af1198455935511c5d09a5f5f6973e395364323", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92ac68fb-6eed-4e8a-a2f9-6e04d7a0f589", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\174.txt", "file_name": "174.txt", "file_type": "text/plain", "file_size": 764, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a3e73cdfaf33533cb7824a9004f108e64a56696319040227cc99438518d330cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bea38b7b-5255-404b-bd3c-e5d140c72a80", "node_type": "1", "metadata": {}, "hash": "1efcaee4dfbb00bc7e50722a5c2111e5473de75036c786bd962dc40a2044e728", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: gradio\r\nlink: https://github.com/gradio-app/gradio\r\ndescription: Gradio is an open-source Python library that is used to build machine learning and data science demos and web applications. With Gradio, you can quickly create a beautiful user interface around your machine learning models or data science workflow and let people \"try it out\" by dragging-and-dropping in their own images, pasting text, recording their own voice, and interacting with your demo, all through the browser. Gradio is useful for demoing your machine learning models for clients/collaborators/users/students, deploying your models quickly with automatic shareable links and getting feedback on model performance, and debugging your model interactively during development using built-in manipulation and interpretation tools. Gradio requires Python 3.7 or higher.", "start_char_idx": 0, "end_char_idx": 849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bea38b7b-5255-404b-bd3c-e5d140c72a80": {"__data__": {"id_": "bea38b7b-5255-404b-bd3c-e5d140c72a80", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\176.txt", "file_name": "176.txt", "file_type": "text/plain", "file_size": 590, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "157ba0a8-bdfd-4208-9cbe-69a021d510f7", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\176.txt", "file_name": "176.txt", "file_type": "text/plain", "file_size": 590, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "95b481e322fbbbeacfdd37db4f539a77b6761f95e647cda5b78b9db065d682c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1699c17-785f-4806-8489-40e367456d8f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\175.txt", "file_name": "175.txt", "file_type": "text/plain", "file_size": 851, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "49678f5782fcef102aab5de3f15c62ce89b3ce55e2a79910a71402f297384852", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc64a91c-d893-49fa-9a3d-dd3e3a716670", "node_type": "1", "metadata": {}, "hash": "bcd31a1f14d6631a17b2b9875243233e13c234e25bad69b48c3c2508b69f99f2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: geoplotlib\r\nlink: https://github.com/andrea-cuttone/geoplotlib\r\ndescription: Geoplotlib is a python toolbox for visualizing geographical data and making maps. This will launch the geoplotlib window and plot the points on OpenStreetMap tiles, also allowing zooming and panning. geoplotlib automatically handles the data loading, the map projection, downloading the map tiles and the graphics rendering with OpenGL. Optional requirements for installation include running `python setup.py install` or using `pip install geoplotlib`. A detailed user guide can be found in the wiki.", "start_char_idx": 0, "end_char_idx": 588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc64a91c-d893-49fa-9a3d-dd3e3a716670": {"__data__": {"id_": "fc64a91c-d893-49fa-9a3d-dd3e3a716670", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\177.txt", "file_name": "177.txt", "file_type": "text/plain", "file_size": 775, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "670c0479-fd8f-4b3e-adb4-d20c8cde8510", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\177.txt", "file_name": "177.txt", "file_type": "text/plain", "file_size": 775, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "54f65b7e7eebe3f2cecd797c8f3dc72c7ff4c8a68a8fa37fdee0c0db761433ca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bea38b7b-5255-404b-bd3c-e5d140c72a80", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\176.txt", "file_name": "176.txt", "file_type": "text/plain", "file_size": 590, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ad2b77e945fd1bc7ab21aa66bbb76f29d0689cd4a7856028cef32869622cb247", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6cd7bb9-e4ce-45e7-baac-23d3c3467108", "node_type": "1", "metadata": {}, "hash": "42c697035dabe43ea70b5fef1e375f51456aaa8676d77e0d3ee8eaf006c4bb47", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ggplot2\r\nlink: https://github.com/tidyverse/ggplot2\r\ndescription: ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. It is used by hundreds of thousands of people to make millions of plots and changes relatively little. To learn more about ggplot2, there are several good starting points including the Data Visualisation and Graphics for communication chapters in R for Data Science, an online course called Data Visualization in R With ggplot2, and a webinar called Plotting Anything with ggplot2. For further help, the RStudio community and Stack Overflow are both great resources.", "start_char_idx": 0, "end_char_idx": 773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6cd7bb9-e4ce-45e7-baac-23d3c3467108": {"__data__": {"id_": "c6cd7bb9-e4ce-45e7-baac-23d3c3467108", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\178.txt", "file_name": "178.txt", "file_type": "text/plain", "file_size": 534, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dee94cef-3594-48b7-ac4d-209f1fa4dc1f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\178.txt", "file_name": "178.txt", "file_type": "text/plain", "file_size": 534, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7e38efd157d0735b05dd8fb6e1b99080666ad40fc0ddb991f90d3ba95f46eeb4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc64a91c-d893-49fa-9a3d-dd3e3a716670", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\177.txt", "file_name": "177.txt", "file_type": "text/plain", "file_size": 775, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1544db7e86c3aee02267a20851adb18c7e15d0628670c085e2dc7219f1f51d02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37393fa8-02db-4f56-9ffc-4b5eddc8e996", "node_type": "1", "metadata": {}, "hash": "e0baacd25a14a99288b5accd9b51c5fbee11cdd167a66ea837de0c296fa23780", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: voila\r\nlink: https://github.com/voila-dashboards/voila\r\ndescription: Voil\u00e0 turns Jupyter notebooks into standalone web applications. Unlike the usual HTML-converted notebooks, each user connecting to the Voil\u00e0 tornado application gets a dedicated Jupyter kernel which can execute the callbacks to changes in Jupyter interactive widgets. Voil\u00e0 can be installed with the mamba (or conda) package manager from conda-forge. To get started with using Voil\u00e0, check out the full documentation: https://voila.readthedocs.io/.", "start_char_idx": 0, "end_char_idx": 528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37393fa8-02db-4f56-9ffc-4b5eddc8e996": {"__data__": {"id_": "37393fa8-02db-4f56-9ffc-4b5eddc8e996", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\179.txt", "file_name": "179.txt", "file_type": "text/plain", "file_size": 399, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d41b507-718b-40fa-8d38-9aa41c328430", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\179.txt", "file_name": "179.txt", "file_type": "text/plain", "file_size": 399, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a715303cd583af7be4b9b004617f192b937a8b05ff83c0ced59535652e6e18c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6cd7bb9-e4ce-45e7-baac-23d3c3467108", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\178.txt", "file_name": "178.txt", "file_type": "text/plain", "file_size": 534, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cfeb6fb56fcd7f2f65b4ebf5d6bab692f1dc850c90e16b4b88e5deda0a7bc3ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecdf3d9f-62f7-4686-bec7-19df5e419bd4", "node_type": "1", "metadata": {}, "hash": "7c1e26d08200d1bd53d060115156a99d26675c11217c9f9920e4a905c19c4325", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: bokeh\r\nlink: https://github.com/bokeh/bokeh\r\ndescription: Bokeh is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics and affords high-performance interactivity across large or streaming datasets. Bokeh can help anyone who wants to create interactive plots, dashboards, and data applications quickly and easily.", "start_char_idx": 0, "end_char_idx": 397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecdf3d9f-62f7-4686-bec7-19df5e419bd4": {"__data__": {"id_": "ecdf3d9f-62f7-4686-bec7-19df5e419bd4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\18.txt", "file_name": "18.txt", "file_type": "text/plain", "file_size": 526, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13162bcf-3ee3-4920-b0ed-18649219bbf4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\18.txt", "file_name": "18.txt", "file_type": "text/plain", "file_size": 526, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "14d6e0cdcffe308e6f8472c0c5f6cd46471ed2236f4fe3d93cece865083cd2a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37393fa8-02db-4f56-9ffc-4b5eddc8e996", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\179.txt", "file_name": "179.txt", "file_type": "text/plain", "file_size": 399, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3d98272d4a0e72774bae1f982347b3f90d3e1cb8335d746d5423411073bf7d6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f9f19cf-4aa5-4a58-a08c-81283e904931", "node_type": "1", "metadata": {}, "hash": "c307e6b9dbd87ae9a6c6ab278f3e66371b920e4acb96083d4c11aff5c1f6763c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ClickHouse\r\nlink: https://github.com/ClickHouse/ClickHouse\r\ndescription: ClickHouse\u00ae is an open-source column-oriented database management system that allows generating analytical data reports in real-time. It offers features such as automating workflows, managing packages, finding and fixing vulnerabilities, providing instant dev environments, using AI to write better code, and collaborating outside of code. ClickHouse also allows users to fund open source developers and provides GitHub community articles.", "start_char_idx": 0, "end_char_idx": 523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f9f19cf-4aa5-4a58-a08c-81283e904931": {"__data__": {"id_": "9f9f19cf-4aa5-4a58-a08c-81283e904931", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\180.txt", "file_name": "180.txt", "file_type": "text/plain", "file_size": 362, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ccaae91-a34f-4989-92dd-625d712654f6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\180.txt", "file_name": "180.txt", "file_type": "text/plain", "file_size": 362, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f417ff9e9afb61df33c1d81a678e95693a6a15a8e770c13dc049e30a432936d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecdf3d9f-62f7-4686-bec7-19df5e419bd4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\18.txt", "file_name": "18.txt", "file_type": "text/plain", "file_size": 526, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8b3cdc603494f9146b1d9e642b6cf43db278e1508a6156c8e3550f89e798badf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac8d788b-8b84-410a-bfda-1ad5777a38ce", "node_type": "1", "metadata": {}, "hash": "6939eeac5ee814d849975c714fd67e8408a0189fdcacad6f1497e39f61726ea3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: stencila\r\nlink: https://github.com/stencila/stencila\r\ndescription: Stencila is a platform for collaborating on, and publishing, dynamic, data-driven content. Our aim is to lower the barriers for creating data-driven documents and make it easier to create beautiful, interactive, and semantically rich articles, web pages, and applications from them.", "start_char_idx": 0, "end_char_idx": 360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac8d788b-8b84-410a-bfda-1ad5777a38ce": {"__data__": {"id_": "ac8d788b-8b84-410a-bfda-1ad5777a38ce", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2534ab4-7337-4321-bbb2-bff3c7e5327f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e1514c5b93783d6fc983e911d9441cd5a1fe2294f0349942d999a7822fd976eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f9f19cf-4aa5-4a58-a08c-81283e904931", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\180.txt", "file_name": "180.txt", "file_type": "text/plain", "file_size": 362, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a6ba5c436af824d42b4f7070c587a7ca14326fecd8a2091d7f8bfd63c1d2bba1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "952220d7-bd0f-4664-a658-1c2acb371e2a", "node_type": "1", "metadata": {}, "hash": "90c2fb15a70c49cafde6d63b3f9104be25120ca25b4291108d9069d953230d35", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: polynote\r\nlink: https://github.com/polynote/polynote\r\ndescription: Polynote is an experimental polyglot notebook environment. Currently, it supports Scala and Python (with or without Spark), SQL, and Vega. For more information, see Polynote's website. Current notebook solutions, like Jupyter and Zeppelin, are lacking in some fundamental features:Automate any workflowHost and manage packagesFind and fix vulnerabilitiesInstant dev environmentsWrite better code with AICollaborate outside of codeFund open source developersGitHub community articles* No suggested jump to resultsCould not load branches{{ refName }} default View all branches{{ refName }} default# Name already in useA tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior.", "start_char_idx": 0, "end_char_idx": 852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "952220d7-bd0f-4664-a658-1c2acb371e2a": {"__data__": {"id_": "952220d7-bd0f-4664-a658-1c2acb371e2a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2534ab4-7337-4321-bbb2-bff3c7e5327f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e1514c5b93783d6fc983e911d9441cd5a1fe2294f0349942d999a7822fd976eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac8d788b-8b84-410a-bfda-1ad5777a38ce", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b875bfadf358799d0d991a62eb8b9a18aa15901001258216ab0831d342f7b5a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0786c093-8f72-4fa3-be4c-67c39d1cf21c", "node_type": "1", "metadata": {}, "hash": "ae0ae154ea4cd641fcdd8d58ff6a673ff65c56ef134160a9527f566a1512bd71", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: polynote\r\nlink: https://github.com/polynote/polynote\r\ndescription: Polynote is an experimental polyglot notebook environment. Currently, it supports Scala and Python (with or without Spark), SQL, and Vega. For more information, see Polynote's website. Current notebook solutions, like Jupyter and Zeppelin, are lacking in some fundamental features:Automate any workflowHost and manage packagesFind and fix vulnerabilitiesInstant dev environmentsWrite better code with AICollaborate outside of codeFund open source developersGitHub community articles* No suggested jump to resultsCould not load branches{{ refName }} default View all branches{{ refName }} default# Name already in useA tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.Cannot retrieve contributors at this timeOpen in GitHub Desktop### Footer navigationYou can\u2019t perform that action at this time.You signed in with another tab or window.", "start_char_idx": 0, "end_char_idx": 1178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0786c093-8f72-4fa3-be4c-67c39d1cf21c": {"__data__": {"id_": "0786c093-8f72-4fa3-be4c-67c39d1cf21c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2534ab4-7337-4321-bbb2-bff3c7e5327f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e1514c5b93783d6fc983e911d9441cd5a1fe2294f0349942d999a7822fd976eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "952220d7-bd0f-4664-a658-1c2acb371e2a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d533e844d9f9eb7402ea20205b2d1c532f3d2f73c890f0b9c710eee26f4cdf97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f70fd2b5-fcba-4b8b-a160-f6e8f38f73bd", "node_type": "1", "metadata": {}, "hash": "bf7f3adf240e36fade40683ce0886909e6bba6bd5fbd91652af952d34f6aa8c0", "class_name": "RelatedNodeInfo"}}, "text": "For more information, see Polynote's website. Current notebook solutions, like Jupyter and Zeppelin, are lacking in some fundamental features:Automate any workflowHost and manage packagesFind and fix vulnerabilitiesInstant dev environmentsWrite better code with AICollaborate outside of codeFund open source developersGitHub community articles* No suggested jump to resultsCould not load branches{{ refName }} default View all branches{{ refName }} default# Name already in useA tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.Cannot retrieve contributors at this timeOpen in GitHub Desktop### Footer navigationYou can\u2019t perform that action at this time.You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.", "start_char_idx": 217, "end_char_idx": 1283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f70fd2b5-fcba-4b8b-a160-f6e8f38f73bd": {"__data__": {"id_": "f70fd2b5-fcba-4b8b-a160-f6e8f38f73bd", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\182.txt", "file_name": "182.txt", "file_type": "text/plain", "file_size": 732, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ec41745-8d3c-4639-8ace-b7b8e8dcef4e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\182.txt", "file_name": "182.txt", "file_type": "text/plain", "file_size": 732, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d32a52277d4cf8afc216edf3ea22a1a3b1ddc0522b0cd1a4baf2b1ca9973e721", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0786c093-8f72-4fa3-be4c-67c39d1cf21c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "af2250de6894e79d4275b344b382281038acc4959f287d4532fe1ecc9a03c76f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11ce88d1-df24-479b-b1b4-deeb7da2c84f", "node_type": "1", "metadata": {}, "hash": "c7c58ef745895ef16e1e57605d0cf0a7b450b7c737cd7644910df3d0f48a7ed4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: rmarkdown\r\nlink: https://github.com/rstudio/rmarkdown\r\ndescription: The **rmarkdown** package helps you create dynamic analysis documents that combine code, rendered output (such as figures), and prose. You bring your data, code, and ideas, and R Markdown renders your content into a polished document that can be used to do data science interactively within the RStudio IDE, reproduce your analyses, collaborate and share code with others, and communicate your results. R Markdown documents can be rendered to many output formats including HTML documents, PDFs, Word files, slideshows, and more, allowing you to focus on the content while R Markdown takes care of your presentation. See more about them in Get Started.", "start_char_idx": 0, "end_char_idx": 730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11ce88d1-df24-479b-b1b4-deeb7da2c84f": {"__data__": {"id_": "11ce88d1-df24-479b-b1b4-deeb7da2c84f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\183.txt", "file_name": "183.txt", "file_type": "text/plain", "file_size": 398, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4c8753b-98f9-4463-9544-9c6650380038", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\183.txt", "file_name": "183.txt", "file_type": "text/plain", "file_size": 398, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c7ecba61b87fa4c21544f0bf1f13e64d1b68577a3ebb0dec6b2177899233a389", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f70fd2b5-fcba-4b8b-a160-f6e8f38f73bd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\182.txt", "file_name": "182.txt", "file_type": "text/plain", "file_size": 732, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f650cb2d0222d9d3201addf95f4e6a138be90b3956d62a972c3a39d85b3632ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "598a7c8c-c7d4-43c3-809f-f66819cc0fa8", "node_type": "1", "metadata": {}, "hash": "1fad62fc1016ba07789942ebd0a7468bc2675c9f4d4c4bbf55bc194dc80598e4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: papermill\r\nlink: https://github.com/nteract/papermill\r\ndescription: **Papermill** is a tool for parameterizing, executing, and analyzing Jupyter Notebooks. This opens up new opportunities for how notebooks can be used. For example, Papermill takes an _opinionated_ approach to notebook parameterization and execution based on our experiences using notebooks at scale in data pipelines.", "start_char_idx": 0, "end_char_idx": 396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "598a7c8c-c7d4-43c3-809f-f66819cc0fa8": {"__data__": {"id_": "598a7c8c-c7d4-43c3-809f-f66819cc0fa8", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\184.txt", "file_name": "184.txt", "file_type": "text/plain", "file_size": 733, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dc9ecce-6402-4382-9260-3330b72c19f3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\184.txt", "file_name": "184.txt", "file_type": "text/plain", "file_size": 733, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4e05e7cf85c0f8771b378de3db382e6fd691bd53179fd00b90c20bcf4afb5dd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11ce88d1-df24-479b-b1b4-deeb7da2c84f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\183.txt", "file_name": "183.txt", "file_type": "text/plain", "file_size": 398, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8e124fb96d4a424be6180cdfeecc21cc71a2647d5aa04f968f76ed9c97cde8a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf831700-8db1-4bd1-876f-67ddd104a245", "node_type": "1", "metadata": {}, "hash": "0c8ea2340748e222cd4926de726e852cac2f9b52ac965a3a1c14fa38643b892d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: notebook\r\nlink: https://github.com/jupyter/notebook\r\ndescription: The Jupyter Notebook is a web-based notebook environment for interactive computing. It allows users to automate workflows, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, and fund open source developers. In this notebook, we will introduce the Jupyter Notebook and its two most recently released major versions, Notebook v5 and Classic Notebook v6, as well as the transition to Notebook v7. We will also provide installation and usage instructions, as well as information on development, contributing, community guidelines, and the Jupyter Development Team.", "start_char_idx": 0, "end_char_idx": 731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf831700-8db1-4bd1-876f-67ddd104a245": {"__data__": {"id_": "bf831700-8db1-4bd1-876f-67ddd104a245", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\185.txt", "file_name": "185.txt", "file_type": "text/plain", "file_size": 90, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e368e7f8-2a49-4fb6-81e8-7635954a667f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\185.txt", "file_name": "185.txt", "file_type": "text/plain", "file_size": 90, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bbb873cb0e775ed61eadf5c2b5883fd62ae9721d7cc18c0fb581eddc7d4ebd80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "598a7c8c-c7d4-43c3-809f-f66819cc0fa8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\184.txt", "file_name": "184.txt", "file_type": "text/plain", "file_size": 733, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "855a58ddac89a2ae07309e5c9d44f0ea9bb8f5da5626ffd0ecfbf6f42eef3ea8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c3bbe37-895d-41d7-8422-46840ffa2840", "node_type": "1", "metadata": {}, "hash": "971e5e5fb4ecd4571f445984d38d82563dc429004406e9c40266b160741ca691", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ml-workspace\r\nlink: https://github.com/ml-tooling/ml-workspace\r\ndescription:", "start_char_idx": 0, "end_char_idx": 87, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c3bbe37-895d-41d7-8422-46840ffa2840": {"__data__": {"id_": "3c3bbe37-895d-41d7-8422-46840ffa2840", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\186.txt", "file_name": "186.txt", "file_type": "text/plain", "file_size": 772, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89ee3ec4-6c4e-43b7-8c13-6d24b39fa6a4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\186.txt", "file_name": "186.txt", "file_type": "text/plain", "file_size": 772, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1218ba8015b90b9cc721da82881fe860ea9fe29f17172a4ab95c21749ab7c1e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf831700-8db1-4bd1-876f-67ddd104a245", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\185.txt", "file_name": "185.txt", "file_type": "text/plain", "file_size": 90, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0c3ac0dba8d840d4b401d6c1ae423de528acb30d708084188ac2c32af72402c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6301be1f-5c4e-4d5b-9247-6c63361cb5f4", "node_type": "1", "metadata": {}, "hash": "fe6bd209972924a1ac17c7117a46231277f6907ab79195b4450df06116e52f8e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: h2o-flow\r\nlink: https://github.com/h2oai/h2o-flow\r\ndescription: _H2O Flow_ is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media to build machine learning workflows. Think of Flow as a hybrid GUI + REPL + storytelling environment for exploratory data analysis and machine learning, with async, re-scriptable record/replay capabilities. Flow sandboxes and evals user-Javascript in the browser via static analysis and tree-rewriting. Flow is written in CoffeeScript, with a veritable heap of little embedded DSLs for reactive dataflow programming, markup generation, lazy evaluation and multicast signals/slots. There is a nice user guide for _H2O Flow_ housed over in the h2o-3 repo.", "start_char_idx": 0, "end_char_idx": 770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6301be1f-5c4e-4d5b-9247-6c63361cb5f4": {"__data__": {"id_": "6301be1f-5c4e-4d5b-9247-6c63361cb5f4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\187.txt", "file_name": "187.txt", "file_type": "text/plain", "file_size": 844, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "236e95b0-c5ef-4e84-a2a9-90c9aef518d2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\187.txt", "file_name": "187.txt", "file_type": "text/plain", "file_size": 844, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b639dff3fb1df64b33c83fee1e5414045fc929bd6764f8bd375aca13e95d22be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c3bbe37-895d-41d7-8422-46840ffa2840", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\186.txt", "file_name": "186.txt", "file_type": "text/plain", "file_size": 772, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f7ff3aecccbd75d7c3a4345397b60c7be05ba802d4bc89f7a352a71542b06d03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "958b8bb8-fac7-41fa-bbd7-d6535173c3b3", "node_type": "1", "metadata": {}, "hash": "a216559fbad1bc472a24d346db10ec764a561570c0243f8c60a92e459be798d5", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: nni\r\nlink: https://github.com/microsoft/nni\r\ndescription: \"NNI automates feature engineering, neural architecture search, hyperparameter tuning, and model compression for deep learning. Find the latest features, API, examples and tutorials in our official documentation (\u7b80\u4f53\u4e2d\u6587\u7248\u70b9\u8fd9\u91cc). See the NNI installation guide to install from pip, or build from source. To install the current release, follow the instructions provided. To update NNI to the latest version, add `--upgrade` flag to the above commands. NNI capabilities include automating workflows, hosting and managing packages, finding and fixing vulnerabilities, instant dev environments, and writing better code with AI. For contribution guidelines, bug tracking, and discussions, please refer to the corresponding sections. The codebase is under MIT license.\"", "start_char_idx": 0, "end_char_idx": 826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "958b8bb8-fac7-41fa-bbd7-d6535173c3b3": {"__data__": {"id_": "958b8bb8-fac7-41fa-bbd7-d6535173c3b3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8820a9f6c54cb0d19a550a481a917ba67a8d9c02ff574a385fd8975bed9149dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6301be1f-5c4e-4d5b-9247-6c63361cb5f4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\187.txt", "file_name": "187.txt", "file_type": "text/plain", "file_size": 844, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c32106a59fca29d39bb928829989bae62131ba6a3d0239ec016abee81928efe9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a40ff73-4abf-410c-b2b7-be5835d0c0f8", "node_type": "1", "metadata": {}, "hash": "46967ea4d6db6a875ad6ebb17d98bd77d774fe3765fa22d3d4a1143fd4f611c2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: maggy\r\nlink: https://github.com/logicalclocks/maggy\r\ndescription: Maggy is a framework for distribution transparent machine learning experiments on Apache Spark. In this post, we introduce a new unified framework for writing core ML training logic as oblivious training functions. Maggy enables you to reuse the same training code whether training small models on your laptop or reusing the same code to scale out hyperparameter tuning or distributed deep learning on a cluster. Maggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context. Maggy uses the same distribution transparent training function in all steps of the machine learning development process. Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the `'spark'` extra, to run on Spark in local mode.", "start_char_idx": 0, "end_char_idx": 1069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a40ff73-4abf-410c-b2b7-be5835d0c0f8": {"__data__": {"id_": "2a40ff73-4abf-410c-b2b7-be5835d0c0f8", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8820a9f6c54cb0d19a550a481a917ba67a8d9c02ff574a385fd8975bed9149dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "958b8bb8-fac7-41fa-bbd7-d6535173c3b3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4315401e5d9dd901ef7c3070dfa6eb78a158c85779bcc0da48371d7ede4b7492", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ad2bfe7-2c5e-4b3b-b398-b0e7e0d9533e", "node_type": "1", "metadata": {}, "hash": "e79c85795c621eee421f1d21e8ec49573d8719439a7a932620c2287fcc4a1aec", "class_name": "RelatedNodeInfo"}}, "text": "In this post, we introduce a new unified framework for writing core ML training logic as oblivious training functions. Maggy enables you to reuse the same training code whether training small models on your laptop or reusing the same code to scale out hyperparameter tuning or distributed deep learning on a cluster. Maggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context. Maggy uses the same distribution transparent training function in all steps of the machine learning development process. Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the `'spark'` extra, to run on Spark in local mode. The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment.", "start_char_idx": 173, "end_char_idx": 1259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ad2bfe7-2c5e-4b3b-b398-b0e7e0d9533e": {"__data__": {"id_": "2ad2bfe7-2c5e-4b3b-b398-b0e7e0d9533e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8820a9f6c54cb0d19a550a481a917ba67a8d9c02ff574a385fd8975bed9149dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a40ff73-4abf-410c-b2b7-be5835d0c0f8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5badc6606a0a69aa84a69affdbf2b397e0b4a7cb4b3c1fea806085f45f2c8e07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6816778e-e7ae-4578-b77b-e7ad59928a26", "node_type": "1", "metadata": {}, "hash": "1f6d0127bb6e814c284df0a6b8b4dc07fcc690d858ce1a32074b29a63ae08cde", "class_name": "RelatedNodeInfo"}}, "text": "Maggy enables you to reuse the same training code whether training small models on your laptop or reusing the same code to scale out hyperparameter tuning or distributed deep learning on a cluster. Maggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context. Maggy uses the same distribution transparent training function in all steps of the machine learning development process. Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the `'spark'` extra, to run on Spark in local mode. The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment. **Lagom** is a Swedish word meaning \"just the right amount\". This is how Maggy uses your resources. Full documentation is available at maggy.ai.", "start_char_idx": 292, "end_char_idx": 1404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6816778e-e7ae-4578-b77b-e7ad59928a26": {"__data__": {"id_": "6816778e-e7ae-4578-b77b-e7ad59928a26", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8820a9f6c54cb0d19a550a481a917ba67a8d9c02ff574a385fd8975bed9149dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ad2bfe7-2c5e-4b3b-b398-b0e7e0d9533e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "730a76b2febd73f551c00064def7ef4ada2857b86e2ca74a346e3a0ede9759e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acce73f6-d7c0-40f8-95b7-07873412ebc7", "node_type": "1", "metadata": {}, "hash": "6aa516f1d0e47400ad6c7bcae9170cf4c08ba6730dad326e62af035fc2cbce1e", "class_name": "RelatedNodeInfo"}}, "text": "Maggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context. Maggy uses the same distribution transparent training function in all steps of the machine learning development process. Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the `'spark'` extra, to run on Spark in local mode. The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment. **Lagom** is a Swedish word meaning \"just the right amount\". This is how Maggy uses your resources. Full documentation is available at maggy.ai. There are various ways to contribute, and any contribution is welcome, please follow the CONTRIBUTING guide to get started. Issues can be reported on the official GitHub repo of Maggy.", "start_char_idx": 490, "end_char_idx": 1589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acce73f6-d7c0-40f8-95b7-07873412ebc7": {"__data__": {"id_": "acce73f6-d7c0-40f8-95b7-07873412ebc7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8820a9f6c54cb0d19a550a481a917ba67a8d9c02ff574a385fd8975bed9149dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6816778e-e7ae-4578-b77b-e7ad59928a26", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b8de0a7b37a43336587d272af7174740f4aa96ed59c4c4476f2794a82f60153c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4745de73-139f-42ec-86eb-0c2eee49d58d", "node_type": "1", "metadata": {}, "hash": "8a057bf30b08fca9be02c4f59850299d162f903cb2d63fd0cd1e847dd38cd195", "class_name": "RelatedNodeInfo"}}, "text": "Maggy uses the same distribution transparent training function in all steps of the machine learning development process. Maggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the `'spark'` extra, to run on Spark in local mode. The programming model consists of wrapping the code containing the model training inside a function. Inside that wrapper function provide all imports and parts that make up your experiment. **Lagom** is a Swedish word meaning \"just the right amount\". This is how Maggy uses your resources. Full documentation is available at maggy.ai. There are various ways to contribute, and any contribution is welcome, please follow the CONTRIBUTING guide to get started. Issues can be reported on the official GitHub repo of Maggy. Please see our publications on maggy.ai to find out how to cite our work.", "start_char_idx": 688, "end_char_idx": 1663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4745de73-139f-42ec-86eb-0c2eee49d58d": {"__data__": {"id_": "4745de73-139f-42ec-86eb-0c2eee49d58d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\189.txt", "file_name": "189.txt", "file_type": "text/plain", "file_size": 609, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d89dfac-cd2c-4930-9e00-798a1a6d809f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\189.txt", "file_name": "189.txt", "file_type": "text/plain", "file_size": 609, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dffb989d9b136d2e5dc989d00537b11217b7acba0db4655d4d43c3daf0c22ba0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acce73f6-d7c0-40f8-95b7-07873412ebc7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "60215f41e0750c3280cfa834b731a3b18b9cfd4f5bb5b75bb16a3c14e587d861", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "312a26f9-6326-4455-9802-447184d75875", "node_type": "1", "metadata": {}, "hash": "b2e25e8a43a9de935a0f76f7eb49671b74833bf9828e76c9fbb1e496b9636240", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: neural-architecture-search\r\nlink: https://github.com/titu1994/neural-architecture-search\r\ndescription: Neural Architecture Search with Controller RNN is a basic implementation of Controller RNN from Neural Architecture Search with Reinforcement Learning and Learning Transferable Architectures for Scalable Image Recognition. The goal is to maximize score in 10 epochs of training on CIFAR-10 by trying a toy CNN model with 4 CNN layers with different filter sizes (16, 32, 64) and kernel sizes (1, 3). After 50 steps, it converges to the \"state space\" of (3x3, 64)-(3x3, 64)-(3x3, 32)-(3x3, 64).", "start_char_idx": 0, "end_char_idx": 607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "312a26f9-6326-4455-9802-447184d75875": {"__data__": {"id_": "312a26f9-6326-4455-9802-447184d75875", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\19.txt", "file_name": "19.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5dab64b8-9499-4b96-8bc9-daa95a7b6f48", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\19.txt", "file_name": "19.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "513986530e84d098855d88b2d819895b226c5b8b3d64657c8efd38236d3990a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4745de73-139f-42ec-86eb-0c2eee49d58d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\189.txt", "file_name": "189.txt", "file_type": "text/plain", "file_size": 609, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bf1602d05ba25c1f0813cadedbafc3c68ed707dc9ae83c74baf923256b2a061b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3351d336-6e7d-4e87-afc8-521de6999497", "node_type": "1", "metadata": {}, "hash": "4102a96d6724c9b884dd6cec3dd9482f2c7395d61f671c9aa1bd63f70eef491f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: bayeslite\r\nlink: https://github.com/probcomp/bayeslite\r\ndescription: Bayeslite is a BQL interpretation and storage for BayesDB, but users and contributors should expect rapidly and dramatically shifting code and behavior at this time. It's important to note that this software should not be expected to treat your data securely. The easiest way to install Bayeslite is to use the package on Anaconda Cloud, and for further information, please see http://probcomp.csail.mit.edu/software/bayesdb.", "start_char_idx": 0, "end_char_idx": 505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3351d336-6e7d-4e87-afc8-521de6999497": {"__data__": {"id_": "3351d336-6e7d-4e87-afc8-521de6999497", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\190.txt", "file_name": "190.txt", "file_type": "text/plain", "file_size": 1012, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "264916e0-4c2e-4106-a71c-8fb69e8ef786", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\190.txt", "file_name": "190.txt", "file_type": "text/plain", "file_size": 1012, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "995aa80ee984bd183b6f65c0da10bd0ea8b8aa24d30ac9c82047bc0c58c508d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a26f9-6326-4455-9802-447184d75875", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\19.txt", "file_name": "19.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c8952bc53db405a8b9360227171bab64353ca6e212d1e0f16900389def4b9156", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f36b09a1-13e3-4d48-b7a3-bb2fddfdf65c", "node_type": "1", "metadata": {}, "hash": "f183ab9f4bda6754d53ffd1f039386c25b208286a8b1a98a39e7c61640843099", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: katib\r\nlink: https://github.com/kubeflow/katib\r\ndescription: Katib is a Kubernetes-native project for automated machine learning (AutoML). Katib supports Hyperparameter Tuning, Early Stopping and Neural Architecture Search. Katib is the project which is agnostic to machine learning (ML) frameworks. It can tune hyperparameters of applications written in any language of the users\u2019 choice and natively supports many ML frameworks, such as TensorFlow, Apache.MXNet, PyTorch, XGBoost, and others. Katib can perform training jobs using any Kubernetes Custom Resources with out. of the box support for Kubeflow Training Operator, Argo Workflows, Tekton Pipelines, and many more. Katib stands for `secretary` in Arabic. Katib supports several search algorithms. Follow the Kubeflow documentation to know more about each algorithm and check the Suggestion service guide to implement your custom algorithm. Please feel free to test the system! Developer guide is a good starting point for our developers.", "start_char_idx": 0, "end_char_idx": 1008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f36b09a1-13e3-4d48-b7a3-bb2fddfdf65c": {"__data__": {"id_": "f36b09a1-13e3-4d48-b7a3-bb2fddfdf65c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\191.txt", "file_name": "191.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df6f39d5-7d24-41bb-a50c-2a0781b36859", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\191.txt", "file_name": "191.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d188af2a0db22bc2ebd1f35c978d07383e92ae969018008a5e868600d0a7fd26", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3351d336-6e7d-4e87-afc8-521de6999497", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\190.txt", "file_name": "190.txt", "file_type": "text/plain", "file_size": 1012, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "99532be29591cab7b4c97f574068fc8eacf24845f24665a5ebbf9acbe186c259", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83aa5243-3c09-4926-95e2-c3a7c6ce7afe", "node_type": "1", "metadata": {}, "hash": "fd19494c40bd6514c58fbf9303b35a3bd8c05ed3f7ce3355d31dde29a97bf11b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ENAS-pytorch\r\nlink: https://github.com/carpedm20/ENAS-pytorch\r\ndescription: Efficient Neural Architecture Search (ENAS) in PyTorch is a framework for discovering novel neural network architectures through parameter sharing, reducing the computational requirement of Neural Architecture Search by 1000x. This PyTorch implementation includes discovering recurrent cells and designing convolutional neural networks. The discovered ENAS cells for datasets like Penn Treebank and WikiText-2 are showcased, along with the CNN network discovered for CIFAR-10. The framework requires training of controller LSTM _\u03b8_ and shared parameters _\u03c9_, and users can explore the training details such as reward, entropy, and loss with tensorboard. More configurations can be found in the link provided.", "start_char_idx": 0, "end_char_idx": 795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83aa5243-3c09-4926-95e2-c3a7c6ce7afe": {"__data__": {"id_": "83aa5243-3c09-4926-95e2-c3a7c6ce7afe", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\192.txt", "file_name": "192.txt", "file_type": "text/plain", "file_size": 477, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fcec702-fb6e-4017-81a5-166f163b922c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\192.txt", "file_name": "192.txt", "file_type": "text/plain", "file_size": 477, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "735d01f7041f675f83248590df22c8a860865adc74f937341fc1f1abe0f1d4e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f36b09a1-13e3-4d48-b7a3-bb2fddfdf65c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\191.txt", "file_name": "191.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3c26fa39608d1fe2f9b55e9b1324be32e30f70876f53e8617eedbc57516a5138", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fab305d7-dafd-4290-9a89-44ce176c7933", "node_type": "1", "metadata": {}, "hash": "0ca7e8fbf6f91090ae926dbd6d4ef36a288c96c43b655122775750278e191759", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ENAS-Tensorflow\r\nlink: https://github.com/mingukkang/ENAS-Tensorflow\r\ndescription: ENAS-Tensorflow is an implementation of Efficient Neural Architecture Search (ENAS) that can work in a Windows 10 environment and allows data augmentation using \"n_aug_img\". This repository contains information on how to run ENAS-Tensorflow and includes graphs and results of ENAS cells discovered in the micro search space for datasets such as MNIST, CIFAR10, and Welding Defects.", "start_char_idx": 0, "end_char_idx": 475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fab305d7-dafd-4290-9a89-44ce176c7933": {"__data__": {"id_": "fab305d7-dafd-4290-9a89-44ce176c7933", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\193.txt", "file_name": "193.txt", "file_type": "text/plain", "file_size": 598, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7f1251e-a42d-46f9-8496-e1b620f6364d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\193.txt", "file_name": "193.txt", "file_type": "text/plain", "file_size": 598, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "97dc5e9aec2bfcf7fbb8fe05e4266ba338c86f29780743ac4a36072fbdaf072f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83aa5243-3c09-4926-95e2-c3a7c6ce7afe", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\192.txt", "file_name": "192.txt", "file_type": "text/plain", "file_size": 477, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dd027fd8603c58142d1927ed4d52645044cf418a4e792bfff1ea2f3a49c7619c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e84d2644-cf4a-4715-babb-8c696b2a15a1", "node_type": "1", "metadata": {}, "hash": "ffd76f48d5e9b42de8cf468823f2d44ff92f6d739f1b711c61c41aa1be599f6a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: enas\r\nlink: https://github.com/melodyguan/enas\r\ndescription: ENAS is a tool that allows efficient neural architecture search via parameter sharing, with implementations for tasks such as CIFAR-10 image classification and Penn Tree Bank language modeling. The macro search space and micro search space can both be explored using specific scripts provided by the authors. It is important to note an errata in the language model implementation on the repository, and to use the correct implementation provided elsewhere. Citation of the authors' paper is encouraged if you use their work.", "start_char_idx": 0, "end_char_idx": 596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e84d2644-cf4a-4715-babb-8c696b2a15a1": {"__data__": {"id_": "e84d2644-cf4a-4715-babb-8c696b2a15a1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d16cda4-babc-4bc4-960a-1bacfab43bda", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5c8658fd3dda81d39bd5b489b07460fd74d18a2c81d43654c1a5c3926e4afabe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fab305d7-dafd-4290-9a89-44ce176c7933", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\193.txt", "file_name": "193.txt", "file_type": "text/plain", "file_size": 598, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "51ca3045e27670e344cd4d1c6628c39741ac5983a18b009065ae264cbf47750a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cac3d7f-275a-4e5e-8787-c629b18b29cd", "node_type": "1", "metadata": {}, "hash": "f8ce4c1bd9d7b2e57bf6f4bde0f3645957ed3fc2b6a3556fc3e2c5b396d8d759", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: autokeras\r\nlink: https://github.com/keras-team/autokeras\r\ndescription: AutoKeras is an AutoML system based on Keras which is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible to everyone. AutoKeras can be easily integrated into any workflow and allows to host and manage packages, find and fix vulnerabilities, and create instant dev environments. Users can write better code with AI and collaborate outside of code. AutoKeras also enables open source developers to get funding. To install AutoKeras, users can use the pip installation and follow the installation guide for more details. Currently, AutoKeras is only compatible with Python >= 3.7 and TensorFlow >= 2.8.0. For learning resources, users can refer to the official website of AutoKeras and ask questions on their GitHub Discussions.", "start_char_idx": 0, "end_char_idx": 864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cac3d7f-275a-4e5e-8787-c629b18b29cd": {"__data__": {"id_": "6cac3d7f-275a-4e5e-8787-c629b18b29cd", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d16cda4-babc-4bc4-960a-1bacfab43bda", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5c8658fd3dda81d39bd5b489b07460fd74d18a2c81d43654c1a5c3926e4afabe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e84d2644-cf4a-4715-babb-8c696b2a15a1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81ecd194aa98dd077ab678726894e5b359cb47a1557999721a4d900dd6095051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f8c3664-2f3b-4840-9662-a99d1b2a60e0", "node_type": "1", "metadata": {}, "hash": "802c1e1fb7a7d7be417b1d1938cd4df0e45f9872ca672a63d2ee10d4aab9b638", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: autokeras\r\nlink: https://github.com/keras-team/autokeras\r\ndescription: AutoKeras is an AutoML system based on Keras which is developed by DATA Lab at Texas A&M University. The goal of AutoKeras is to make machine learning accessible to everyone. AutoKeras can be easily integrated into any workflow and allows to host and manage packages, find and fix vulnerabilities, and create instant dev environments. Users can write better code with AI and collaborate outside of code. AutoKeras also enables open source developers to get funding. To install AutoKeras, users can use the pip installation and follow the installation guide for more details. Currently, AutoKeras is only compatible with Python >= 3.7 and TensorFlow >= 2.8.0. For learning resources, users can refer to the official website of AutoKeras and ask questions on their GitHub Discussions. The contributing guide is available for users to learn the best practices and refer to the critical issues added to the milestones for the releases.", "start_char_idx": 0, "end_char_idx": 1013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f8c3664-2f3b-4840-9662-a99d1b2a60e0": {"__data__": {"id_": "0f8c3664-2f3b-4840-9662-a99d1b2a60e0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d16cda4-babc-4bc4-960a-1bacfab43bda", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5c8658fd3dda81d39bd5b489b07460fd74d18a2c81d43654c1a5c3926e4afabe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cac3d7f-275a-4e5e-8787-c629b18b29cd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "969cbd20239eb4dce776b6f7d19c7576cc45ea1ac90244d8d165c3e2c7a3491d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6446494-aaf3-4198-9520-6963b190e3d9", "node_type": "1", "metadata": {}, "hash": "41d43d8a9bb6af681e507b82bab5f6e7ed1c09e7fb369c1dc724a46f90ea8e29", "class_name": "RelatedNodeInfo"}}, "text": "The goal of AutoKeras is to make machine learning accessible to everyone. AutoKeras can be easily integrated into any workflow and allows to host and manage packages, find and fix vulnerabilities, and create instant dev environments. Users can write better code with AI and collaborate outside of code. AutoKeras also enables open source developers to get funding. To install AutoKeras, users can use the pip installation and follow the installation guide for more details. Currently, AutoKeras is only compatible with Python >= 3.7 and TensorFlow >= 2.8.0. For learning resources, users can refer to the official website of AutoKeras and ask questions on their GitHub Discussions. The contributing guide is available for users to learn the best practices and refer to the critical issues added to the milestones for the releases. The development of AutoKeras is acknowledged by the authors in the Journal of machine Learning research.", "start_char_idx": 183, "end_char_idx": 1118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6446494-aaf3-4198-9520-6963b190e3d9": {"__data__": {"id_": "a6446494-aaf3-4198-9520-6963b190e3d9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\195.txt", "file_name": "195.txt", "file_type": "text/plain", "file_size": 85, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22c55992-fb13-40f8-806b-0611273c8547", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\195.txt", "file_name": "195.txt", "file_type": "text/plain", "file_size": 85, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "feff674f88ca224913c086340c7abe55c651b9a8af3a3dbc31163b4f41972162", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f8c3664-2f3b-4840-9662-a99d1b2a60e0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9d9547c123cc424a27760afc28bd761aa6de014f51ac10a49d60770f491bb856", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24ab2465-3c3a-4d45-b72f-2a48c253b351", "node_type": "1", "metadata": {}, "hash": "aaa276ce5088c8f0e8c1b610a957da28cb06416815341cfcaddc5eb79af24c75", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: trickster\r\nlink: https://github.com/spring-epfl/trickster\r\ndescription:", "start_char_idx": 0, "end_char_idx": 82, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24ab2465-3c3a-4d45-b72f-2a48c253b351": {"__data__": {"id_": "24ab2465-3c3a-4d45-b72f-2a48c253b351", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\196.txt", "file_name": "196.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24dc7cf5-5234-4aed-868c-d22f29185bae", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\196.txt", "file_name": "196.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "758585948ae3e682d4d256de3bebd32d3956b0e84ac718d978df712f8438c49d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6446494-aaf3-4198-9520-6963b190e3d9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\195.txt", "file_name": "195.txt", "file_type": "text/plain", "file_size": 85, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8487671ac08ed491d5f23795934f79b21117424f062e1059db5b549a923ee2c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63656294-e3d9-4d92-acb4-fbc4e5ba8701", "node_type": "1", "metadata": {}, "hash": "ddb82e8bb9741c5abaf01057e4ba93b0f51ac85912ab8350553fedf74d6cd2c9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: textfool\r\nlink: https://github.com/bogdan-kulynych/textfool\r\ndescription: This text is about textfool, a proof-of-concept project for producing \"imperceptible\" adversarial examples for text classifiers. It includes instructions for installing the necessary dependencies, running the training and crafting scripts, and provides examples of paraphrasing. It is important to note that this work is not published or peer-reviewed, and has no relationship to \"Deep Text Classification Can be Fooled\" by B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi.", "start_char_idx": 0, "end_char_idx": 560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63656294-e3d9-4d92-acb4-fbc4e5ba8701": {"__data__": {"id_": "63656294-e3d9-4d92-acb4-fbc4e5ba8701", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\197.txt", "file_name": "197.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5b1707b-c225-41a7-90ea-20a7fe5a7c72", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\197.txt", "file_name": "197.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5d078ce7564b5a0a744ef3eae864a3ed1236333e29ffd11bc37cc1127baae4a3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24ab2465-3c3a-4d45-b72f-2a48c253b351", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\196.txt", "file_name": "196.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "219993c091a4f645fe2777057c46bd6f8eec74de74d647112f9e7b6aa7361799", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f8403a5-a055-4c8e-b968-fb65ddd81724", "node_type": "1", "metadata": {}, "hash": "c56a6152596b07f4de64a8433004961a0ce37798e6ca6150c8ec510e45dff893", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mia\r\nlink: https://github.com/spring-epfl/mia\r\ndescription:", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f8403a5-a055-4c8e-b968-fb65ddd81724": {"__data__": {"id_": "7f8403a5-a055-4c8e-b968-fb65ddd81724", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\198.txt", "file_name": "198.txt", "file_type": "text/plain", "file_size": 719, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b6ec820-c642-45a6-b29e-74c002eea37b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\198.txt", "file_name": "198.txt", "file_type": "text/plain", "file_size": 719, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7806ee1b80bf0dbb098feb67c2d2527823bb616196263ef30730a28db0456853", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63656294-e3d9-4d92-acb4-fbc4e5ba8701", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\197.txt", "file_name": "197.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "048c488a2598525a2ba8746b2d1bf40ab1a3cb1279ccb2225da9a935f8d29bb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cca2cef-ea53-4391-9ede-4cfd66ee2781", "node_type": "1", "metadata": {}, "hash": "b50f388908a672ba1a20d1ba1882d3dd112871b312d3961530b9096b2331e6ab", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: DEEPSEC\r\nlink: https://github.com/ryderling/DEEPSEC\r\ndescription: DEEPSEC is a uniform platform for security analysis of deep learning models, which comprehensively and systematically integrates the state-of-the-art adversarial attacks, defenses, and relative utility metrics of them. Developed by Xiang Ling, Shouling Ji, Jiaxu Zou, Jiannan Wang, Chunming Wu, Bo Li and Ting Wang, DEEPSEC provides a standardized environment for analyzing and securing deep learning models. The repository includes descriptions, requirements, datasets, and instructions for usage and experiments. It also provides evaluations and codes for contributions, and has been reconstructed for better readability and adaptability.", "start_char_idx": 0, "end_char_idx": 717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cca2cef-ea53-4391-9ede-4cfd66ee2781": {"__data__": {"id_": "1cca2cef-ea53-4391-9ede-4cfd66ee2781", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9330043a-e89b-4a01-bd74-7ee21012814d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eab5171987623ade90fc6893d6a7ec16e014b8e6690c2a2ff94df8dbbda36eef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f8403a5-a055-4c8e-b968-fb65ddd81724", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\198.txt", "file_name": "198.txt", "file_type": "text/plain", "file_size": 719, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bfdf74d67bb692e43133682926e1227eb19a7761cc08348544792a121f2a3fe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae4fe739-1d37-43aa-95b2-f023540b8c1c", "node_type": "1", "metadata": {}, "hash": "3cf3e7754bb080390ffb48adcf61a8b9b422132de9d1d58e008fb832a83f710c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cleverhans\r\nlink: https://github.com/cleverhans-lab/cleverhans\r\ndescription: CleverHans is a Python library designed to benchmark machine learning systems' vulnerability to adversarial examples. It contains reference implementations of attacks against machine learning models to help with benchmarking models against adversarial examples. Since v4.0.0, CleverHans supports JAX, PyTorch, and TF2, and welcomes contributions for all three frameworks. To set up CleverHans, one needs to install Jax, PyTorch, or TensorFlow 2. Users can install CleverHans using pip or by cloning the Github repository. CleverHans offers tutorials and examples to demonstrate its functionalities, but these should not be considered part of the API. When reporting benchmarks, users are asked to specify the version of CleverHans used.", "start_char_idx": 0, "end_char_idx": 824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae4fe739-1d37-43aa-95b2-f023540b8c1c": {"__data__": {"id_": "ae4fe739-1d37-43aa-95b2-f023540b8c1c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9330043a-e89b-4a01-bd74-7ee21012814d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eab5171987623ade90fc6893d6a7ec16e014b8e6690c2a2ff94df8dbbda36eef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cca2cef-ea53-4391-9ede-4cfd66ee2781", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8d64af21015c9221bca9f63753718460a15238f9bbbf9716c32d56f01ece9988", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1eca416-d4b7-44d1-bc88-2321df80aff0", "node_type": "1", "metadata": {}, "hash": "e8c44df30e1312f2c50637ddb7c7e8a60eafb1c6b548e700a4bb0835ce9569d3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cleverhans\r\nlink: https://github.com/cleverhans-lab/cleverhans\r\ndescription: CleverHans is a Python library designed to benchmark machine learning systems' vulnerability to adversarial examples. It contains reference implementations of attacks against machine learning models to help with benchmarking models against adversarial examples. Since v4.0.0, CleverHans supports JAX, PyTorch, and TF2, and welcomes contributions for all three frameworks. To set up CleverHans, one needs to install Jax, PyTorch, or TensorFlow 2. Users can install CleverHans using pip or by cloning the Github repository. CleverHans offers tutorials and examples to demonstrate its functionalities, but these should not be considered part of the API. When reporting benchmarks, users are asked to specify the version of CleverHans used. The name CleverHans refers to a story about a horse that appeared to have learned how to answer arithmetic questions but had only learned to read social cues.", "start_char_idx": 0, "end_char_idx": 983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1eca416-d4b7-44d1-bc88-2321df80aff0": {"__data__": {"id_": "c1eca416-d4b7-44d1-bc88-2321df80aff0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9330043a-e89b-4a01-bd74-7ee21012814d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eab5171987623ade90fc6893d6a7ec16e014b8e6690c2a2ff94df8dbbda36eef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae4fe739-1d37-43aa-95b2-f023540b8c1c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5a4829efc5240b229584da43a1cbacbd63d5b8248723a8531679d2150a81ecc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "752c5dea-e847-4b7a-ba6e-f88fc44a3e46", "node_type": "1", "metadata": {}, "hash": "b3da101ea335067a297c44ed53bfe64af9303546bb40fd474b9f72ec32716dd0", "class_name": "RelatedNodeInfo"}}, "text": "It contains reference implementations of attacks against machine learning models to help with benchmarking models against adversarial examples. Since v4.0.0, CleverHans supports JAX, PyTorch, and TF2, and welcomes contributions for all three frameworks. To set up CleverHans, one needs to install Jax, PyTorch, or TensorFlow 2. Users can install CleverHans using pip or by cloning the Github repository. CleverHans offers tutorials and examples to demonstrate its functionalities, but these should not be considered part of the API. When reporting benchmarks, users are asked to specify the version of CleverHans used. The name CleverHans refers to a story about a horse that appeared to have learned how to answer arithmetic questions but had only learned to read social cues. The library is maintained by the CleverHans Lab at the University of Toronto and was previously maintained by Ian Goodfellow and Nicolas Papernot.", "start_char_idx": 206, "end_char_idx": 1130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "752c5dea-e847-4b7a-ba6e-f88fc44a3e46": {"__data__": {"id_": "752c5dea-e847-4b7a-ba6e-f88fc44a3e46", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\2.txt", "file_name": "2.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b2ba341-d75d-4f78-a10c-634f35ab3bab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\2.txt", "file_name": "2.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "77762e06e416d926609ccc5f0cf9b54dff0f3229542eb85fd195cd0288e2c2d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1eca416-d4b7-44d1-bc88-2321df80aff0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "38179dc80f9b79ae5d2dfd1960e8cae0c1e07fbd36125784ae6b40fbbd758efc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62c2013e-4ec7-43d5-9d5f-f48aa75e8c24", "node_type": "1", "metadata": {}, "hash": "74fdfaa4a8e08a6d6abe997fdd39ffff94bfc2ba697f5e275b1c4609ff7c033b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: fastText\r\nlink: https://github.com/facebookresearch/fastText\r\ndescription: FastText is a library for efficient learning of word representations and sentence classification. It has two main use cases: word representation learning and text classification. These were described in two papers, and the library is constantly being built and tested under various docker images using circleci. FastText builds on modern Mac OS and Linux distributions, and requires a compiler with good C++11 support. The library can be built using make or cmake, and requires certain dependencies. Full documentation is available in the repository, and the library is MIT-licensed.", "start_char_idx": 0, "end_char_idx": 669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62c2013e-4ec7-43d5-9d5f-f48aa75e8c24": {"__data__": {"id_": "62c2013e-4ec7-43d5-9d5f-f48aa75e8c24", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\20.txt", "file_name": "20.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7f6fea0-7202-41e8-b8a8-6941cb6d0b60", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\20.txt", "file_name": "20.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "322f274514154b6e0be47c428044906d9640f64a6fa7d02739c91cf7e03f3b1d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "752c5dea-e847-4b7a-ba6e-f88fc44a3e46", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\2.txt", "file_name": "2.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f9dbb857bbf413992bdcef1dff9cab0be15e57edf8693b12fc887210edfdea14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ddc91e7-660b-4569-b184-8e72c9b45557", "node_type": "1", "metadata": {}, "hash": "8e8ac359fbd32594acb0a6545e63ac0b26bb993240a4c54101d11106013f95b6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: parquet-mr\r\nlink: https://github.com/apache/parquet-mr\r\ndescription: Parquet-MR contains the java implementation of the Parquet format. Parquet is a columnar storage format for Hadoop; it provides efficient storage and encoding of data. Parquet uses the record shredding and assembly algorithm described in the Dremel paper to represent nested structures. Parquet-MR uses Maven to build and depends on the thrift compiler (protoc is now managed by maven plugin). Parquet is a very active project, and new features are being added quickly. The project has Java Vector API support and Map/Reduce integration for input and output formats. Parquet also has implementations for Thrift, Avro, and Protobuf conversion. The project also integrates with Apache Pig and Hive. To contribute to the project, pull requests can be made against the parquet-mr Git repository.", "start_char_idx": 0, "end_char_idx": 871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ddc91e7-660b-4569-b184-8e72c9b45557": {"__data__": {"id_": "8ddc91e7-660b-4569-b184-8e72c9b45557", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\200.txt", "file_name": "200.txt", "file_type": "text/plain", "file_size": 566, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee049f5d-5fa1-41ba-867e-f34dbbc1b52a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\200.txt", "file_name": "200.txt", "file_type": "text/plain", "file_size": 566, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5637fb012e317b911eb22d318b900aeba055d50f34c8b9dc2dbc405495974b84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62c2013e-4ec7-43d5-9d5f-f48aa75e8c24", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\20.txt", "file_name": "20.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "80d06f999bc43012a30226b550d80074e69bc02a8d3220c3de37c6b3cbb9b35e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f36c5620-cb17-49a3-844b-9748ef2a0751", "node_type": "1", "metadata": {}, "hash": "ae5e92148dd3741db1a99f881898adb5385b72b5acfcabffa8de6d68169a1d86", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: adversarial-robustness-toolbox\r\nlink: https://github.com/Trusted-AI/adversarial-robustness-toolbox\r\ndescription: The Adversarial Robustness Toolbox (ART) is a Python library for Machine Learning Security that provides tools to defend and evaluate Machine Learning models and applications against adversarial threats. It supports all popular machine learning frameworks, data types and machine learning tasks. ART is hosted by the Linux Foundation AI & Data Foundation and was partially supported by the Defense Advanced Research Projects Agency (DARPA).", "start_char_idx": 0, "end_char_idx": 564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f36c5620-cb17-49a3-844b-9748ef2a0751": {"__data__": {"id_": "f36c5620-cb17-49a3-844b-9748ef2a0751", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\201.txt", "file_name": "201.txt", "file_type": "text/plain", "file_size": 822, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2605691e-e220-4ece-a16c-f715af41d148", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\201.txt", "file_name": "201.txt", "file_type": "text/plain", "file_size": 822, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cada060f67b3f30c37ec6406188dd4b1079a00997cc9ea59c34a619fdee0ffdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ddc91e7-660b-4569-b184-8e72c9b45557", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\200.txt", "file_name": "200.txt", "file_type": "text/plain", "file_size": 566, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2402efca3eca46895c5caec0deacbe314195f18c77a409fe355c19d8415df2ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26823ed9-fdfa-459d-af1d-029b47342a51", "node_type": "1", "metadata": {}, "hash": "f4acc351d4165903a45ee2512f040d85437b7625bc621ce2802226b26aa33701", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: EvadeML-Zoo\r\nlink: https://github.com/mzweilin/EvadeML-Zoo\r\ndescription: EvadeML-Zoo is a GitHub repository that provides pre-trained models for adversarial attacks and defenses on various datasets. The goal of this project is to enable researchers and practitioners to evaluate the robustness of machine learning models and develop better defenses against adversarial attacks. In this section, we will explain how to use EvadeML-Zoo in your workflow. The installation process involves steps like installing dependencies, fetching submodules, and downloading pre-trained models. Once installed, you can use the provided Python scripts to generate adversarial examples and test the robustness of your models. You are encouraged to cite the provided research papers if you use EvadeML-Zoo for academic research.", "start_char_idx": 0, "end_char_idx": 820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26823ed9-fdfa-459d-af1d-029b47342a51": {"__data__": {"id_": "26823ed9-fdfa-459d-af1d-029b47342a51", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\202.txt", "file_name": "202.txt", "file_type": "text/plain", "file_size": 79, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2eadce5d-225b-4b96-baf5-d94f122f10c3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\202.txt", "file_name": "202.txt", "file_type": "text/plain", "file_size": 79, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c8a9bfca37a8f6372565e586b573410147530ad8f535dbf0b53bfabbec84bdd4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f36c5620-cb17-49a3-844b-9748ef2a0751", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\201.txt", "file_name": "201.txt", "file_type": "text/plain", "file_size": 822, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "44114687710598ce1a3069f45ed85fba965da365a49f18afadafd8aea82e1530", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04ecdf90-0ecd-41d9-8d58-bcfa8b494f7d", "node_type": "1", "metadata": {}, "hash": "553daadfe6991190e7951ee79c2ae33a5cf013e65e5e960e594fba8312b8611a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: foolbox\r\nlink: https://github.com/bethgelab/foolbox\r\ndescription:", "start_char_idx": 0, "end_char_idx": 76, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04ecdf90-0ecd-41d9-8d58-bcfa8b494f7d": {"__data__": {"id_": "04ecdf90-0ecd-41d9-8d58-bcfa8b494f7d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\203.txt", "file_name": "203.txt", "file_type": "text/plain", "file_size": 1020, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58468981-c200-480c-afe3-3d74915299c0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\203.txt", "file_name": "203.txt", "file_type": "text/plain", "file_size": 1020, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9d2acf0db35e561c64e8a85ef1be16da4171e063fb72c763ff4231043ddf64f8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26823ed9-fdfa-459d-af1d-029b47342a51", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\202.txt", "file_name": "202.txt", "file_type": "text/plain", "file_size": 79, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fa23ac1c2b2aa9523f6793ea6b2c59ef016e727dda4cbf75a8c4a075404f90a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75d590e1-289b-4a34-8810-2f0e6cefbc29", "node_type": "1", "metadata": {}, "hash": "318ab038e3b65436db8d8cf08817c475144b63922148b3dd3a48b77760a462d9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: artificial-adversary\r\nlink: https://github.com/airbnb/artificial-adversary\r\ndescription: When classifying user-generated text, there are many ways that users can modify their content to avoid detection. This library called \"Artificial-Adversary\" allows you to generate texts using these methods and simulate these kinds of attacks on your machine learning models. By exposing your model to these texts offline, you will be able to better prepare for them when you encounter them in an online setting. This project is under active development and can be installed. The library allows users to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. The library also funds open-source developers and provides community articles. Included in the library are various attacks such as generating attacked texts and simulating attacks on texts. Contributing and acknowledgments are also encouraged.", "start_char_idx": 0, "end_char_idx": 1018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75d590e1-289b-4a34-8810-2f0e6cefbc29": {"__data__": {"id_": "75d590e1-289b-4a34-8810-2f0e6cefbc29", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\204.txt", "file_name": "204.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a62499a-831f-423d-a292-0f923580462e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\204.txt", "file_name": "204.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4a96f6c18d00949ae17cce3a7a09ec50837521a080c724403122b8311f813c0c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04ecdf90-0ecd-41d9-8d58-bcfa8b494f7d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\203.txt", "file_name": "203.txt", "file_type": "text/plain", "file_size": 1020, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e811f1e163e9c0b8ea14efe0f39f1af894744591cae2d13df5cc2b2f95519e91", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca672886-aefa-4d24-bc2b-1b07acc31ae9", "node_type": "1", "metadata": {}, "hash": "3da70674a215adab08f998fd2ef89967a42e30cf986260350cb9c13666b3659d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: advertorch\r\nlink: https://github.com/BorealisAI/advertorch\r\ndescription: AdverTorch is a Python toolbox for adversarial robustness research. The primary functionalities are implemented in PyTorch. Specifically, AdverTorch contains modules for generating adversarial perturbations and defending against adversarial examples, also scripts for adversarial training. To install AdverTorch, simply run `pip install advertorch` or clone the repo and run `python setup.py install`.", "start_char_idx": 0, "end_char_idx": 485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca672886-aefa-4d24-bc2b-1b07acc31ae9": {"__data__": {"id_": "ca672886-aefa-4d24-bc2b-1b07acc31ae9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c04190e-dc1d-4cf4-ad1b-4b4b2219d1ea", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "71c4000e6131c08a640826181fc8a33ce493b2371ca352277d737de1c2aaa57e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75d590e1-289b-4a34-8810-2f0e6cefbc29", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\204.txt", "file_name": "204.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8a1190d64dcb1aec67240f3d90262d7f1bf3a9933a2d364e6e587a575b1b3cbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed9dd215-94d9-45d1-9aa9-413e5e884fe4", "node_type": "1", "metadata": {}, "hash": "e6a2d43deeb899ca03db34894e6ba7bb51bd6dbd90a6cdf981869e932125a866", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: alibi-detect\r\nlink: https://github.com/SeldonIO/alibi-detect\r\ndescription: Alibi Detect is an open source Python library focused on outlier, adversarial, and drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. Both TensorFlow and PyTorch backends are supported for drift detection. For more background on the importance of monitoring outliers and distributions in a production setting, check out this talk from the Challenges in Deploying and Monitoring Machine Learning Systems ICML 2020 workshop, based on the paper Monitoring and explainability of models in production and referencing Alibi Detect. For a thorough introduction to drift detection, check out Protecting Your Machine Learning Against Drift: An Introduction. The package, alibi-detect can be installed from conda-forge using mamba. The supported algorithms include Outlier Detection, Adversarial Detection, and Drift Detection.", "start_char_idx": 0, "end_char_idx": 977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed9dd215-94d9-45d1-9aa9-413e5e884fe4": {"__data__": {"id_": "ed9dd215-94d9-45d1-9aa9-413e5e884fe4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c04190e-dc1d-4cf4-ad1b-4b4b2219d1ea", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "71c4000e6131c08a640826181fc8a33ce493b2371ca352277d737de1c2aaa57e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca672886-aefa-4d24-bc2b-1b07acc31ae9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ec2188e8bbfbf7883876e1a14075a052bdaa1a06ff5cd95f3ee2b4bceb3e4010", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb91bf48-e379-4f69-ba31-5b99556a9066", "node_type": "1", "metadata": {}, "hash": "dc13a999e1f2e4febf0e26ada8058a179165387b75abce281d91d5142bd81597", "class_name": "RelatedNodeInfo"}}, "text": "The package aims to cover both online and offline detectors for tabular data, text, images and time series. Both TensorFlow and PyTorch backends are supported for drift detection. For more background on the importance of monitoring outliers and distributions in a production setting, check out this talk from the Challenges in Deploying and Monitoring Machine Learning Systems ICML 2020 workshop, based on the paper Monitoring and explainability of models in production and referencing Alibi Detect. For a thorough introduction to drift detection, check out Protecting Your Machine Learning Against Drift: An Introduction. The package, alibi-detect can be installed from conda-forge using mamba. The supported algorithms include Outlier Detection, Adversarial Detection, and Drift Detection. Alibi-detect is integrated in the open source machine learning model deployment platform Seldon Core and model serving framework KFServing.", "start_char_idx": 186, "end_char_idx": 1117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb91bf48-e379-4f69-ba31-5b99556a9066": {"__data__": {"id_": "eb91bf48-e379-4f69-ba31-5b99556a9066", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\206.txt", "file_name": "206.txt", "file_type": "text/plain", "file_size": 746, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e469725b-c578-4a35-8d2a-f04eb507b78c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\206.txt", "file_name": "206.txt", "file_type": "text/plain", "file_size": 746, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e9ecb0ebf721b472b2b672d475cfd22847d017e92b886d064a4745c303784a76", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed9dd215-94d9-45d1-9aa9-413e5e884fe4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4a5ca8155654db3037b100a0b6da924b123b0b6e98d044028e01163b0df8e643", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6b4e491-e748-4eaa-b25c-9ad59b7f243b", "node_type": "1", "metadata": {}, "hash": "9a053ad3d75f1d74d87208102af14142c4047aeeb405c37a7f8aed7c47204dde", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AdvBox\r\nlink: https://github.com/advboxes/AdvBox\r\ndescription: \"Advbox Family is a series of AI model security tools set of Baidu Open.Source,including the generation, detection and protection of adversarial.examples, as well as attack and defense cases for different AI applications. Advbox Family support Python 3.*. A Lightweight Adv SDK For PaddlePaddle to generate adversarial examples. Adversarialbox is a toolbox to generate adversarial examples that fool neural.networks in PaddlePaddle\u3001PyTorch\u3001Caffe2\u3001MxNet\u3001Keras\u3001TensorFlow and Advbox can.benchmark the robustness of machine learning models.Advbox give a command line.tool to generate adversarial examples with Zero-Coding. It is inspired and.based on FoolBox v1.\"", "start_char_idx": 0, "end_char_idx": 734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6b4e491-e748-4eaa-b25c-9ad59b7f243b": {"__data__": {"id_": "f6b4e491-e748-4eaa-b25c-9ad59b7f243b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\207.txt", "file_name": "207.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23350226-4c59-40e4-918e-83833f39ef1e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\207.txt", "file_name": "207.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bc734204673af921b7592ca16abf49ea64ce4de9c473ce990a1a51d8f99829c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb91bf48-e379-4f69-ba31-5b99556a9066", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\206.txt", "file_name": "206.txt", "file_type": "text/plain", "file_size": 746, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "38fd0fa98568144521d7aa54042b73448e201e70d6d9382635cd1f9fd32e308d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4323255-e2a1-4cc9-acec-24a2eb3f55f1", "node_type": "1", "metadata": {}, "hash": "6418f844425f040b2968ba5f2bcbd681289901579dcc798f5320ef3c62b53a7d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AdversarialDNN-Playground\r\nlink: https://github.com/QData/AdversarialDNN-Playground\r\ndescription: The AdversarialDNN-Playground is a web service that allows users to visualize the creation of adversarial samples to neural networks. It is similar to Google's TensorFlow Playground, but focuses on evasion attacks in adversarial machine learning. Information on its installation can be found in the repository, along with screenshots and demo information. To modify seed images, users can edit the `images_to_generate.csv` file and run the `json_gen.py` script. The webserver can be deployed using the `run.py` script, and the required Python packages can be installed using the `requirements.txt` file with the command `pip3 install -r requirements.txt`.", "start_char_idx": 0, "end_char_idx": 764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4323255-e2a1-4cc9-acec-24a2eb3f55f1": {"__data__": {"id_": "b4323255-e2a1-4cc9-acec-24a2eb3f55f1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\208.txt", "file_name": "208.txt", "file_type": "text/plain", "file_size": 263, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9de24fff-46db-406e-8f9f-5d9fdabd019c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\208.txt", "file_name": "208.txt", "file_type": "text/plain", "file_size": 263, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d81099d130b20e39809400db5eeb70e99f10afa284ff795b2845aba279b2a3f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6b4e491-e748-4eaa-b25c-9ad59b7f243b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\207.txt", "file_name": "207.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "caf2e19240a4a17fa6b6d52db4402ff1f1e0f613c8b28f8378d83398c002fb92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c17069b-fc3f-4dca-ad04-760ff48b11d1", "node_type": "1", "metadata": {}, "hash": "60aeea8eec8e441a00a8fa899610e647eeb50e587b3cd2fbf42d933f325fb192", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: whylogs\r\nlink: https://github.com/whylabs/whylogs\r\ndescription: Have you ever needed a simple and efficient logging tool to help you understand the distribution and patterns of your data without sacrificing performance? That's where whylogs comes in.", "start_char_idx": 0, "end_char_idx": 261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c17069b-fc3f-4dca-ad04-760ff48b11d1": {"__data__": {"id_": "4c17069b-fc3f-4dca-ad04-760ff48b11d1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\209.txt", "file_name": "209.txt", "file_type": "text/plain", "file_size": 739, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6da94435-69a2-4804-b6db-5650b426e935", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\209.txt", "file_name": "209.txt", "file_type": "text/plain", "file_size": 739, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "077ecd76a5a3aea3adce633363639bc9a73dd4a03b1347a97031ff2e49731639", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4323255-e2a1-4cc9-acec-24a2eb3f55f1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\208.txt", "file_name": "208.txt", "file_type": "text/plain", "file_size": 263, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "531c5316cfd15cd749c4852d70ced2a7b2a375f57b76ff7752884fa98455386b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daf99d3c-e5ee-46b5-bfc7-b3df897585db", "node_type": "1", "metadata": {}, "hash": "023803ae8ab17561ce63c0dd00794b6a09853712a8581bafcb329d7a0a8306b6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pytorch\r\nlink: https://github.com/pytorch/pytorch\r\ndescription: PyTorch is a Python package that provides two high-level features: A GPU-Ready Tensor Library and Dynamic Neural Networks with Tape-Based Autograd. PyTorch is not a Python binding into a monolithic C++ framework and is built to be deeply integrated into Python. You can use it naturally like you would use NumPy/SciPy/scikit-learn etc. PyTorch is designed to be intuitive, linear in thought, and easy to use, and has minimal framework overhead. PyTorch is currently maintained by Soumith Chintala, Gregory Chanan, Dmytro Dzhulgakov, Edward Yang, and Nikita Shulga, with major contributions coming from hundreds of talented individuals in various forms and means.", "start_char_idx": 0, "end_char_idx": 737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "daf99d3c-e5ee-46b5-bfc7-b3df897585db": {"__data__": {"id_": "daf99d3c-e5ee-46b5-bfc7-b3df897585db", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e315a7f-841b-42e9-8caa-7da2831a2a23", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "044acc9e51a6a068a5e6d91ddd14d1b3996f16523199a6dc1cd9b3f973e57dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c17069b-fc3f-4dca-ad04-760ff48b11d1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\209.txt", "file_name": "209.txt", "file_type": "text/plain", "file_size": 739, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7c18ca465b8f18ed2f93dd1cf498fe553817045581c281acacfc44b64e42f45a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "134b5d8f-36eb-4103-b62d-a076642f0697", "node_type": "1", "metadata": {}, "hash": "698927c2df9d2c2b12b42a21009b5007016f34896e42675ba61a1aee2e8de5dc", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: arrow\r\nlink: https://github.com/apache/arrow\r\ndescription: Introducing the Apache Arrow project, it is a development platform for in-memory analytics. It contains numerous technologies that enable big data systems to process and move data quickly. The major components of the project include: Automate any workflow, Host and manage packages, Find and fix vulnerabilities, Instant dev environments, Write better code with AI, Collaborate outside of code, Fund open source developers, and GitHub community articles. GitHub desktop can be used to open the project, and it is an Apache Software Foundation project. The Arrow libraries contain many distinct software components, and the official Arrow libraries in this repository are in different stages of implementing the Arrow format and related features. Additionally, the current feature matrix can be found on git main. Please read the latest project contribution guide, as everyone is welcome to be involved in the project, even if they do not plan to contribute to Apache Arrow itself or Arrow integrations in other projects.", "start_char_idx": 0, "end_char_idx": 1090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "134b5d8f-36eb-4103-b62d-a076642f0697": {"__data__": {"id_": "134b5d8f-36eb-4103-b62d-a076642f0697", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e315a7f-841b-42e9-8caa-7da2831a2a23", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "044acc9e51a6a068a5e6d91ddd14d1b3996f16523199a6dc1cd9b3f973e57dbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "daf99d3c-e5ee-46b5-bfc7-b3df897585db", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e47ac800ca6f1b4fd17408fadb981ea43099c543e1db43e7034653475be245f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f25f9bc9-0825-4180-b32d-58ac30620618", "node_type": "1", "metadata": {}, "hash": "ebd5960bb794af8d950b9c48616245aaee362d2b71213de0d6e386815967271b", "class_name": "RelatedNodeInfo"}}, "text": "It contains numerous technologies that enable big data systems to process and move data quickly. The major components of the project include: Automate any workflow, Host and manage packages, Find and fix vulnerabilities, Instant dev environments, Write better code with AI, Collaborate outside of code, Fund open source developers, and GitHub community articles. GitHub desktop can be used to open the project, and it is an Apache Software Foundation project. The Arrow libraries contain many distinct software components, and the official Arrow libraries in this repository are in different stages of implementing the Arrow format and related features. Additionally, the current feature matrix can be found on git main. Please read the latest project contribution guide, as everyone is welcome to be involved in the project, even if they do not plan to contribute to Apache Arrow itself or Arrow integrations in other projects. Finally, the footer navigation can be accessed for further information, and the commands mentioned in the provided sentence may cause unexpected behavior.", "start_char_idx": 162, "end_char_idx": 1245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f25f9bc9-0825-4180-b32d-58ac30620618": {"__data__": {"id_": "f25f9bc9-0825-4180-b32d-58ac30620618", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\210.txt", "file_name": "210.txt", "file_type": "text/plain", "file_size": 996, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7d0d0c3-f210-4a45-88f8-3cd5930ddfa7", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\210.txt", "file_name": "210.txt", "file_type": "text/plain", "file_size": 996, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e6449f43a58da956830d5e3f7ecbfe77c48daa31ff5dfeea3f150ccf6baac6b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "134b5d8f-36eb-4103-b62d-a076642f0697", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "345e7c329e3d2f892116ca5eddfb1d7f6bfa531fbf3bd4315bb4ad1ffa1e2bbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2c5e5f8-b6eb-4c42-bc5b-94d6343a0699", "node_type": "1", "metadata": {}, "hash": "02ca904a6e4c70ee30df604b335a56f06c67489f941d5bbfd53d9b0552011215", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: tensorflow\r\nlink: https://github.com/tensorflow/tensorflow\r\ndescription: TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence Research organization to conduct machine learning and deep neural networks research. The system is general enough to be applicable in a wide variety of other domains, as well. TensorFlow provides stable Python and C++ APIs, as well as non-guaranteed backward compatible API for other languages. Keep up-to-date with release announcements and security updates by subscribing to announce@tensorflow.org and see the TensorFlow install guide for the pip package, to enable GPU support, use a Docker container, and build from source. For more examples, see the TensorFlow tutorials. If you want to contribute to TensorFlow, be sure to review the contribution guidelines. This project adheres to TensorFlow's code of conduct. By participating, you are expected to uphold this code.", "start_char_idx": 0, "end_char_idx": 994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2c5e5f8-b6eb-4c42-bc5b-94d6343a0699": {"__data__": {"id_": "e2c5e5f8-b6eb-4c42-bc5b-94d6343a0699", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\211.txt", "file_name": "211.txt", "file_type": "text/plain", "file_size": 852, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da8a2c30-5865-4755-9d0c-39b1f3447d96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\211.txt", "file_name": "211.txt", "file_type": "text/plain", "file_size": 852, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "97e96d3d3f6339db59c98de7213d68c70eb058784cc80fd96182a6655fefed1b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f25f9bc9-0825-4180-b32d-58ac30620618", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\210.txt", "file_name": "210.txt", "file_type": "text/plain", "file_size": 996, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0d54f16715abcfae7fc42e86796c8d9b054a435da4eff33ce413d029b1fbe16f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a0f882f-b024-4086-baba-6f005b77c993", "node_type": "1", "metadata": {}, "hash": "9af98407450b70b1d68656e80e66b2895cc488ce1daf8fbedff1ab0ba13f71f4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: seldon-core\r\nlink: https://github.com/SeldonIO/seldon-core\r\ndescription: Seldon Core is an open source platform to deploy machine learning models on Kubernetes at massive scale. Seldon Core converts your ML models or language wrappers into production REST/GRPC microservices. It provides advanced machine learning capabilities out of the box, such as advanced metrics, request logging, explainers, outlier detectors, A/B tests, canaries, and more. With over 2M installs, Seldon Core is used across organisations to manage large scale deployment of machine learning models, and key benefits include automating any workflow, hosting and managing packages, finding and fixing vulnerabilities, instant dev environments, writing better code with AI, and collaborating outside of code. The installation process is described in the documentation.", "start_char_idx": 0, "end_char_idx": 850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a0f882f-b024-4086-baba-6f005b77c993": {"__data__": {"id_": "1a0f882f-b024-4086-baba-6f005b77c993", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\212.txt", "file_name": "212.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abf962b4-bb99-43a7-bdaa-4bc4281791ee", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\212.txt", "file_name": "212.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8b9393fd0c49c5df4d8e340d03c57fa8d1edc2303108cc98b50ebdb6374ace55", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2c5e5f8-b6eb-4c42-bc5b-94d6343a0699", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\211.txt", "file_name": "211.txt", "file_type": "text/plain", "file_size": 852, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9fc43a667564aadd0ffb86503aec82b33ad932f3ccbbd9e186c6a21463714202", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58e09746-c6a2-4c4a-ad7a-7e16a3c092d9", "node_type": "1", "metadata": {}, "hash": "bb6430cc400b0013aba29647b0bb0b66e5490ba158efd81acb21ad8caf816143", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: RedisAI\r\nlink: https://github.com/RedisAI/RedisAI\r\ndescription: RedisAI is a Redis module for executing Deep Learning/Machine Learning models and managing their data. Its purpose is being a \"workhorse\" for model serving, by providing out-of-the-box support for popular DL/ML frameworks and unparalleled performance. RedisAI both maximizes computation throughput and reduces latency by adhering to the principle of data locality, as well as simplifies the deployment and serving of graphs by leveraging on Redis's production-proven infrastructure. To read RedisAI docs, visit redisai.io. To see RedisAI in action, visit the demos page. RedisAI is a Redis module. To run it you'll need a Redis server (v6.0.0 or greater), the module's shared library, and its dependencies. The following sections describe how to get started with RedisAI.", "start_char_idx": 0, "end_char_idx": 846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58e09746-c6a2-4c4a-ad7a-7e16a3c092d9": {"__data__": {"id_": "58e09746-c6a2-4c4a-ad7a-7e16a3c092d9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\213.txt", "file_name": "213.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49bf831c-28bd-4d22-b81e-edd06f3606d4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\213.txt", "file_name": "213.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c273d6996eb2a2300d56911793e349a4e730aef9d3ae330d58c12f642437ea51", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a0f882f-b024-4086-baba-6f005b77c993", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\212.txt", "file_name": "212.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4f286eed46d8ebea0c1588b802d7385895dc28d9691110c0999b447f9127b30a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1369991c-bb24-4d9f-b3a3-bbac43b344fb", "node_type": "1", "metadata": {}, "hash": "f5925968b4bd22ea33c14e24bd9821bba9700594dd1ac60258a1f0d6bb07897d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: server\r\nlink: https://github.com/triton-inference-server/server\r\ndescription: Triton Inference Server is an open source inference serving software that streamlines AI inferencing. Triton enables teams to deploy any AI model from multiple deep learning and machine learning frameworks, including TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more. Triton supports inference across cloud, data center,edge and embedded devices on NVIDIA GPUs, x86 and ARM CPU, or AWS Inferentia. Triton delivers optimized performance for many query types, including real time, batched, ensembles and audio/video streaming. Major features include serving models in 3 easy steps and client support and examples, with additional documentation for contributions and troubleshooting tips available through the NVIDIA Developer Triton page.", "start_char_idx": 0, "end_char_idx": 851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1369991c-bb24-4d9f-b3a3-bbac43b344fb": {"__data__": {"id_": "1369991c-bb24-4d9f-b3a3-bbac43b344fb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58e09746-c6a2-4c4a-ad7a-7e16a3c092d9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\213.txt", "file_name": "213.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a333431b7406502bd2a3f7311cf41c7d139ecd427444da297d75fb47c47626ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdcc1de5-1d06-404d-b7d0-dec8cb8bbeb1", "node_type": "1", "metadata": {}, "hash": "95c4e4e94f128a76574c92f0b2a22079f5e13732b602e6bf67c61ba95e684730", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: openscoring\r\nlink: https://github.com/openscoring/openscoring\r\ndescription: REST web service for scoring PMML models. Openscoring client and server uber-JAR files are distributed via the GitHub releases page, and the Openscoring webapp WAR file is distributed via the Maven Central repository. This README file corresponds to latest source code snapshot. In order to follow its instructions as closely as possible, it's recommended to download the latest binary release. The current version is **2.1.1** (25 September, 2022): ### Source code snapshot. Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar.", "start_char_idx": 0, "end_char_idx": 917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdcc1de5-1d06-404d-b7d0-dec8cb8bbeb1": {"__data__": {"id_": "bdcc1de5-1d06-404d-b7d0-dec8cb8bbeb1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1369991c-bb24-4d9f-b3a3-bbac43b344fb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b9af886b2c1aca1b410060eb4dbb0757deebda9b18029d6a7116de445ae95610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8645e359-c24e-4831-9e00-2982201276a2", "node_type": "1", "metadata": {}, "hash": "ca88cd3d7e8c56335c71a32b34506a303a4e475da9204d5def47a4a42fca26d1", "class_name": "RelatedNodeInfo"}}, "text": "Openscoring client and server uber-JAR files are distributed via the GitHub releases page, and the Openscoring webapp WAR file is distributed via the Maven Central repository. This README file corresponds to latest source code snapshot. In order to follow its instructions as closely as possible, it's recommended to download the latest binary release. The current version is **2.1.1** (25 September, 2022): ### Source code snapshot. Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring.", "start_char_idx": 129, "end_char_idx": 999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8645e359-c24e-4831-9e00-2982201276a2": {"__data__": {"id_": "8645e359-c24e-4831-9e00-2982201276a2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdcc1de5-1d06-404d-b7d0-dec8cb8bbeb1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2e69f030324ad1ce71bd84df7e4449ee65a87a95bfd6ebe4660236eaefed3ef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3da233cb-2dcd-42dc-aa35-7ab800d73106", "node_type": "1", "metadata": {}, "hash": "14a0a79dc19f2cea85857bc58e2bdd2481ed7a2d37d6e1488e04fc6121c77cca", "class_name": "RelatedNodeInfo"}}, "text": "Openscoring client and server uber-JAR files are distributed via the GitHub releases page, and the Openscoring webapp WAR file is distributed via the Maven Central repository. This README file corresponds to latest source code snapshot. In order to follow its instructions as closely as possible, it's recommended to download the latest binary release. The current version is **2.1.1** (25 September, 2022): ### Source code snapshot. Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring. The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes.", "start_char_idx": 129, "end_char_idx": 1128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3da233cb-2dcd-42dc-aa35-7ab800d73106": {"__data__": {"id_": "3da233cb-2dcd-42dc-aa35-7ab800d73106", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8645e359-c24e-4831-9e00-2982201276a2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "700540bc4917d8754e59b2987528396b01d023d9942bd37c6ae26c31accb48ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e84cf80-c3aa-4b98-ba68-f222210f952d", "node_type": "1", "metadata": {}, "hash": "5573a6a02bff4ffacefbd75d22bff87f7cea64a9a3442b37459761c221e92a02", "class_name": "RelatedNodeInfo"}}, "text": "This README file corresponds to latest source code snapshot. In order to follow its instructions as closely as possible, it's recommended to download the latest binary release. The current version is **2.1.1** (25 September, 2022): ### Source code snapshot. Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring. The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes. Please specify `--help` for more information.", "start_char_idx": 305, "end_char_idx": 1174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e84cf80-c3aa-4b98-ba68-f222210f952d": {"__data__": {"id_": "3e84cf80-c3aa-4b98-ba68-f222210f952d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3da233cb-2dcd-42dc-aa35-7ab800d73106", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "29be9cc47ae619eabd304f88ee1e0745689b33d6e2c6d8e0a46e40eacf61d136", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac9f7bfc-f801-4f82-bbe2-5b99c7af3adb", "node_type": "1", "metadata": {}, "hash": "dc696b41da5f76e5aab1c6f35784137b84b29c73fde6875ac99e20d3ec3dfa64", "class_name": "RelatedNodeInfo"}}, "text": "This README file corresponds to latest source code snapshot. In order to follow its instructions as closely as possible, it's recommended to download the latest binary release. The current version is **2.1.1** (25 September, 2022): ### Source code snapshot. Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring. The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes. Please specify `--help` for more information. Advanced configuration can be done by copying the sample Typesafe's Config configuration file `openscoring-.server/application.conf.sample` to a new file `application.conf`, and customize its content to current needs.", "start_char_idx": 305, "end_char_idx": 1392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac9f7bfc-f801-4f82-bbe2-5b99c7af3adb": {"__data__": {"id_": "ac9f7bfc-f801-4f82-bbe2-5b99c7af3adb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e84cf80-c3aa-4b98-ba68-f222210f952d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f6d9994e0e1ac56ebd67dc9bdde6736c3e61bbaf058305cea220cdfe5dbf408a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c532ca25-eb20-40e6-9c02-fa71216ac39f", "node_type": "1", "metadata": {}, "hash": "c70b662a6fdbf4b88e06d205b2883137981cf156eb801d529bffbbb6cf649af7", "class_name": "RelatedNodeInfo"}}, "text": "Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring. The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes. Please specify `--help` for more information. Advanced configuration can be done by copying the sample Typesafe's Config configuration file `openscoring-.server/application.conf.sample` to a new file `application.conf`, and customize its content to current needs. Use the `config.file` system property to impose changes on the JVM.", "start_char_idx": 563, "end_char_idx": 1460, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c532ca25-eb20-40e6-9c02-fa71216ac39f": {"__data__": {"id_": "c532ca25-eb20-40e6-9c02-fa71216ac39f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac9f7bfc-f801-4f82-bbe2-5b99c7af3adb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "458fba9a5360e0f75f061d08950d147ccf8a5b55b0248fb51d7ce4822449f030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c114f88a-5e71-4d16-aa73-f6248f66031d", "node_type": "1", "metadata": {}, "hash": "e5e900c17afd2b90f521e5fbef1992c12c97ae6c71189bc240375c2c93f32357", "class_name": "RelatedNodeInfo"}}, "text": "Enter the project root directory and build using Apache Maven: The build produces two uber-JAR files and a WAR file. The example PMML file `DecisionTreeIris.pmml` along with example JSON and CSV files can be found in the `openscoring-service/src/etc` directory. Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring. The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes. Please specify `--help` for more information. Advanced configuration can be done by copying the sample Typesafe's Config configuration file `openscoring-.server/application.conf.sample` to a new file `application.conf`, and customize its content to current needs. Use the `config.file` system property to impose changes on the JVM. Moreover, the local configuration overrides the default configuration that is defined in the reference REST web service configuration file `openscoring-.service/src/main/reference.conf`.", "start_char_idx": 563, "end_char_idx": 1647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c114f88a-5e71-4d16-aa73-f6248f66031d": {"__data__": {"id_": "c114f88a-5e71-4d16-aa73-f6248f66031d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c532ca25-eb20-40e6-9c02-fa71216ac39f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "488671e14e25677afdded70846eb44a2da125da719ba3023b5264a0dc0640fcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "266070a7-8688-4e82-8382-6902cc9617c5", "node_type": "1", "metadata": {}, "hash": "ccef6e298759cfc52ba1c41eaed9be8d0273dc8accb48f77dac785c1d055b4b6", "class_name": "RelatedNodeInfo"}}, "text": "Launch the executable uber-JAR file: java -jar openscoring-server-executable-${version}.jar. By default, the REST web service is started at http://localhost:8080/openscoring. The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes. Please specify `--help` for more information. Advanced configuration can be done by copying the sample Typesafe's Config configuration file `openscoring-.server/application.conf.sample` to a new file `application.conf`, and customize its content to current needs. Use the `config.file` system property to impose changes on the JVM. Moreover, the local configuration overrides the default configuration that is defined in the reference REST web service configuration file `openscoring-.service/src/main/reference.conf`. The deployment and undeployment of models can be automated by launching the `org.openscoring.client.DirectoryDeployer` Java application class from the uber-JAR file, which listens for PMML file addition and removal events on the specified directory (\"PMML directory watchdog\").", "start_char_idx": 825, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "266070a7-8688-4e82-8382-6902cc9617c5": {"__data__": {"id_": "266070a7-8688-4e82-8382-6902cc9617c5", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b65166f75c90f6d988ca8d0cef6ea0c4f10f0dbd92c60d05e902ea51aeb991e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c114f88a-5e71-4d16-aa73-f6248f66031d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cd619d741e3bacbde0919eb4a9c373ea93b052dad890aaa4f483917385c47bd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90d22f7e-09fc-47f4-9fd4-529445041d19", "node_type": "1", "metadata": {}, "hash": "11f8beeb4ca523c48746f267efbd8b1cd5153a257e73727693e21ae4ed02f831", "class_name": "RelatedNodeInfo"}}, "text": "The main class `org.openscoring.server.Main` accepts a number of configuration options for URI customization and other purposes. Please specify `--help` for more information. Advanced configuration can be done by copying the sample Typesafe's Config configuration file `openscoring-.server/application.conf.sample` to a new file `application.conf`, and customize its content to current needs. Use the `config.file` system property to impose changes on the JVM. Moreover, the local configuration overrides the default configuration that is defined in the reference REST web service configuration file `openscoring-.service/src/main/reference.conf`. The deployment and undeployment of models can be automated by launching the `org.openscoring.client.DirectoryDeployer` Java application class from the uber-JAR file, which listens for PMML file addition and removal events on the specified directory (\"PMML directory watchdog\"). The Model REST API endpoints are available for managing models. Please refer to the README file for more information.", "start_char_idx": 1000, "end_char_idx": 2043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90d22f7e-09fc-47f4-9fd4-529445041d19": {"__data__": {"id_": "90d22f7e-09fc-47f4-9fd4-529445041d19", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\215.txt", "file_name": "215.txt", "file_type": "text/plain", "file_size": 797, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a6ec49f4-2721-4ba8-9946-4b52de895769", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\215.txt", "file_name": "215.txt", "file_type": "text/plain", "file_size": 797, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "54a2bebe4b52bafd32093c0f61e4da5ee0a4eeac392626ae04d40209aada6b10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "266070a7-8688-4e82-8382-6902cc9617c5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1ddddd9eb995340e455be1a6449a9ad21f683ba73934c1896d9baca84584da42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5d73d73-5606-4f11-b112-441c7f84422b", "node_type": "1", "metadata": {}, "hash": "905fca2a6c8611ec9c7203e48f929a71226cb085df532fb8bd6eed1183d1f2a7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: kserve\r\nlink: https://github.com/kserve/kserve\r\ndescription: KServe provides a Kubernetes Custom Resource Definition for serving machine learning (ML) models on arbitrary frameworks. It enables a simple, pluggable, and complete story for production ML serving, including prediction, pre-processing, post-processing, and explainability. KServe encapsulates the complexity of autoscaling, networking, health checking, and server configuration to bring cutting-edge serving features like GPU autoscaling, Scale to Zero, and Canary Rollouts to your ML deployments. KServe is being used across various organizations and is an important add-on component of Kubeflow. Learn more about KServe, its supported features, and participate in the KServe community through its website documentation.", "start_char_idx": 0, "end_char_idx": 795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5d73d73-5606-4f11-b112-441c7f84422b": {"__data__": {"id_": "b5d73d73-5606-4f11-b112-441c7f84422b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\216.txt", "file_name": "216.txt", "file_type": "text/plain", "file_size": 1011, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6dba962f-eda8-4374-a375-69ac4d51998d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\216.txt", "file_name": "216.txt", "file_type": "text/plain", "file_size": 1011, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eeb39b5524a2f87c10c0b7c0817b7a5c6b4f8d313b492164280e3a3e5337fb89", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90d22f7e-09fc-47f4-9fd4-529445041d19", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\215.txt", "file_name": "215.txt", "file_type": "text/plain", "file_size": 797, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b1db318aa31197eec2273b11d46db47e927b22ba272ba01ecbea9d88ff59270f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e233ad7-b95e-4c19-aaa6-a99adea87c30", "node_type": "1", "metadata": {}, "hash": "98525399e683edbb74a1b385eb0db3fb8d79131ade80c147e8fe53bd35dea359", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: multi-model-server\r\nlink: https://github.com/awslabs/multi-model-server\r\ndescription: The Multi-Model Server is a service that sets up HTTP endpoints to handle model inference requests. You can use the MMS Server CLI or the pre-configured Docker images to start the service. It is recommended to install and run MMS in a virtual environment using Virtualenv which creates virtual Python environments to provide isolation of dependencies and ease dependency management. MMS requires Python, pip, and Java 8 which can be installed on Ubuntu, CentOS, or macOS. To install MMS, MXNet must be installed first, followed by MMS itself. After installation, the MMS model server can quickly be started using a variety of CLI commands. MMS enables users to package all of their model artifacts into a single model archive, making it easier to share and deploy models. Other relevant documents, recommended production deployments, and external demos powered by MMS are featured, and contributions are welcome.", "start_char_idx": 0, "end_char_idx": 1009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e233ad7-b95e-4c19-aaa6-a99adea87c30": {"__data__": {"id_": "8e233ad7-b95e-4c19-aaa6-a99adea87c30", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\217.txt", "file_name": "217.txt", "file_type": "text/plain", "file_size": 1090, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c722044-a4cb-40a2-9f4c-6a9b44d69d94", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\217.txt", "file_name": "217.txt", "file_type": "text/plain", "file_size": 1090, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cfa8e0139b669dc91f815d6a8f40146b8e38e794c0169c4e59a688580b83bb35", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5d73d73-5606-4f11-b112-441c7f84422b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\216.txt", "file_name": "216.txt", "file_type": "text/plain", "file_size": 1011, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a498d4bf807b87ee6ef23e5a96b188348672e7567cf60f011c548cdd6e6cef00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "658b7a90-6d07-4f5b-93ca-cd718096f955", "node_type": "1", "metadata": {}, "hash": "436089018bac36de42a55cc2db6457cf3c02da923a542927bd77f04847d70e22", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ForestFlow\r\nlink: https://github.com/ForestFlow/ForestFlow\r\ndescription: ForestFlow is a scalable policy-based cloud-native machine learning model server that automates any workflow and provides various features such as hosting and managing packages, finding and fixing vulnerabilities, instant dev environments, writing better code with AI, and collaborating outside of code. ForestFlow looks to address the proliferation of model serving formats and standards for inference API specifications by adopting widely adopted open source frameworks, formats, and API specifications. ForestFlow also offers pluggable formats, allowing it to continue evolving as the industry matures and support for additional features is needed. Currently, ForestFlow supports models described via MLfLow Model format and a BASIC REST API for model deployment as well as providing two interfaces for inference, including a BASIC REST API and GraphPipe API specification. The ForestFlow team encourages feedback, ideas, and contributions to help develop the roadmap and implementation of ForestFlow.", "start_char_idx": 0, "end_char_idx": 1088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "658b7a90-6d07-4f5b-93ca-cd718096f955": {"__data__": {"id_": "658b7a90-6d07-4f5b-93ca-cd718096f955", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\218.txt", "file_name": "218.txt", "file_type": "text/plain", "file_size": 480, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14d39e40-bfdc-4c06-aadc-0e77c175a183", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\218.txt", "file_name": "218.txt", "file_type": "text/plain", "file_size": 480, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1ea71533118a8d14addcdd3f3a9bc352893d5fa885bf24bb250bbfaf393d6f64", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e233ad7-b95e-4c19-aaa6-a99adea87c30", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\217.txt", "file_name": "217.txt", "file_type": "text/plain", "file_size": 1090, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2abfbe218fd95dca226002855a3ebf5b74c02fb8b907c187cb0bb5c5e699c2a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb30e67e-bfda-4f39-a79f-fe614fb6dcc0", "node_type": "1", "metadata": {}, "hash": "86bb359d7c9d2c07d4ad051fea8971741616cf351fceac9bedde6fc70f7d8bc6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: jina\r\nlink: https://github.com/jina-ai/jina\r\ndescription: Jina is an MLOps framework to build multimodal AI microservice-based applications written in Python that can communicate via gRPC, HTTP and WebSocket protocols. It allows developers to build and serve services and pipelines while scaling and deploying them to production while removing the complexity, letting them focus on the logic/algorithmic part, saving valuable time and resources for engineering teams.", "start_char_idx": 0, "end_char_idx": 478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb30e67e-bfda-4f39-a79f-fe614fb6dcc0": {"__data__": {"id_": "bb30e67e-bfda-4f39-a79f-fe614fb6dcc0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\219.txt", "file_name": "219.txt", "file_type": "text/plain", "file_size": 883, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93f4ac72-646a-448b-8d9f-940d48b8a556", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\219.txt", "file_name": "219.txt", "file_type": "text/plain", "file_size": 883, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0bb4a9a4db5a4f660890e32b6dada48ce83607e3cff14b5e2929441032362c53", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "658b7a90-6d07-4f5b-93ca-cd718096f955", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\218.txt", "file_name": "218.txt", "file_type": "text/plain", "file_size": 480, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8184bc3ba43968b8a9d5fe857324fcc8f70f51f409fc5536b5bf865068a4a22a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cd70036-1699-4a65-bc8f-f90410dafb88", "node_type": "1", "metadata": {}, "hash": "a1284df775681ba30d27f993caab6742e4d3c0532cac46f6f0b301b0adeeb144", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: deepdetect\r\nlink: https://github.com/beniz/deepdetect\r\ndescription: \"DeepDetect (https://www.deepdetect.com/) is a machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications. It has support for both training and inference, with automatic conversion to embedded platforms with TensorRT (NVidia GPU) and NCNN (ARM CPU). It implements support for supervised and unsupervised deep learning of images, text, time series and other data, with focus on simplicity and ease of use, test and connection into existing applications. It supports classification, object detection, segmentation, regression, autoencoders, and relies on external machine learning libraries through a very generic and flexible API. At the moment it has support for machine learning functionalities per library.\"", "start_char_idx": 0, "end_char_idx": 881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cd70036-1699-4a65-bc8f-f90410dafb88": {"__data__": {"id_": "0cd70036-1699-4a65-bc8f-f90410dafb88", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\22.txt", "file_name": "22.txt", "file_type": "text/plain", "file_size": 546, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf1b32f9-cfc3-470e-8f36-a1fee2153929", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\22.txt", "file_name": "22.txt", "file_type": "text/plain", "file_size": 546, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e680e760d0d751f218a7c31e62b8ae12284fbff61a90f71be7a192b7e75011ee", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb30e67e-bfda-4f39-a79f-fe614fb6dcc0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\219.txt", "file_name": "219.txt", "file_type": "text/plain", "file_size": 883, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "32dbe14032ab3f63b4ce53343d610820e3dbbd11f3311cb7d29c8a9c49111602", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a830415-566a-418c-94d4-887cb4f8992d", "node_type": "1", "metadata": {}, "hash": "c004392bc5a61f6cb1d7d8b411dd3349951946313dd3c03e80a93017e73cc604", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: snakemake\r\nlink: https://github.com/snakemake/snakemake\r\ndescription: The Snakemake workflow management system is a popular tool to create reproducible and scalable data analyses. Workflows are described via a human-readable, Python-based language and can be seamlessly scaled to various environments without modifying the workflow definition. Additionally, Snakemake workflows can specify required software that will be automatically deployed to any execution environment. The homepage for Snakemake is https://snakemake.github.io/.", "start_char_idx": 0, "end_char_idx": 544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a830415-566a-418c-94d4-887cb4f8992d": {"__data__": {"id_": "4a830415-566a-418c-94d4-887cb4f8992d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\220.txt", "file_name": "220.txt", "file_type": "text/plain", "file_size": 431, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c9affb97-0c29-4021-baf2-863825851d58", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\220.txt", "file_name": "220.txt", "file_type": "text/plain", "file_size": 431, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4fed7b509f23711dd295a78d7c967837cad9bcc8113980246a3c3204120c5b7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cd70036-1699-4a65-bc8f-f90410dafb88", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\22.txt", "file_name": "22.txt", "file_type": "text/plain", "file_size": 546, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1f7b092c25fa4717757720ca02423a1af0d14aff8aeba25e0a7f4bd078fd6772", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "607e6268-383e-4a35-aacc-2189bc3559a5", "node_type": "1", "metadata": {}, "hash": "233054b8465cbfc08d671c4b8b783e1ca338f025aca98efd5edc062e154385cd", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: evidently\r\nlink: https://github.com/evidentlyai/evidently\r\ndescription: Introducing the library Evidently, except for the installation instructions, the following sentence can be extracted: \"Evidently is an open-source Python library for data scientists and ML engineers. It helps evaluate, test, and monitor the performance of ML models from validation to production. It works with tabular, text data and embeddings.\"", "start_char_idx": 0, "end_char_idx": 429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "607e6268-383e-4a35-aacc-2189bc3559a5": {"__data__": {"id_": "607e6268-383e-4a35-aacc-2189bc3559a5", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\221.txt", "file_name": "221.txt", "file_type": "text/plain", "file_size": 870, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e493a907-1754-4919-bb87-2f7bf5cde5f9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\221.txt", "file_name": "221.txt", "file_type": "text/plain", "file_size": 870, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3fb1b60b430ee5a599fd41024b7d75a2739995a88a8bb46dfe055673dfaec5f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a830415-566a-418c-94d4-887cb4f8992d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\220.txt", "file_name": "220.txt", "file_type": "text/plain", "file_size": 431, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "edf3dfb13f87b57a5168b4987515bd0cff806ad46d50e356ae35398e4be7979e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afa355a0-2718-4c88-914b-0bbb313b3eb2", "node_type": "1", "metadata": {}, "hash": "24f3e4f23827c504dd94c5b71dca0e94e0d00329f66acda1e9396d0c8e0915a8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cortex\r\nlink: https://github.com/cortexlabs/cortex\r\ndescription: The cortex is a production infrastructure designed for machine learning at scale, featuring serverless workloads, automated cluster management, and CI/CD and observability integrations. It allows for deploying, managing, and scaling machine learning models in production by elastically scaling clusters with CPU and GPU instances, running workloads on spot instances with automated on-demand backups, and creating multiple clusters with different configurations. Cortex also sends metrics to any monitoring tool or uses pre-built Grafana dashboards and streams logs to any log management tool or uses the pre-built CloudWatch integration. Moreover, it runs on top of EKS to scale workloads reliably and cost-effectively and deploys clusters into a VPC on the AWS account to keep data private.", "start_char_idx": 0, "end_char_idx": 868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afa355a0-2718-4c88-914b-0bbb313b3eb2": {"__data__": {"id_": "afa355a0-2718-4c88-914b-0bbb313b3eb2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\222.txt", "file_name": "222.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4b63884-e2d3-4a10-80c7-7874da26bddf", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\222.txt", "file_name": "222.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "96d3ac6df0259c0cf68aeef9b90798648455e23c5ac66f74b6a5d79fa38d17d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "607e6268-383e-4a35-aacc-2189bc3559a5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\221.txt", "file_name": "221.txt", "file_type": "text/plain", "file_size": 870, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "207577b96f660d266941a885554cb2bc352f6774ebb8dcc4efcf492c427a0829", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "990324bc-d980-4d29-9d83-53322d829963", "node_type": "1", "metadata": {}, "hash": "ebebf623e60225c3b265e34c75ffb293f6bbcde8d184179f7891c7204675decc", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: BentoML\r\nlink: https://github.com/bentoml/BentoML\r\ndescription:", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "990324bc-d980-4d29-9d83-53322d829963": {"__data__": {"id_": "990324bc-d980-4d29-9d83-53322d829963", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\223.txt", "file_name": "223.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99a871ed-8ab0-4918-9dc5-c75a6957d853", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\223.txt", "file_name": "223.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "801bf6ca41d6f1a09c5f9cd175ad61a549c4ca00f92000a1770a6283f5191dd6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afa355a0-2718-4c88-914b-0bbb313b3eb2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\222.txt", "file_name": "222.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9d0229405a2747fe7a8f95586f4c0edb4d4d8ab000126dcaf7289d4897c4d4d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d31e390b-7222-4b4b-9072-97ac49471559", "node_type": "1", "metadata": {}, "hash": "097df8ef76662261cc66f1507d0cdfa3d8ad6304b881ef631d8001c959aa879f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: zenml\r\nlink: https://github.com/zenml-io/zenml\r\ndescription: ZenML is an extensible, open-source MLOps framework for creating portable, production-ready machine learning pipelines. By decoupling infrastructure from code, ZenML enables developers across your organization to collaborate more effectively as they develop to production. ZenML provides a user-friendly syntax designed for ML workflows, compatible with any cloud or tool. It enables centralized pipeline management, enabling developers to write code once and effortlessly deploy it to various infrastructures.", "start_char_idx": 0, "end_char_idx": 582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d31e390b-7222-4b4b-9072-97ac49471559": {"__data__": {"id_": "d31e390b-7222-4b4b-9072-97ac49471559", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\224.txt", "file_name": "224.txt", "file_type": "text/plain", "file_size": 417, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83412d26-6498-4f0d-8b4c-a5fedef10173", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\224.txt", "file_name": "224.txt", "file_type": "text/plain", "file_size": 417, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "344ee2134a3fea19fcc0e2ab3df6a6ef320aa4a7bde2dbe1ca149442893a8f94", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "990324bc-d980-4d29-9d83-53322d829963", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\223.txt", "file_name": "223.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a21c609da11ee03175703e7cc0e8b831d9a1ca3c7a4e6606da6e20e8bd44b852", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c76c1843-995f-4b4d-9bfd-2c0bb26a1e37", "node_type": "1", "metadata": {}, "hash": "a371fe81ae632259a3d47d7957e08b89892e43c78939a8d6d55f61b679b32658", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: TonY\r\nlink: https://github.com/tony-framework/TonY\r\ndescription: TonY is a framework to natively run deep learning jobs on Apache Hadoop. It currently supports TensorFlow, PyTorch, MXNet, and Horovod. TonY enables running either single node or distributed training as a Hadoop application. This native connector, together with other TonY features, aims to run machine learning jobs reliably and flexibly.", "start_char_idx": 0, "end_char_idx": 415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c76c1843-995f-4b4d-9bfd-2c0bb26a1e37": {"__data__": {"id_": "c76c1843-995f-4b4d-9bfd-2c0bb26a1e37", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\225.txt", "file_name": "225.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18b97950-5b23-48c2-abaf-b032f82d760c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\225.txt", "file_name": "225.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "15e4793128cf176d1f52ec545002ba19af2c335a1bdca0ac261c81d73b5a697c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d31e390b-7222-4b4b-9072-97ac49471559", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\224.txt", "file_name": "224.txt", "file_type": "text/plain", "file_size": 417, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b5f10759fcdaf1e00987eded77e08577be592e31b7012d317e38bcc399d00f26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b6f58dd-24dc-49b3-b5b5-2dd196dcab86", "node_type": "1", "metadata": {}, "hash": "f6f45d34a2f4b4734782ced5a3a797d869c401c3dc4c0ba79c19b37c0dece652", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: skaffold\r\nlink: https://github.com/GoogleContainerTools/skaffold\r\ndescription: Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. You can iterate on your application source code locally then deploy to local or remote Kubernetes clusters. Skaffold handles the workflow for building, pushing and deploying your application. It also provides building blocks and describe customizations for a CI/CD pipeline. For a managed experience of Skaffold, you can install the Google `Cloud Code` extensions.", "start_char_idx": 0, "end_char_idx": 555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b6f58dd-24dc-49b3-b5b5-2dd196dcab86": {"__data__": {"id_": "9b6f58dd-24dc-49b3-b5b5-2dd196dcab86", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\226.txt", "file_name": "226.txt", "file_type": "text/plain", "file_size": 725, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7186d9f6-cba7-47a8-a2e8-57cb21e56e79", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\226.txt", "file_name": "226.txt", "file_type": "text/plain", "file_size": 725, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a9467ed5cdc9d0a8b6190f71c1d859d3b96c33d507e364552914fa3154c74556", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c76c1843-995f-4b4d-9bfd-2c0bb26a1e37", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\225.txt", "file_name": "225.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "09a29828e430b3f4cb36e140bfe3203041288c2a337480cdc7ee2abf48e89b56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "818fabd7-f5db-42ec-8561-43baa12fd2d8", "node_type": "1", "metadata": {}, "hash": "1490a24bc9d73647183daaaabc52faabc510471a6331fa2fcef74d4f7c9dbd4d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: tfx\r\nlink: https://github.com/tensorflow/tfx\r\ndescription: TensorFlow Extended (TFX) is a Google-production-scale machine learning platform based on TensorFlow. It provides a configuration framework to express ML pipelines consisting of TFX components. TFX pipelines can be orchestrated using Apache Airflow and Kubeflow Pipelines. Both the components themselves as well as the integrations with orchestration systems can be extended. TFX components interact with a ML Metadata backend that keeps a record of component runs, input and output artifacts, and runtime configuration. This metadata backend enables advanced functionality like experiment tracking or warmstarting/resuming ML models from previous runs.", "start_char_idx": 0, "end_char_idx": 723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "818fabd7-f5db-42ec-8561-43baa12fd2d8": {"__data__": {"id_": "818fabd7-f5db-42ec-8561-43baa12fd2d8", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\227.txt", "file_name": "227.txt", "file_type": "text/plain", "file_size": 625, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "001e396d-cd75-431f-a765-e29eb8ed0bf1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\227.txt", "file_name": "227.txt", "file_type": "text/plain", "file_size": 625, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9212c033f68bb601f72a92794c50bc2ba4f5443779208e565001f700c6ab2e97", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b6f58dd-24dc-49b3-b5b5-2dd196dcab86", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\226.txt", "file_name": "226.txt", "file_type": "text/plain", "file_size": 725, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dafca829501a401090a9576ce3f0fe2964775a9fdeff6109dc257846a07c9cdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5bee6a0-8fa4-46b4-b8f4-4982961e11b2", "node_type": "1", "metadata": {}, "hash": "b1e211256a72a641fa706f2fae4293e991f77e8a62225d9822ef966df83a2833", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: redisml\r\nlink: https://github.com/RedisLabsModules/redisml\r\ndescription: RedisML is a Redis module that implements several machine learning models as Redis data types. The stored models are fully operational and support the prediction/evaluation process. RedisML is a turnkey solution for using trained models in a production environment. Load ML models from any platform, immediately ready to serve. RedisML is planned to be replaced by RedisAI, adding support for deep learning. RedisAI will provide a unified interface for tensor, scientific, deep learning, and other machine learning operations within Redis.", "start_char_idx": 0, "end_char_idx": 623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5bee6a0-8fa4-46b4-b8f4-4982961e11b2": {"__data__": {"id_": "d5bee6a0-8fa4-46b4-b8f4-4982961e11b2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\228.txt", "file_name": "228.txt", "file_type": "text/plain", "file_size": 893, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4cb0c00b-c699-4718-8736-9d0c2c471abc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\228.txt", "file_name": "228.txt", "file_type": "text/plain", "file_size": 893, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b908d7cd46b1ebfca6d49fcdd70db6992f51dc70509d5c26bb1c7af135cbf702", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "818fabd7-f5db-42ec-8561-43baa12fd2d8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\227.txt", "file_name": "227.txt", "file_type": "text/plain", "file_size": 625, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d8d46535395f7568419bca5c6e3db81cdfb163b0147d65cbe2d6d5eb5faf52e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca6c2b87-7a3d-4b54-8935-7421283a887c", "node_type": "1", "metadata": {}, "hash": "12aa84b31e6f9c32f881853a39126db7e881792a4af10b9f3cb3aae917c80816", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pycaret\r\nlink: https://github.com/pycaret/pycaret\r\ndescription: PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that speeds up the experiment cycle exponentially and makes you more productive. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, Optuna, Hyperopt, Ray, and few more. The design and simplicity of PyCaret are inspired by the emerging role of citizen data scientists, a term first used by Gartner. Citizen Data Scientists are power users who can perform both simple and moderately sophisticated analytical tasks that would previously have required more technical expertise. PyCaret was inspired by the caret library in R programming language.", "start_char_idx": 0, "end_char_idx": 891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca6c2b87-7a3d-4b54-8935-7421283a887c": {"__data__": {"id_": "ca6c2b87-7a3d-4b54-8935-7421283a887c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8474b7ab-88ce-423b-972e-e5a20c38d4a6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "59050bb8b9d6a3e536afca7b68d3f12cfd9fa8a6f4869dcb3bcb117e7b2d887b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5bee6a0-8fa4-46b4-b8f4-4982961e11b2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\228.txt", "file_name": "228.txt", "file_type": "text/plain", "file_size": 893, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d3323d91e6319a2fb88d6a8cc4afd01e88f2f77c577f91a6c90f003959187239", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1513ae2b-b41a-43e5-9b4c-538bcd249223", "node_type": "1", "metadata": {}, "hash": "6c3b139144593b9f8e1a33a0b754dec300063742411c3297ff7835e46963db48", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pai\r\nlink: https://github.com/microsoft/pai\r\ndescription: Open Platform for AI (OpenPAI) is a full-stack solution designed to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant development environments, write better code with AI, collaborate outside of code, fund open source developers, and more. OpenPAI incorporates a mature design that has a proven track record in Microsoft's large-scale production environment, and it supports popular AI frameworks and heterogeneous hardware. It is the most complete solution for deep learning, providing a complete training pipeline at one cluster, and it is optimized for distributed computing with docker technology. OpenPAI also provides end-to-end manuals for both cluster users and administrators. The admin manual covers topics such as installation and upgrade, basic cluster management, users and groups management, alerts management, and customization, while the user manual includes job submission and monitoring, data management, collaboration and sharing. There is also a VS Code extension and a command line tool available.", "start_char_idx": 0, "end_char_idx": 1130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1513ae2b-b41a-43e5-9b4c-538bcd249223": {"__data__": {"id_": "1513ae2b-b41a-43e5-9b4c-538bcd249223", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8474b7ab-88ce-423b-972e-e5a20c38d4a6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "59050bb8b9d6a3e536afca7b68d3f12cfd9fa8a6f4869dcb3bcb117e7b2d887b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca6c2b87-7a3d-4b54-8935-7421283a887c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8ec066e08f94e0d000431de039651c0a9574bf8a4adf826b68c50172ddb34737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc490d04-b15a-4c9e-b4fc-e711be129d27", "node_type": "1", "metadata": {}, "hash": "56b5abf5d249714c4c675e323c1c4563ff1fee45d68d4d6908ca1bbb718059a1", "class_name": "RelatedNodeInfo"}}, "text": "OpenPAI incorporates a mature design that has a proven track record in Microsoft's large-scale production environment, and it supports popular AI frameworks and heterogeneous hardware. It is the most complete solution for deep learning, providing a complete training pipeline at one cluster, and it is optimized for distributed computing with docker technology. OpenPAI also provides end-to-end manuals for both cluster users and administrators. The admin manual covers topics such as installation and upgrade, basic cluster management, users and groups management, alerts management, and customization, while the user manual includes job submission and monitoring, data management, collaboration and sharing. There is also a VS Code extension and a command line tool available. OpenPAI is completely open under the MIT license, and contributions from academia and industry are highly welcome.", "start_char_idx": 352, "end_char_idx": 1245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc490d04-b15a-4c9e-b4fc-e711be129d27": {"__data__": {"id_": "bc490d04-b15a-4c9e-b4fc-e711be129d27", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6244de17-417c-453b-be2b-a68eb683fc36", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5feea03aa01c9ab6b26ba1fdca1f73b179507e9bdd109d642655a6a6625011e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1513ae2b-b41a-43e5-9b4c-538bcd249223", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "089d06d9150607264ee09bd8d0891b731e53d5e6164e74bdfeb6898a3946f2fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6857a3f-7f01-40da-8000-50f17e9b0a73", "node_type": "1", "metadata": {}, "hash": "866c69365eb2aba5742bb7f415aa2105dccbbf950dab9fd8f96041e06fd02076", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: airflow\r\nlink: https://github.com/apache/airflow\r\ndescription: Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative. Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include Luigi, Oozie and Azkaban.", "start_char_idx": 0, "end_char_idx": 935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6857a3f-7f01-40da-8000-50f17e9b0a73": {"__data__": {"id_": "a6857a3f-7f01-40da-8000-50f17e9b0a73", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6244de17-417c-453b-be2b-a68eb683fc36", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5feea03aa01c9ab6b26ba1fdca1f73b179507e9bdd109d642655a6a6625011e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc490d04-b15a-4c9e-b4fc-e711be129d27", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "69e81092100858d707e13bdfdff9f348b0a34a390386313304d1554ec4a14f45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b307fe2e-9636-4211-87c7-f0da18f084cf", "node_type": "1", "metadata": {}, "hash": "6b34cb18e3fdf72eefe12c5eace8fc15edb237045b296e5fe37c2575b2810a06", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: airflow\r\nlink: https://github.com/apache/airflow\r\ndescription: Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative. Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include Luigi, Oozie and Azkaban. Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's XCom feature).", "start_char_idx": 0, "end_char_idx": 1278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b307fe2e-9636-4211-87c7-f0da18f084cf": {"__data__": {"id_": "b307fe2e-9636-4211-87c7-f0da18f084cf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6244de17-417c-453b-be2b-a68eb683fc36", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5feea03aa01c9ab6b26ba1fdca1f73b179507e9bdd109d642655a6a6625011e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6857a3f-7f01-40da-8000-50f17e9b0a73", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eb37da82ce47aff29c0236e0248cd7864acb9c430ea5e6140f388d5cf3dc6985", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a35a7f16-c612-4f17-aa65-3ad58eff5cd9", "node_type": "1", "metadata": {}, "hash": "2be00faddf32b12e77ce2117abcbda8ee211d7e3072301d6aba6c523b770c31d", "class_name": "RelatedNodeInfo"}}, "text": "The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include Luigi, Oozie and Azkaban. Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's XCom feature). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.", "start_char_idx": 369, "end_char_idx": 1404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a35a7f16-c612-4f17-aa65-3ad58eff5cd9": {"__data__": {"id_": "a35a7f16-c612-4f17-aa65-3ad58eff5cd9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6244de17-417c-453b-be2b-a68eb683fc36", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5feea03aa01c9ab6b26ba1fdca1f73b179507e9bdd109d642655a6a6625011e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b307fe2e-9636-4211-87c7-f0da18f084cf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3aa8f79885d4e6b4ba096786ec4ad8b7a168eb9acd610c8597b76686c3f07e53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "461436f4-1e93-4540-bef0-d2087c6b4523", "node_type": "1", "metadata": {}, "hash": "e56e352941cca58b54470c3debf27386f4e9cbbed20890e3082bf8a4a8f99d25", "class_name": "RelatedNodeInfo"}}, "text": "Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include Luigi, Oozie and Azkaban. Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's XCom feature). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work. Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.", "start_char_idx": 478, "end_char_idx": 1526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "461436f4-1e93-4540-bef0-d2087c6b4523": {"__data__": {"id_": "461436f4-1e93-4540-bef0-d2087c6b4523", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\230.txt", "file_name": "230.txt", "file_type": "text/plain", "file_size": 642, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4c60d868-559f-42ab-bafb-61d5c7a734b5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\230.txt", "file_name": "230.txt", "file_type": "text/plain", "file_size": 642, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2f6aa2c10b7cd7f871f823c926f6f50c9ad9d2b276c7364dc5f4f04d63480a3e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a35a7f16-c612-4f17-aa65-3ad58eff5cd9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7fff17429a69acb387dd4abda13ba04788c9ea2e40d02cdca917706683836b90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c55ee3d-f8d2-4b7e-b9d6-4b15bb24ae50", "node_type": "1", "metadata": {}, "hash": "172689a7fbc220b466d6cb300073eff6d94fee01156dfd6d0ad1d10206c1705b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: onepanel\r\nlink: https://github.com/onepanelio/onepanel\r\ndescription: Onepanel is an end-to-end computer vision platform that allows users to label, build, train, tune, deploy and automate in a unified platform that runs on any cloud and on-premises. It seamlessly integrates several open source projects such as Argo, Couler, CVAT, JupyterLab, and NNI. Onepanel is licensed under Apache 2.0 and users are encouraged to contribute back to the communities that support its operation. See the quick start guide to get started, and to submit a feature request, bug report, or documentation issue, open a GitHub pull request or issue.", "start_char_idx": 0, "end_char_idx": 640, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c55ee3d-f8d2-4b7e-b9d6-4b15bb24ae50": {"__data__": {"id_": "3c55ee3d-f8d2-4b7e-b9d6-4b15bb24ae50", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\231.txt", "file_name": "231.txt", "file_type": "text/plain", "file_size": 1051, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d76129e3-edb6-4b27-b369-742c7e2e3ba5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\231.txt", "file_name": "231.txt", "file_type": "text/plain", "file_size": 1051, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fb8710d0eb597fe154229f5598780407e6e09dc8b92e3fc2078b8e3ad8e67777", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "461436f4-1e93-4540-bef0-d2087c6b4523", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\230.txt", "file_name": "230.txt", "file_type": "text/plain", "file_size": 642, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3919e8f66b3f13e9199fbfc4443de1e31157b28e746044842fa572ac9e9099be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bce6c32e-3ab8-4192-8cab-fdc561c1fc9b", "node_type": "1", "metadata": {}, "hash": "1721269f606bbfb28bd5c8de9096d476013bf715177fbba3dadb39777af42682", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mleap\r\nlink: https://github.com/combust/mleap\r\ndescription: MLeap allows data scientists and engineers to deploy machine learning pipelines from Spark and Scikit-learn to a portable format and execution engine. Using the MLeap execution engine and serialization format, we provide a performant, portable, and easy-to-integrate production library for machine learning data pipelines and algorithms. For portability, MLeap is built on the JVM and only uses serialization formats that are widely adopted. The software also provides a high level of integration with existing technologies. MLeap is built against Scala 2.12 and Java 11 and allows users to choose the right version of the `mleap-spark` module to export their pipeline. Additionally, MLeap provides documentation on link with Maven or SBT and Spark integration. There is support for PySpark integration, create and export a Spark pipeline, create and export a Scikit-Learn pipeline, and load and transform using MLeap. For more documentation, please see the MLeap documentation.", "start_char_idx": 0, "end_char_idx": 1049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bce6c32e-3ab8-4192-8cab-fdc561c1fc9b": {"__data__": {"id_": "bce6c32e-3ab8-4192-8cab-fdc561c1fc9b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\232.txt", "file_name": "232.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "744021de-da76-4255-9386-8184cbc246fb", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\232.txt", "file_name": "232.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b4b2aaa2658d6fee479f6491b90e1888e32975f23bf54154e5b82ba88d41baa4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c55ee3d-f8d2-4b7e-b9d6-4b15bb24ae50", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\231.txt", "file_name": "231.txt", "file_type": "text/plain", "file_size": 1051, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b967992c8eb13f223d59ce7095fcd3c0eaad1be8365dbffd309e61498db7efcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddace1f7-1087-4fa6-bf67-ceff6c13e973", "node_type": "1", "metadata": {}, "hash": "3feca1de1fb73314a6a1df8e83813726e60dbf38cb92cbe16bc36a70810296b8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: TensorRT\r\nlink: https://github.com/NVIDIA/TensorRT\r\ndescription: TensorRT is an open source software component of NVIDIA that includes sources for TensorRT plugins and parsers (Caffe and ONNX), as well as sample applications demonstrating usage and capabilities of the TensorRT platform. To build the TensorRT-OSS components, you will first need to download the TensorRT GA build (optional if using TensorRT container) and generate the TensorRT-OSS build container, which can be configured for building TensorRT OSS out-of-the-box. After setting up the build environment, you can generate Makefiles or VS project (Windows) and build.", "start_char_idx": 0, "end_char_idx": 644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddace1f7-1087-4fa6-bf67-ceff6c13e973": {"__data__": {"id_": "ddace1f7-1087-4fa6-bf67-ceff6c13e973", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\233.txt", "file_name": "233.txt", "file_type": "text/plain", "file_size": 421, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4a5584f-1c74-46d1-b56c-82e30d75c5cf", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\233.txt", "file_name": "233.txt", "file_type": "text/plain", "file_size": 421, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d470648b72039f4f37557fc15d1a7bf8deab89821c24411616a15684acf689d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bce6c32e-3ab8-4192-8cab-fdc561c1fc9b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\232.txt", "file_name": "232.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ff114a9611c6117b37c40d40308a517237a32d2d5dd67a2b729a4d0d8ad39584", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a264cfe-cd3e-40ff-a268-063bbe0160d7", "node_type": "1", "metadata": {}, "hash": "3b7ba91ee252cc09510f4f6b22d71cde7a85bf29e96faef4e6477fb8149d4c80", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: kubeflow\r\nlink: https://github.com/kubeflow/kubeflow\r\ndescription: Kubeflow is a cloud-native platform for machine learning operations, used for pipelines, training, and deployment. The official docs for Kubeflow can be found at kubeflow.org and the Kubeflow community is organized into working groups that focus on specific pieces of the ML platform. Please refer to the Community page for more information.", "start_char_idx": 0, "end_char_idx": 419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a264cfe-cd3e-40ff-a268-063bbe0160d7": {"__data__": {"id_": "5a264cfe-cd3e-40ff-a268-063bbe0160d7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\234.txt", "file_name": "234.txt", "file_type": "text/plain", "file_size": 607, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab6c6f17-4ab1-4329-b665-1ee1070e2ad2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\234.txt", "file_name": "234.txt", "file_type": "text/plain", "file_size": 607, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "530e38f753453fb7a9f18676d1cda7443299041f8bedf670d71b27eacf534c5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddace1f7-1087-4fa6-bf67-ceff6c13e973", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\233.txt", "file_name": "233.txt", "file_type": "text/plain", "file_size": 421, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6016cc3e0ff7476934b74bb03924edfe4337fa03dd297bb892e62d5be9a12aaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e77ed13-c024-49e9-8e38-0e8903b819e2", "node_type": "1", "metadata": {}, "hash": "b273d5b8022ec576814c93cc5e4ea9541f5ee8344cc72e4ad634828311620da2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cml\r\nlink: https://github.com/iterative/cml\r\ndescription: Continuous Machine Learning (CML) is an open-source CLI tool for implementing continuous integration & delivery (CI/CD) with a focus on MLOps. Use it to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, and write better code with AI. CML provides a number of functions to help package the outputs of ML workflows into a CML report. CML can help train and evaluate models \u2014 and then generate a visual report with results and metrics \u2014 automatically on every pull request.", "start_char_idx": 0, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e77ed13-c024-49e9-8e38-0e8903b819e2": {"__data__": {"id_": "3e77ed13-c024-49e9-8e38-0e8903b819e2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\235.txt", "file_name": "235.txt", "file_type": "text/plain", "file_size": 483, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c186466-93be-4e75-bdb4-e9c3683f8678", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\235.txt", "file_name": "235.txt", "file_type": "text/plain", "file_size": 483, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2b2fe8fe01cbc5b8b99bb438f1c6e66717ac75b0f859b50d916a58ef3cf3327d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a264cfe-cd3e-40ff-a268-063bbe0160d7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\234.txt", "file_name": "234.txt", "file_type": "text/plain", "file_size": 607, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2f9f351fa6a7715f19ffdd993bab109f6b93487aba04b90fe9d99890cb707a26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07cd03f5-11f0-493e-83bc-dc6dd2c98688", "node_type": "1", "metadata": {}, "hash": "f1eb330acef469c487da31da33533c03640281713d5d73f68dc28bf2edee5172", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: flyte\r\nlink: https://github.com/flyteorg/flyte\r\ndescription: Flyte is an open-source orchestrator that facilitates building production-grade data and ML pipelines. It is built for scalability and reproducibility, leveraging Kubernetes as its underlying platform. With Flyte, user teams can construct pipelines using the Python SDK, and seamlessly deploy them on both cloud and on-premises environments, enabling distributed processing and efficient resource utilization.", "start_char_idx": 0, "end_char_idx": 481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07cd03f5-11f0-493e-83bc-dc6dd2c98688": {"__data__": {"id_": "07cd03f5-11f0-493e-83bc-dc6dd2c98688", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\236.txt", "file_name": "236.txt", "file_type": "text/plain", "file_size": 558, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "067a8088-66ce-4ba6-aeee-0ae4e4bc2f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\236.txt", "file_name": "236.txt", "file_type": "text/plain", "file_size": 558, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a2a121bd9ed95991a7940c6ccf306e7ec5596bbbe1ec78d8a9a37b222e20159d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e77ed13-c024-49e9-8e38-0e8903b819e2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\235.txt", "file_name": "235.txt", "file_type": "text/plain", "file_size": 483, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eedb3d026469e156f16812c1bef87163e61b96678b2322851180baf988344e16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e76b53dc-9a2e-449a-be82-3c86f3f6beec", "node_type": "1", "metadata": {}, "hash": "012b5d4366b5449027326accc68ebd8ccaf21f390c93a3bfe850cc54e2d558c7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: determined\r\nlink: https://github.com/determined-ai/determined\r\ndescription: Determined is an open-source deep learning training platform that makes building models fast and easy. Determined integrates these features into an easy-to-use, high-performance deep learning environment \u2014 which means you can spend your time building models instead of managing infrastructure. To use Determined, you can continue using popular DL frameworks such as TensorFlow and PyTorch; you just need to update your model code to integrate with the Determined API.", "start_char_idx": 0, "end_char_idx": 554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e76b53dc-9a2e-449a-be82-3c86f3f6beec": {"__data__": {"id_": "e76b53dc-9a2e-449a-be82-3c86f3f6beec", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1379036-f5c2-4ce2-9dc4-1f07d99a8846", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7e631d3fd5ee0b670789bad54c50a673aabe68700039516dc44226284f791c35", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07cd03f5-11f0-493e-83bc-dc6dd2c98688", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\236.txt", "file_name": "236.txt", "file_type": "text/plain", "file_size": 558, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "16e5e4683f395d00511c6a86cc3f22656f3601bda784a43b012e110b1e5c9eb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "491469b2-53cd-4588-9489-7611d17f9583", "node_type": "1", "metadata": {}, "hash": "714aa8637d24363c41fc73dede8769d013fa6bf2c4e18a9e1298aaf51b3bbe86", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: clearml\r\nlink: https://github.com/allegroai/clearml\r\ndescription: ClearML is a ML/DL development and production suite that can automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and manage experiments with its Experiment Manager. ClearML is supported by the team behind clear.ml, where they build deep learning pipelines and infrastructure for enterprise companies. ClearML contains five main modules, and adding only two lines of code can get you started using ClearML. ClearML is designed to require effortless integration so that teams can preserve their existing methods and practices. ClearML tracks and controls the process of training production-grade deep learning models by associating code version control, research projects, performance metrics, and model provenance. ClearML is licensed under Apache License, Version 2.0 and documentation and community support is available on the official website and on YouTube.", "start_char_idx": 0, "end_char_idx": 1082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "491469b2-53cd-4588-9489-7611d17f9583": {"__data__": {"id_": "491469b2-53cd-4588-9489-7611d17f9583", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1379036-f5c2-4ce2-9dc4-1f07d99a8846", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7e631d3fd5ee0b670789bad54c50a673aabe68700039516dc44226284f791c35", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e76b53dc-9a2e-449a-be82-3c86f3f6beec", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9aa05bd763adaf9b66434747aef5b2e0bd1c1ac5f097aa0e8c45a622e2286467", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1547094-a26e-4b38-9d84-1daab33ce0fa", "node_type": "1", "metadata": {}, "hash": "48d2591b7726d91054b5d4b19323e0c8742f1e325282554f32a0145427db7ad3", "class_name": "RelatedNodeInfo"}}, "text": "ClearML is supported by the team behind clear.ml, where they build deep learning pipelines and infrastructure for enterprise companies. ClearML contains five main modules, and adding only two lines of code can get you started using ClearML. ClearML is designed to require effortless integration so that teams can preserve their existing methods and practices. ClearML tracks and controls the process of training production-grade deep learning models by associating code version control, research projects, performance metrics, and model provenance. ClearML is licensed under Apache License, Version 2.0 and documentation and community support is available on the official website and on YouTube. Feature requests and bug reports can be reported in GitHub issues, and ClearML is committed to always being backwardly compatible.", "start_char_idx": 387, "end_char_idx": 1213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1547094-a26e-4b38-9d84-1daab33ce0fa": {"__data__": {"id_": "e1547094-a26e-4b38-9d84-1daab33ce0fa", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\238.txt", "file_name": "238.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de22733b-502e-4dbf-b070-9d41a6928b9b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\238.txt", "file_name": "238.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e37803aebe1e170dc26f038cc94fedbaff3a566cfa84deebd5886cfd82aea604", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "491469b2-53cd-4588-9489-7611d17f9583", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "da2d46bfe7b19630cad5c977131e09f5ab8f6386b5b998ff927e9eb2c42a1054", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acc5b919-8e92-47ef-a49f-e0302a659093", "node_type": "1", "metadata": {}, "hash": "1156b1d919e5a711014c8be38b6efd9b5ea88b04e14ef3469a134b4007bccfb6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: studio\r\nlink: https://github.com/studioml/studio\r\ndescription:", "start_char_idx": 0, "end_char_idx": 73, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acc5b919-8e92-47ef-a49f-e0302a659093": {"__data__": {"id_": "acc5b919-8e92-47ef-a49f-e0302a659093", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\239.txt", "file_name": "239.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a41f28b-e2a8-478e-99c1-4ce440c2a4b7", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\239.txt", "file_name": "239.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f15c05a328921b147b16e124a4bcb82ec73fc02a64c22e0ad73db4392b73b6d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1547094-a26e-4b38-9d84-1daab33ce0fa", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\238.txt", "file_name": "238.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3909c2bceea2e5a1066c760017c061877b7f9720df83eb4524f15c90ce860758", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7acdb0a-ec0f-40af-a3be-867695649b73", "node_type": "1", "metadata": {}, "hash": "68b11497018d9e380d17b0944dd23ec3d75be0f45a9fecac7225139d66b52969", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: steppy\r\nlink: https://github.com/minerva-ml/steppy\r\ndescription: Steppy is a library that addresses two common problems faced by data scientists when building machine learning pipelines. It introduces two simple abstractions: `Step` and `Transformer`, considered the minimal interface for building such pipelines. This library requires `python3.5` or above and is heavily tested on multiple machine learning challenges and educational projects. Steppy aims to become a practical tool for data scientists who can run their experiments easily and change their pipelines with just a few manipulations in the code. It also offers a collection of high-quality implementations of the top deep learning architectures under the same intuitive interface through its steppy-toolkit. Steppy is MIT-licensed and contributions are welcome as outlined in the CONTRIBUTING guidelines.", "start_char_idx": 0, "end_char_idx": 880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7acdb0a-ec0f-40af-a3be-867695649b73": {"__data__": {"id_": "b7acdb0a-ec0f-40af-a3be-867695649b73", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\24.txt", "file_name": "24.txt", "file_type": "text/plain", "file_size": 729, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b27a818-bd96-4a1f-ace1-345e5d3f058d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\24.txt", "file_name": "24.txt", "file_type": "text/plain", "file_size": 729, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4bfe9733ce613bfb3e09ad8d3182dbe3494a6dd41d875ddf0a101ab83c700292", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acc5b919-8e92-47ef-a49f-e0302a659093", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\239.txt", "file_name": "239.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "78f4e2c835209b495f20dd7cb0dccf3f1a02c1e528c4a165e81c974e6d000df0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0354f21f-a5f7-4184-90fc-3f5e413361b7", "node_type": "1", "metadata": {}, "hash": "deb336b5aa3b006cede0e55c3f872b1047d221ebf3793d0aceef5c1e2e85cced", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mybinder.org-deploy\r\nlink: https://github.com/jupyterhub/mybinder.org-deploy\r\ndescription: This repository is the deployment, configuration, and Site Reliability documentation files for the public mybinder.org service. Please note that this repository is participating in a study into sustainability of open source projects, with data being gathered about this repository for approximately the next 12 months. If you wish to deploy your own Binder instance, please do not use these files. Instead, review the BinderHub documentation and the `jupyterhub/binderhub` repo to set up your deployment. The Site Reliability Guide contains the collected wisdom of operating mybinder.org, as well as other useful information.", "start_char_idx": 0, "end_char_idx": 727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0354f21f-a5f7-4184-90fc-3f5e413361b7": {"__data__": {"id_": "0354f21f-a5f7-4184-90fc-3f5e413361b7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\240.txt", "file_name": "240.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b9e0307-70ee-4f2b-8019-fc806dd17dc3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\240.txt", "file_name": "240.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fe211141bd5274637ca71c59bfa9fac2eb8881ffe94ab66cb64dca8214bc2887", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7acdb0a-ec0f-40af-a3be-867695649b73", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\24.txt", "file_name": "24.txt", "file_type": "text/plain", "file_size": 729, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "057fec6ef4cc568f303ab3d4056d9ce193b57bc761344e1f34cdcb1ea1a8cfaa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c92ef262-2bb8-4865-a1e6-d557c81eaff8", "node_type": "1", "metadata": {}, "hash": "bf23da54fcab24be8c7d25409db60f3633e1285da4e322c916a812bc45832ac3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: quilt\r\nlink: https://github.com/quiltdata/quilt\r\ndescription:", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c92ef262-2bb8-4865-a1e6-d557c81eaff8": {"__data__": {"id_": "c92ef262-2bb8-4865-a1e6-d557c81eaff8", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\241.txt", "file_name": "241.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fbffcb8c-4da0-4423-8925-30d3dbdce156", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\241.txt", "file_name": "241.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a28ac099cc6181e98180003c1de3223d3f29f082eed9a07563f4444a57fb35b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0354f21f-a5f7-4184-90fc-3f5e413361b7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\240.txt", "file_name": "240.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5d15a9c7f7d1057790363c4fbde40fb308df6bd63eb465003d13445cd8869264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8ac4fe7-8910-4928-92d9-de2ffc12988b", "node_type": "1", "metadata": {}, "hash": "38511cfb078f352a8c4855914d5177be42f6dd115eafb5905b723ac9a2f116b4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: sacred\r\nlink: https://github.com/IDSIA/sacred\r\ndescription:", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8ac4fe7-8910-4928-92d9-de2ffc12988b": {"__data__": {"id_": "b8ac4fe7-8910-4928-92d9-de2ffc12988b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\242.txt", "file_name": "242.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b25e27f-fa12-4e0f-9e53-4a61010bdcf1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\242.txt", "file_name": "242.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ec435e8d8d7a3c0e21fc3a753ad309517916b5ae0955e28d3a3bd4cf3bc18bdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c92ef262-2bb8-4865-a1e6-d557c81eaff8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\241.txt", "file_name": "241.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c483f2c05e523a151d6bd27c3767b5bf2d3c64d64677110fe553fb28229b2fcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01b34120-fa60-436b-a079-1d24eac145ec", "node_type": "1", "metadata": {}, "hash": "a1b12083db6ac7096eeb8aa2cfdb431b9c685083777aa703262f2f01ad7b6b19", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: polyaxon\r\nlink: https://github.com/polyaxon/polyaxon\r\ndescription: Welcome to Polyaxon, a platform for building, training, and monitoring large-scale deep learning applications. Polyaxon deploys into any data center, cloud provider, or can be hosted and managed by Polyaxon, supporting all the major deep learning frameworks such as TensorFlow, MXNet, Caffe, Torch, etc. Polyaxon makes it faster, easier, and more efficient to develop deep learning applications by managing workloads with smart container and node management. It turns GPU servers into shared, self-service resources for your team or organization. Polyaxon supports and simplifies distributed jobs, as well as hyperparameters tuning, parallel executions, and DAGs and workflows. Check out our documentation to learn more about Polyaxon and its features.", "start_char_idx": 0, "end_char_idx": 830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01b34120-fa60-436b-a079-1d24eac145ec": {"__data__": {"id_": "01b34120-fa60-436b-a079-1d24eac145ec", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\243.txt", "file_name": "243.txt", "file_type": "text/plain", "file_size": 522, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "692f4a62-d411-4ef8-949c-b73941e653aa", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\243.txt", "file_name": "243.txt", "file_type": "text/plain", "file_size": 522, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6524083997467b0b08de8565e7609a8aa6bcd37aa6ba17690286617d0cb7ec12", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8ac4fe7-8910-4928-92d9-de2ffc12988b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\242.txt", "file_name": "242.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d6464577151c424999cb43153b4dc70fa2c91b35348046f459c1a0b128a7f731", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2d7a324-cac0-43d6-abf1-9841f613b5f2", "node_type": "1", "metadata": {}, "hash": "eb2bc4d875cae401ef841fc92bca653e678cdf055bf3dac968fd8d65d0b7b3b6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: predictionio\r\nlink: https://github.com/apache/predictionio\r\ndescription: Apache PredictionIO is an open source machine learning framework for developers, data scientists, and end users. It supports event collection, deployment of algorithms, evaluation, querying predictive results via REST APIs. It is based on scalable open source services like Hadoop, HBase (and other DBs), Elasticsearch, Spark and implements what is called a Lambda Architecture. To get started, check out http://predictionio.apache.org!", "start_char_idx": 0, "end_char_idx": 520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2d7a324-cac0-43d6-abf1-9841f613b5f2": {"__data__": {"id_": "d2d7a324-cac0-43d6-abf1-9841f613b5f2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "583508e3-d4be-4c1b-85f3-5d557b1d2220", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b9ad48f964dfa8ec4896d7afd8146cf2954be82c397d2accee060fd2848d2178", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01b34120-fa60-436b-a079-1d24eac145ec", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\243.txt", "file_name": "243.txt", "file_type": "text/plain", "file_size": 522, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "07395ca97c45577a4057f4b2aec081a19a7b39ec56f716938d5cb9f65d0854e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29bbb71b-7d11-42f4-92f3-3fc507391e2b", "node_type": "1", "metadata": {}, "hash": "6741954ab7707e3bfd4863716f333ddfb433f6b57daaa1d0bd6bd69e7f88a2e3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pachyderm\r\nlink: https://github.com/pachyderm/pachyderm\r\ndescription: Pachyderm is a cost-effective data engineering automation platform that enables complex pipeline creation with sophisticated data transformation capabilities across all types of data. It provides parallel processing of multi-stage, language-agnostic pipelines with data versioning and lineage tracking, delivering the ultimate CI/CD engine for data. Pachyderm can be deployed locally or on AWS/GCE/Azure in about 5 minutes, and its comprehensive documentation includes tutorials, example projects, and advanced features. If you want to see some examples and learn about core use cases for Pachyderm, check out their official documentation or refer to their contributing guide. Pachyderm automatically reports anonymized usage metrics, which can be disabled by setting the env variable `METRICS` to `false` in the pachd container.", "start_char_idx": 0, "end_char_idx": 910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29bbb71b-7d11-42f4-92f3-3fc507391e2b": {"__data__": {"id_": "29bbb71b-7d11-42f4-92f3-3fc507391e2b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "583508e3-d4be-4c1b-85f3-5d557b1d2220", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b9ad48f964dfa8ec4896d7afd8146cf2954be82c397d2accee060fd2848d2178", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2d7a324-cac0-43d6-abf1-9841f613b5f2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9846ad237d9cc6b116781e1d650c7e6ecd562528d194869919be0933168f056d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc3df753-959f-4793-8307-90f642bd3250", "node_type": "1", "metadata": {}, "hash": "0db5c8cc0338f9174a510d546f537b17a5d860c6cc028e23196df01ddd6bf99d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pachyderm\r\nlink: https://github.com/pachyderm/pachyderm\r\ndescription: Pachyderm is a cost-effective data engineering automation platform that enables complex pipeline creation with sophisticated data transformation capabilities across all types of data. It provides parallel processing of multi-stage, language-agnostic pipelines with data versioning and lineage tracking, delivering the ultimate CI/CD engine for data. Pachyderm can be deployed locally or on AWS/GCE/Azure in about 5 minutes, and its comprehensive documentation includes tutorials, example projects, and advanced features. If you want to see some examples and learn about core use cases for Pachyderm, check out their official documentation or refer to their contributing guide. Pachyderm automatically reports anonymized usage metrics, which can be disabled by setting the env variable `METRICS` to `false` in the pachd container. Lastly, Pachyderm has moved some components to a source-available limited license, which allows for access to the source code for modification or redistribution but prohibits use of the code to create competing offerings.", "start_char_idx": 0, "end_char_idx": 1132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc3df753-959f-4793-8307-90f642bd3250": {"__data__": {"id_": "dc3df753-959f-4793-8307-90f642bd3250", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "583508e3-d4be-4c1b-85f3-5d557b1d2220", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b9ad48f964dfa8ec4896d7afd8146cf2954be82c397d2accee060fd2848d2178", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29bbb71b-7d11-42f4-92f3-3fc507391e2b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3ebd38635f3ef48f8fcb01423ed829d4fcdf2b6affae2640eeb69a2c7bdc3c68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c31bb63-2f5f-44f0-bca8-252890763a6e", "node_type": "1", "metadata": {}, "hash": "098d254854264a6b4d038024effafcb89b38b43e21793be7a197a36ade6c7ca0", "class_name": "RelatedNodeInfo"}}, "text": "It provides parallel processing of multi-stage, language-agnostic pipelines with data versioning and lineage tracking, delivering the ultimate CI/CD engine for data. Pachyderm can be deployed locally or on AWS/GCE/Azure in about 5 minutes, and its comprehensive documentation includes tutorials, example projects, and advanced features. If you want to see some examples and learn about core use cases for Pachyderm, check out their official documentation or refer to their contributing guide. Pachyderm automatically reports anonymized usage metrics, which can be disabled by setting the env variable `METRICS` to `false` in the pachd container. Lastly, Pachyderm has moved some components to a source-available limited license, which allows for access to the source code for modification or redistribution but prohibits use of the code to create competing offerings. For more information, check out their License FAQ Page.", "start_char_idx": 265, "end_char_idx": 1188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c31bb63-2f5f-44f0-bca8-252890763a6e": {"__data__": {"id_": "1c31bb63-2f5f-44f0-bca8-252890763a6e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c13c6518-9797-4e0d-9141-7049fe1346d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7cf5b5de90cba8ae431f72e2764cc54a3323ca75ca6dd6d150f8892fe134e775", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc3df753-959f-4793-8307-90f642bd3250", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "20df9eb150b100648291ced7995aa47ef9346ca9b2135d6d64c2e92913398eb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dda69810-d236-46a3-b769-60adeb812233", "node_type": "1", "metadata": {}, "hash": "ab1d83ab6d4dfac2d9157408ffe3de43f2b6f3c8259062114a213e1e393cc338", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: modeldb\r\nlink: https://github.com/VertaAI/modeldb\r\ndescription: ModelDB is an open-source system to version machine learning models including their ingredients code, data, config, and environment, and to track ML metadata across the model lifecycle. Use ModelDB in order to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. If you are looking for a hosted version of ModelDB, please reach out at modeldb@verta.ai. This version of ModelDB is built upon its predecessor from CSAIL, MIT and the previous version can be found on Github. The ModelDB project is now maintained by Verta.ai. The official documentation for ModelDB can be found here, and for Getting Started guides, Tutorials, and API reference check out our docs. To report a bug, file a documentation issue, or submit a feature request, please open a GitHub issue.", "start_char_idx": 0, "end_char_idx": 957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dda69810-d236-46a3-b769-60adeb812233": {"__data__": {"id_": "dda69810-d236-46a3-b769-60adeb812233", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c13c6518-9797-4e0d-9141-7049fe1346d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7cf5b5de90cba8ae431f72e2764cc54a3323ca75ca6dd6d150f8892fe134e775", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c31bb63-2f5f-44f0-bca8-252890763a6e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a2cf2c2ad1ef1e1086565619052be7dfbb83a18e3ffe416ca5f613563fdb0d38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "090b3b46-9a65-446b-addb-6c6f36d618cf", "node_type": "1", "metadata": {}, "hash": "d7021b5895ee5f06d01651c49a470d522b2bea1ab0a5cdea438f76973206326f", "class_name": "RelatedNodeInfo"}}, "text": "Use ModelDB in order to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. If you are looking for a hosted version of ModelDB, please reach out at modeldb@verta.ai. This version of ModelDB is built upon its predecessor from CSAIL, MIT and the previous version can be found on Github. The ModelDB project is now maintained by Verta.ai. The official documentation for ModelDB can be found here, and for Getting Started guides, Tutorials, and API reference check out our docs. To report a bug, file a documentation issue, or submit a feature request, please open a GitHub issue. For help, questions, contribution discussions, and release announcements, please join us on Slack.", "start_char_idx": 261, "end_char_idx": 1056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "090b3b46-9a65-446b-addb-6c6f36d618cf": {"__data__": {"id_": "090b3b46-9a65-446b-addb-6c6f36d618cf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\246.txt", "file_name": "246.txt", "file_type": "text/plain", "file_size": 916, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6b670f2-0215-4223-97e2-b2b3a1102108", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\246.txt", "file_name": "246.txt", "file_type": "text/plain", "file_size": 916, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dae798c412ec35fab50eaa31f1b7781e051d46fc227f63e67e79f1c525e6c1fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dda69810-d236-46a3-b769-60adeb812233", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "af6845b13816fd24b419bb4180cc4af27beb1fe84add64527eff1d6d518cbda1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe7a46cb-cbaa-44b7-a170-95fbceace055", "node_type": "1", "metadata": {}, "hash": "20ae4c8665e2e418dc953675deb463fd9adff5e5d8a7d578950f4a2e282c372e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: modelstore\r\nlink: https://github.com/operatorai/modelstore\r\ndescription: `modelstore` is a Python library that allows you to version, export, save and download machine learning models in your choice of storage. It is an open source model registry that is being built in the open. With `modelstore`, you can automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, and fund open source developers. `modelstore` supports various storage types and machine learning libraries, and it is available for installation through `pip`. To learn more about `modelstore`, you can refer to the documentation or the full example in this Colab notebook. You can also give feedback by completing this survey: https://forms.gle/XShU3zrZcnLRWsk36. `modelstore` is licensed under the Apache License, Version 2.0.", "start_char_idx": 0, "end_char_idx": 914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe7a46cb-cbaa-44b7-a170-95fbceace055": {"__data__": {"id_": "fe7a46cb-cbaa-44b7-a170-95fbceace055", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b36ba21-c6ab-45e4-bf49-a849a2b98c73", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "febacccd8c023366b1e030801db718352bd05de4058971dd0ff1cb829428387d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "090b3b46-9a65-446b-addb-6c6f36d618cf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\246.txt", "file_name": "246.txt", "file_type": "text/plain", "file_size": 916, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8202b562baf5547dcbd7bceb78b61cef551021aa3fce967c1a6ef20e821e9571", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d614a7c-db0e-4fad-b858-c94398bf472b", "node_type": "1", "metadata": {}, "hash": "e6a9a23a894d65527f3f8bff54c67432cd83779c5cc1c3bd0237750aab748a82", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: modelchimp\r\nlink: https://github.com/ModelChimp/modelchimp\r\ndescription: Introducing ModelChimp - an experiment tracker for Deep Learning and Machine Learning experiments. With ModelChimp, data scientists and machine learning engineers/enthusiasts can spend more time on experiments and not on managing the data related to the experiments. ModelChimp provides features like automating any workflow, hosting and managing packages, finding and fixing vulnerabilities, creating instant dev environments, collaborating outside of code, funding open source developers and writing better code with AI. Choose either Docker based installation or the manual approach. For Docker based installation, download Docker from https://docs.docker.com/install/ and after starting ModelChimp server, access it at http://localhost:8000. For manual installation, refer to the documentation. Production Deployment can be done referring to the docker-compose.local.yml with the container orchestration of your choice.", "start_char_idx": 0, "end_char_idx": 1007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d614a7c-db0e-4fad-b858-c94398bf472b": {"__data__": {"id_": "6d614a7c-db0e-4fad-b858-c94398bf472b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b36ba21-c6ab-45e4-bf49-a849a2b98c73", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "febacccd8c023366b1e030801db718352bd05de4058971dd0ff1cb829428387d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe7a46cb-cbaa-44b7-a170-95fbceace055", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c2a476ab44b50748d73d084dde2e11537b0454c436c0dc9da04d27114df38586", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "708f2286-03a5-4523-a013-1b3b8409f3ba", "node_type": "1", "metadata": {}, "hash": "65d3a1b4ab6280f0e4e7c7cf5afcd85483ab0a9cade8ce76a97577de6b5aff61", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: modelchimp\r\nlink: https://github.com/ModelChimp/modelchimp\r\ndescription: Introducing ModelChimp - an experiment tracker for Deep Learning and Machine Learning experiments. With ModelChimp, data scientists and machine learning engineers/enthusiasts can spend more time on experiments and not on managing the data related to the experiments. ModelChimp provides features like automating any workflow, hosting and managing packages, finding and fixing vulnerabilities, creating instant dev environments, collaborating outside of code, funding open source developers and writing better code with AI. Choose either Docker based installation or the manual approach. For Docker based installation, download Docker from https://docs.docker.com/install/ and after starting ModelChimp server, access it at http://localhost:8000. For manual installation, refer to the documentation. Production Deployment can be done referring to the docker-compose.local.yml with the container orchestration of your choice. If you'd like to start ModelChimp manually, use the following command \"docker-compose -f docker-compose.local.yml up --build -d\" to start the containers in daemon mode on the machine where ModelChimp resides.", "start_char_idx": 0, "end_char_idx": 1216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "708f2286-03a5-4523-a013-1b3b8409f3ba": {"__data__": {"id_": "708f2286-03a5-4523-a013-1b3b8409f3ba", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\248.txt", "file_name": "248.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e591e5d6-8d4e-45e5-a792-19676a6d9605", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\248.txt", "file_name": "248.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f86fb72b3a7b83fa3d534c42fbafc56fe98137f5a586e2532616139746b95416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d614a7c-db0e-4fad-b858-c94398bf472b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0489765092ff9e4e7254b35d0c81baee36a3e9631c6b495c8ad659f8c737b88e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c001b22-4e9c-4532-81d0-603b1df4c15c", "node_type": "1", "metadata": {}, "hash": "edc6497200dd0b335ba5a80aa21e0c3c3ec2525bcd3c4ef6e31f97a6c45974fd", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mlflow\r\nlink: https://github.com/mlflow/mlflow\r\ndescription:", "start_char_idx": 0, "end_char_idx": 71, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c001b22-4e9c-4532-81d0-603b1df4c15c": {"__data__": {"id_": "3c001b22-4e9c-4532-81d0-603b1df4c15c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\249.txt", "file_name": "249.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13983090-f65f-48b9-b71d-bd4c9aa8490d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\249.txt", "file_name": "249.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8c1f3bb0f1237255e366c92cef7f9c4df1b39eef534ab16eae98e12f26f4ed4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "708f2286-03a5-4523-a013-1b3b8409f3ba", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\248.txt", "file_name": "248.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "69388d2f288aa2cb0320ecf898a853a4ef4208c8ed37aff5d7deb25a41e9afbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1753368e-0d73-48e8-abb8-dd2c261a18ee", "node_type": "1", "metadata": {}, "hash": "a4ff98fb01029362af8d41b7fafede01cd4b07a098c8e3b1c1a9b03cca9b84ce", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: MLWatcher\r\nlink: https://github.com/anodot/MLWatcher\r\ndescription:", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1753368e-0d73-48e8-abb8-dd2c261a18ee": {"__data__": {"id_": "1753368e-0d73-48e8-abb8-dd2c261a18ee", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\25.txt", "file_name": "25.txt", "file_type": "text/plain", "file_size": 390, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72554f62-e08a-480d-8392-871cf190eb85", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\25.txt", "file_name": "25.txt", "file_type": "text/plain", "file_size": 390, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8bc747d1d56de1fdc993fe23608bb35c7be71da7b721450ef761190b58a2f864", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c001b22-4e9c-4532-81d0-603b1df4c15c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\249.txt", "file_name": "249.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "584f6043d9855bcfb6d579888d6768bc462a83262e2f836d5bc0b1ce12089ce1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "728eaf11-fd8e-4045-84c5-85c6180a80d3", "node_type": "1", "metadata": {}, "hash": "341acc2be03688661221ae91e3b92d8b45f1bca568dfb622b275537511d41592", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: zeppelin\r\nlink: https://github.com/apache/zeppelin\r\ndescription: Introducing **Zeppelin**! Zeppelin is a web-based notebook that enables interactive data analytics. With Zeppelin, you can make beautiful data-driven, interactive, and collaborative documents with SQL, Scala, and more. To learn more about Zeppelin and its features, visit our website https://zeppelin.apache.org.", "start_char_idx": 0, "end_char_idx": 388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "728eaf11-fd8e-4045-84c5-85c6180a80d3": {"__data__": {"id_": "728eaf11-fd8e-4045-84c5-85c6180a80d3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\250.txt", "file_name": "250.txt", "file_type": "text/plain", "file_size": 792, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fa9a752-9ff4-4586-9201-c2ebb5a2e67f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\250.txt", "file_name": "250.txt", "file_type": "text/plain", "file_size": 792, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ff23c6297c47a8c9e69620ff7ac570441660969c80c758ac21ec1af18aacc97a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1753368e-0d73-48e8-abb8-dd2c261a18ee", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\25.txt", "file_name": "25.txt", "file_type": "text/plain", "file_size": 390, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6d3925f1f8ef5e00c1e5e0405874809f1803cdfb207076c3a3471c2ed96ec90e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b91ef1a-bf99-4af1-bd0f-abd24fc7602d", "node_type": "1", "metadata": {}, "hash": "4e7f911dab2bd009dd24515ca8ae4644c99fc86ce6eaca10aadcd509ba82953a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lakeFS\r\nlink: https://github.com/treeverse/lakeFS\r\ndescription: ## lakeFS is Data Version Control (Git for Data)\r\n\r\nlakeFS is an open-source tool that transforms your object storage into a Git-like repository. It enables you to manage your data lake the way you manage your code. With lakeFS you can build repeatable, atomic, and versioned data lake operations - from complex ETL jobs to data science and analytics. lakeFS supports AWS S3, Azure Blob Storage, and Google Cloud Storage as its underlying storage service. It is API compatible with S3 and works seamlessly with all modern data frameworks such as Spark, Hive, AWS Athena, DuckDB, and Presto. For more information, see the documentation.\r\n\r\nNote: This is excluding any information related to the installation process.", "start_char_idx": 0, "end_char_idx": 790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b91ef1a-bf99-4af1-bd0f-abd24fc7602d": {"__data__": {"id_": "2b91ef1a-bf99-4af1-bd0f-abd24fc7602d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\251.txt", "file_name": "251.txt", "file_type": "text/plain", "file_size": 579, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3eb513b-74c4-466d-b043-b9fbb99eaea7", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\251.txt", "file_name": "251.txt", "file_type": "text/plain", "file_size": 579, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ec00643748c76067c4aded80f134abec9ff4989f62e0f17e66c3280ebaa4c985", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "728eaf11-fd8e-4045-84c5-85c6180a80d3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\250.txt", "file_name": "250.txt", "file_type": "text/plain", "file_size": 792, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0e0e8ef11090e95d3933ab003ceca5f7ee8ee4ad0bf2f58fbea34f164826d39a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01f74b0d-6c93-4516-a962-0d33fba1d09c", "node_type": "1", "metadata": {}, "hash": "a3efdd1e2c1023f98e67c8d55f592cfe3fab3b312d32c0e6a06c613585dba6fc", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: flor\r\nlink: https://github.com/ucbrise/flor\r\ndescription: FLOR is a suite of machine learning tools for hindsight logging. Hindsight logging is an optimistic logging practice favored by agile model developers. Model developers log training metrics such as the loss and accuracy by default, and selectively restore additional training data --- like tensor histograms, images, and overlays --- post-hoc, if and when there is evidence of a problem. FLOR is software developed at UC Berkeley's RISE Lab, and is being released as part of an accompanying VLDB publication.", "start_char_idx": 0, "end_char_idx": 577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01f74b0d-6c93-4516-a962-0d33fba1d09c": {"__data__": {"id_": "01f74b0d-6c93-4516-a962-0d33fba1d09c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\252.txt", "file_name": "252.txt", "file_type": "text/plain", "file_size": 84, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8658ee9e-5f94-4165-bde8-5eb944f66b4d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\252.txt", "file_name": "252.txt", "file_type": "text/plain", "file_size": 84, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c80f5c87353c2062aca301a0408d8f3681dcec98c9bc5f82fa992899ceb0fcbb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b91ef1a-bf99-4af1-bd0f-abd24fc7602d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\251.txt", "file_name": "251.txt", "file_type": "text/plain", "file_size": 579, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d00e0751fb8e57ae2d5a416ccc2f6c1a8401dac41a23fc04aafed29a34c20a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b8d66cb-acf6-4b2a-b811-87a71dd65c0a", "node_type": "1", "metadata": {}, "hash": "34615348cccab55c26d0f27cf5fa17d21ed4ee94353d59a51081e6007140c1fd", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: hangar-py\r\nlink: https://github.com/tensorwerk/hangar-py\r\ndescription:", "start_char_idx": 0, "end_char_idx": 81, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b8d66cb-acf6-4b2a-b811-87a71dd65c0a": {"__data__": {"id_": "7b8d66cb-acf6-4b2a-b811-87a71dd65c0a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\253.txt", "file_name": "253.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37041e6b-f3af-4bbf-83e7-7a6b054dd476", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\253.txt", "file_name": "253.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fc862f09188684a1a0c71326160ea64fcd66833d7136d2cdc564313b5a723c41", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01f74b0d-6c93-4516-a962-0d33fba1d09c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\252.txt", "file_name": "252.txt", "file_type": "text/plain", "file_size": 84, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "70fdb8f33d755e44a7fc36561668f2df60f59f191fbd98240dfb97b83981c54d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b43f319b-1c45-4a9d-909e-6822424e233d", "node_type": "1", "metadata": {}, "hash": "67f3a26343c7fe8fa2922159f0bb5f2d60ffa64a9639be6718cdf4f43992abd3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: dvc\r\nlink: https://github.com/iterative/dvc\r\ndescription:", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b43f319b-1c45-4a9d-909e-6822424e233d": {"__data__": {"id_": "b43f319b-1c45-4a9d-909e-6822424e233d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\254.txt", "file_name": "254.txt", "file_type": "text/plain", "file_size": 749, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40b2b11c-e13b-4b31-a3eb-e28c95e4bed2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\254.txt", "file_name": "254.txt", "file_type": "text/plain", "file_size": 749, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "764122612d2dcdcdee065b92aab461cafe2bfff36cd2d4e17165f4e65d023fc4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b8d66cb-acf6-4b2a-b811-87a71dd65c0a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\253.txt", "file_name": "253.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "58693ddd26735ee12b975e2726472a72e905d24560d07f802e1febc35be8fc2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0c4dd9e-ead3-4bff-a81a-979589f1187b", "node_type": "1", "metadata": {}, "hash": "11251056cf0e52b752dc47d9921ccf38f9feed0f323d0d7f3fa0e5724139ab5a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: FGLab\r\nlink: https://github.com/Kaixhin/FGLab\r\ndescription: FGLab is a machine learning dashboard, designed to make prototyping experiments easier. Experiment details and results are sent to a database, which allows analytics to be performed after their completion. The server is FGLab, and the clients are FGMachines. FGLab tries to follow the SemVer standard whenever possible. Releases can be found here. There are 3 ways to run FGLab: Installing locally, via Docker, or hosted on Heroku. Run `node lab` (or `npm start`) to start FGLab. You can now access the user interface from a browser on the current machine. Please read the overview to understand how FGLab and FGMachine cooperate - both are needed in order to run experiments.", "start_char_idx": 0, "end_char_idx": 747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0c4dd9e-ead3-4bff-a81a-979589f1187b": {"__data__": {"id_": "d0c4dd9e-ead3-4bff-a81a-979589f1187b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\255.txt", "file_name": "255.txt", "file_type": "text/plain", "file_size": 316, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bed6ce8a-dbe0-4ef7-8e1c-167947e586b9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\255.txt", "file_name": "255.txt", "file_type": "text/plain", "file_size": 316, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6fc147b702aae2b805b253ae485eb17248444bd420460a300018bfbd5f63a844", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b43f319b-1c45-4a9d-909e-6822424e233d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\254.txt", "file_name": "254.txt", "file_type": "text/plain", "file_size": 749, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bc8066bac8c38e37e0303a3a37a65ece6dbc382151f52c664b3c29d8b1a75390", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87a92049-411f-4a25-86cc-8e830b8f4df5", "node_type": "1", "metadata": {}, "hash": "d0a20af322bbe03f3739d55bdfa0d0f62c2c0cea551e45fe0df172f34640871e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: catalyst\r\nlink: https://github.com/catalyst-team/catalyst\r\ndescription: 'Catalyst helps you implement compact but full-featured Deep Learning pipelines.with just a few lines of code. You get a training loop with metrics, early-.stopping, model checkpointing, and other features without the boilerplate.'", "start_char_idx": 0, "end_char_idx": 314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87a92049-411f-4a25-86cc-8e830b8f4df5": {"__data__": {"id_": "87a92049-411f-4a25-86cc-8e830b8f4df5", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\256.txt", "file_name": "256.txt", "file_type": "text/plain", "file_size": 753, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e442320d-4037-4bc7-8595-edfa35ae9456", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\256.txt", "file_name": "256.txt", "file_type": "text/plain", "file_size": 753, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f4517d6e4277e2e2626809a58362392b0b2bfc3b04313da88a7793eff3e065f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0c4dd9e-ead3-4bff-a81a-979589f1187b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\255.txt", "file_name": "255.txt", "file_type": "text/plain", "file_size": 316, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a124188a7e379ef9ccac00b482c368cde1f51fc9ce63579392400e7d632ec5de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d76d8899-a808-4c5f-9856-5808f7399579", "node_type": "1", "metadata": {}, "hash": "856ae395597c21b4588be6097dc810fa297e4775471a8451ecb5af9a000352b0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: d6tflow\r\nlink: https://github.com/d6t/d6tflow\r\ndescription: For data scientists and data engineers, `d6tflow` is a python library which makes building complex data science workflows easy, fast and intuitive. It is primarily designed for data scientists to build better models faster. For data engineers, it can also be a lightweight alternative and help productionize data science models faster. Unlike other data pipeline/workflow solutions, `d6tflow` focuses on managing data science research workflows instead of managing production data pipelines. With `d6tflow` you can easily chain together complex data flows and execute them. You can quickly load input and output data for each task. It makes your workflow very clear and intuitive.", "start_char_idx": 0, "end_char_idx": 751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d76d8899-a808-4c5f-9856-5808f7399579": {"__data__": {"id_": "d76d8899-a808-4c5f-9856-5808f7399579", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\257.txt", "file_name": "257.txt", "file_type": "text/plain", "file_size": 504, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b75d4795-94a5-4cf9-8df7-f7ed22423be6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\257.txt", "file_name": "257.txt", "file_type": "text/plain", "file_size": 504, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "39e8d89e8242523384aa553886263ca71aeb307bbe45569abe8581e27364b1b4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87a92049-411f-4a25-86cc-8e830b8f4df5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\256.txt", "file_name": "256.txt", "file_type": "text/plain", "file_size": 753, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "45390fd830df222a45acd0d9597700df6b7bc42d8227dcd604a9c0037e735913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1b5c9fb-611f-4e05-9141-b7e38bb2d5fa", "node_type": "1", "metadata": {}, "hash": "2c75a82c20d9ac6f7ba835d694c8221cccbbc5c82b2d4bf11f8c85b0619e7a7a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: incubator-marvin\r\nlink: https://github.com/apache/incubator-marvin\r\ndescription: Introducing the Apache Marvin AI Platform - an incubator project designed to help automate workflows, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, and fund open source developers all within the GitHub community. Subscribe to the user and dev mailing lists or follow the Contribution Guidelines to get involved.", "start_char_idx": 0, "end_char_idx": 502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1b5c9fb-611f-4e05-9141-b7e38bb2d5fa": {"__data__": {"id_": "c1b5c9fb-611f-4e05-9141-b7e38bb2d5fa", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\258.txt", "file_name": "258.txt", "file_type": "text/plain", "file_size": 612, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "410672fa-014f-4455-a82f-9e20ca8141a4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\258.txt", "file_name": "258.txt", "file_type": "text/plain", "file_size": 612, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2128a6099597a4f21ca5c0b88358900fbad7933110b548664531a41a9fa8f11e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d76d8899-a808-4c5f-9856-5808f7399579", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\257.txt", "file_name": "257.txt", "file_type": "text/plain", "file_size": 504, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1ff4226def0cb6bc8d7df6386cb995e877346f6c986bdc248d7f6815604b886c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc07e5f1-915d-4abf-96c6-f77dad5b3ee1", "node_type": "1", "metadata": {}, "hash": "3479ddcedfd7ac3aaaceeadd72ec95b76ce058f3d2a7dd0be844343c6114db4d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: aim\r\nlink: https://github.com/aimhubio/aim\r\ndescription: Aim is an open-source, self-hosted AI Metadata tracking tool designed to handle 100,000s of tracked metadata sequences. Aim provides a performant and beautiful UI for exploring and comparing metadata such as training runs or agents executions. Additionally, its SDK enables programmatic access to tracked metadata \u2014 perfect for automations and Jupyter Notebook analysis. Aim's mission is to democratize AI dev tools \ud83c\udfaf and the aim logs all your AI metadata, enables a UI to observe/compare them, and an SDK to query them programmatically.", "start_char_idx": 0, "end_char_idx": 605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc07e5f1-915d-4abf-96c6-f77dad5b3ee1": {"__data__": {"id_": "cc07e5f1-915d-4abf-96c6-f77dad5b3ee1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\259.txt", "file_name": "259.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6369e180-1485-4fc7-9815-1239aa7b1246", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\259.txt", "file_name": "259.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "009f65d5f8c797506b59edb4bfad6e67f2378a437e748b87af7091f5d594594c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1b5c9fb-611f-4e05-9141-b7e38bb2d5fa", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\258.txt", "file_name": "258.txt", "file_type": "text/plain", "file_size": 612, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e179ae93b66fbc0eebd0257ba09f1fb7dec9e6d718b44ce43e0f975ce0d95d4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "538782fc-a847-4a95-a9f6-d597c1505930", "node_type": "1", "metadata": {}, "hash": "daed32e5284a5eac7fb88c1f3691045d5e51e008bbcd6370b438158643ed3d5e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: tf-encrypted\r\nlink: https://github.com/tf-encrypted/tf-encrypted\r\ndescription: TF Encrypted is a framework for encrypted machine learning in TensorFlow. It looks and feels like TensorFlow, taking advantage of the ease-of-use of the Keras API while enabling training and prediction over encrypted data via secure multi-party computation and homomorphic encryption. TF Encrypted aims to make privacy-preserving machine learning readily available, without requiring expertise in cryptography, distributed systems, or high performance computing. See below for more background material, explore the examples, or visit the documentation to learn more about how to use the library.", "start_char_idx": 0, "end_char_idx": 685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "538782fc-a847-4a95-a9f6-d597c1505930": {"__data__": {"id_": "538782fc-a847-4a95-a9f6-d597c1505930", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\26.txt", "file_name": "26.txt", "file_type": "text/plain", "file_size": 773, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f8c9b27-f5c3-4269-ac33-c14d2e3186e5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\26.txt", "file_name": "26.txt", "file_type": "text/plain", "file_size": 773, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a47ad8326550868c2b8e1a0833c2f1435746a0712e6c8b4e628a8adaaf2acfeb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc07e5f1-915d-4abf-96c6-f77dad5b3ee1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\259.txt", "file_name": "259.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0a8e121bf1d19f4678767c5848395a9421d7d27e1336b2a1160944b3324463cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9eb084ea-b1c4-4dd6-be9e-0566427f9af7", "node_type": "1", "metadata": {}, "hash": "74ff669394af8727b23f0330ef80f1d91355adbd59f900acc98e204b13c5a9ba", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: serving\r\nlink: https://github.com/tensorflow/serving\r\ndescription: TensorFlow Serving is a flexible, high-performance serving system for machine.learning models, designed for production environments. It deals with the._inference_ aspect of machine learning, taking models after _training_ and.managing their lifetimes, providing clients with versioned access via a high-.performance, reference-counted lookup table. TensorFlow Serving provides out-.of-the-box integration with TensorFlow models, but can be easily extended to.serve other types of models and data. To note a few features, you can serve a Tensorflow model in 60 seconds and follow an end-to-end training and serving tutorial. Please refer to the official TensorFlow website for more information.", "start_char_idx": 0, "end_char_idx": 771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9eb084ea-b1c4-4dd6-be9e-0566427f9af7": {"__data__": {"id_": "9eb084ea-b1c4-4dd6-be9e-0566427f9af7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\260.txt", "file_name": "260.txt", "file_type": "text/plain", "file_size": 834, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "653580ba-9804-4817-a658-1d654d434395", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\260.txt", "file_name": "260.txt", "file_type": "text/plain", "file_size": 834, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "28e5378c97b1b5514d2a64c997f8aadb302993c10f9c604e789da65bcb6b8fd1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "538782fc-a847-4a95-a9f6-d597c1505930", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\26.txt", "file_name": "26.txt", "file_type": "text/plain", "file_size": 773, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3f51d3d63e81160ab7738264b2d55838a8bf4044ebc63c558a1ba40f4b9f874a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11804f52-b7fd-4f5a-831e-9fa4b2626a83", "node_type": "1", "metadata": {}, "hash": "61e56a0912016fb24c4b910d104d20b0b282d67667ac731bb5e89a90e7513db2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: substra\r\nlink: https://github.com/Substra/substra\r\ndescription: Substra is an open source federated learning (FL) software. It enables the.training and validation of machine learning models on distributed datasets. It.provides a flexible Python interface and a web application to run federated.learning training at scale. This specific repository is the low-level Python.library used to interact with a Substra network. Substra's main usage is in production environments. It has already been.deployed and used by hospitals and biotech companies (see the MELLODDY project.for instance). Substra can also be used on a single machine to perform FL.simulations and debug code. Substra was originally developed by Owkin and is now hosted by the Linux.Foundation for AI and Data. Today Owkin is the main contributor to Substra.", "start_char_idx": 0, "end_char_idx": 832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11804f52-b7fd-4f5a-831e-9fa4b2626a83": {"__data__": {"id_": "11804f52-b7fd-4f5a-831e-9fa4b2626a83", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\261.txt", "file_name": "261.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7e3d947-82f2-4775-9941-b5643f9482b0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\261.txt", "file_name": "261.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "646523f292e006c4c5f9b0d701148f2506ca17125a6f352bccb080eedb5ce0d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9eb084ea-b1c4-4dd6-be9e-0566427f9af7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\260.txt", "file_name": "260.txt", "file_type": "text/plain", "file_size": 834, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4e2495f51d87cf0cbe340125206a7fe558ffe51a21fd3f62724d0ecd7623deeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4d7367d-b46b-417a-9f1d-b7e0a97768cf", "node_type": "1", "metadata": {}, "hash": "194afbc2fa7b0bc8814c3f28652ae3b15d5ca6d84fb10fe77bfb0ffa81633f9b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: privacy\r\nlink: https://github.com/tensorflow/privacy\r\ndescription: This repository contains the source code for TensorFlow Privacy, a Python library that includes implementations of TensorFlow optimizers for training machine learning models with differential privacy. The library comes with tutorials and analysis tools for computing the privacy guarantees provided. The TensorFlow Privacy library is under continual development, always welcoming contributions. In particular, we always welcome help towards resolving the issues currently open.", "start_char_idx": 0, "end_char_idx": 555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4d7367d-b46b-417a-9f1d-b7e0a97768cf": {"__data__": {"id_": "a4d7367d-b46b-417a-9f1d-b7e0a97768cf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\262.txt", "file_name": "262.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "229093f3-3138-4816-9550-b7625c8dd1f9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\262.txt", "file_name": "262.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8949e8c5172a2e7deea18854cf3da8ebb1ff76cfaefc5b29a341367c3d523366", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11804f52-b7fd-4f5a-831e-9fa4b2626a83", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\261.txt", "file_name": "261.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a60284e39f6e77c8cd53c8e1e73b5ef450c8f5bbb1d8cbab74690fc4a56f6e50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1c21496-cf4f-4114-a988-f5ca08cd6a00", "node_type": "1", "metadata": {}, "hash": "fd3bed912b31992c9137a1af70bb22a3f57de6270da4b6ab40010799257cb392", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: SEAL\r\nlink: https://github.com/microsoft/SEAL\r\ndescription:", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1c21496-cf4f-4114-a988-f5ca08cd6a00": {"__data__": {"id_": "b1c21496-cf4f-4114-a988-f5ca08cd6a00", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3384521b-7f77-4c93-b6fb-f3e87ba96198", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "453d5eb78bcfae0268cb3ceb9d4352c80f9cd83178b7bf7d6bb49b7a85ff1f8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4d7367d-b46b-417a-9f1d-b7e0a97768cf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\262.txt", "file_name": "262.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1433696c8ddc556d90b971ee2eba9c1c742f3fbfd29e9714f0127c9f5bb2cc35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6c59747-7be1-466f-a30d-b783577c59a6", "node_type": "1", "metadata": {}, "hash": "5af24904dad09583fc0dc77ced2d2d0846b48650d8522956d99c626969994ad7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: PySyft\r\nlink: https://github.com/OpenMined/PySyft\r\ndescription: PySyft is a Python library for secure and private Deep Learning. PySyft decouples private data from model training, using Federated Learning, Differential Privacy, and Encrypted Computation (like Multi-Party Computation (MPC) and Homomorphic Encryption (HE)) within the main Deep Learning frameworks like PyTorch and TensorFlow. The Syft ecosystem seeks to change the limitation of human collaboration by allowing you to write software which can compute over information you do not own on machines you do not have control over. This not only includes servers in the cloud but also personal desktops, laptops, mobile phones, websites, and edge devices. The PyGrid library serves as an API for the management and deployment of PySyft at scale. PySyft is available on PyPI and Conda, and a more detailed explanation of PySyft can be found in the white paper on Arxiv.", "start_char_idx": 0, "end_char_idx": 939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c59747-7be1-466f-a30d-b783577c59a6": {"__data__": {"id_": "e6c59747-7be1-466f-a30d-b783577c59a6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3384521b-7f77-4c93-b6fb-f3e87ba96198", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "453d5eb78bcfae0268cb3ceb9d4352c80f9cd83178b7bf7d6bb49b7a85ff1f8e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1c21496-cf4f-4114-a988-f5ca08cd6a00", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "74f69a9fded38f9214525ee93a27cc5b7367e8d6cce66924004ad6c1e60828ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8534770-760a-4fa3-a0b0-163b6825dfd0", "node_type": "1", "metadata": {}, "hash": "bd4906e75f7ea4bdca860b30f7d37996e9697cf317595fa23162fe80d2938f93", "class_name": "RelatedNodeInfo"}}, "text": "PySyft decouples private data from model training, using Federated Learning, Differential Privacy, and Encrypted Computation (like Multi-Party Computation (MPC) and Homomorphic Encryption (HE)) within the main Deep Learning frameworks like PyTorch and TensorFlow. The Syft ecosystem seeks to change the limitation of human collaboration by allowing you to write software which can compute over information you do not own on machines you do not have control over. This not only includes servers in the cloud but also personal desktops, laptops, mobile phones, websites, and edge devices. The PyGrid library serves as an API for the management and deployment of PySyft at scale. PySyft is available on PyPI and Conda, and a more detailed explanation of PySyft can be found in the white paper on Arxiv. The software is in beta, and support is offered through the #support Slack channel.", "start_char_idx": 140, "end_char_idx": 1023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8534770-760a-4fa3-a0b0-163b6825dfd0": {"__data__": {"id_": "b8534770-760a-4fa3-a0b0-163b6825dfd0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\264.txt", "file_name": "264.txt", "file_type": "text/plain", "file_size": 1204, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b1b3d92-6a39-44d3-a7d3-297ccdd96c5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\264.txt", "file_name": "264.txt", "file_type": "text/plain", "file_size": 1204, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b6e4a6c7f73d1625e726b966ae7f13d6acd7bd2f4eca1fc4afceddbb7339ed4e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6c59747-7be1-466f-a30d-b783577c59a6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "24e734a7bc93fae81aaeaad6b30ef431c7c0ade81b53528300570776e3158403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "723ed975-1add-4c2d-a413-14d22565705a", "node_type": "1", "metadata": {}, "hash": "e9fba163e62580bdb53ed9bba583d75719b129d6938c8210c47bfb36350873a9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: differential-privacy\r\nlink: https://github.com/google/differential-privacy\r\ndescription: \"Differential Privacy is a concept that provides techniques to protect sensitive information while still allowing data analysis. This repository contains libraries to generate \u03b5- and (\u03b5, \u03b4)-differentially private statistics over datasets. It contains various tools and supports multiple algorithms. The DP building block libraries and Privacy on Beam are suitable for research, experimental, or production use cases, while some other tools are currently experimental and subject to change. It is important to note that to run the differential privacy library, Bazel and Git installation are required. Once installed, you can clone the differential-privacy directory and build the differential privacy library using Bazel. It is essential to keep in mind the caveats of the DP building block libraries that require a bound on the maximum number of contributions each user can make to a single aggregation. Our floating-point implementations are subject to vulnerabilities. We will continue to publish updates and improvements, but please note that this is not an officially supported Google product.\"", "start_char_idx": 0, "end_char_idx": 1199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "723ed975-1add-4c2d-a413-14d22565705a": {"__data__": {"id_": "723ed975-1add-4c2d-a413-14d22565705a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\265.txt", "file_name": "265.txt", "file_type": "text/plain", "file_size": 961, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dc4c0d8-e421-4b38-a621-5b14d3ceddac", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\265.txt", "file_name": "265.txt", "file_type": "text/plain", "file_size": 961, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "97abfd90ff35a2aba9cd714701c9e3bc5e8126d8d08d87e9489e20c7b470a524", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8534770-760a-4fa3-a0b0-163b6825dfd0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\264.txt", "file_name": "264.txt", "file_type": "text/plain", "file_size": 1204, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "17d78bb60ff8c27d3413cbf8a1976ee2c87635558100d47290a67549aea4430d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ea4a482-450e-4b9e-99eb-dd4453e2f14d", "node_type": "1", "metadata": {}, "hash": "79e0f9a4e7a74ed4f216d56571eba8020e1355741d3ca18f7e0142663578326f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: he-transformer\r\nlink: https://github.com/NervanaSystems/he-transformer\r\ndescription: The **Intel\u00ae HE transformer for nGraph\u2122** is a Homomorphic Encryption (HE) backend to the **Intel\u00ae nGraph Compiler**, Intel's graph compiler for Artificial Neural Networks. Homomorphic encryption is a form of encryption that allows computation on encrypted data, and is an attractive remedy to increasing concerns about data privacy in the field of machine learning. This project is meant as a proof-of-concept to demonstrate the feasibility of HE on local machines. The goal is to measure performance of various HE schemes for deep learning. Currently, we support the CKKS encryption scheme, implemented by the Simple Encrypted Arithmetic Library (SEAL) from Microsoft Research. Additionally, we integrate with the **Intel\u00ae nGraph\u2122 Compiler and runtime engine for TensorFlow** to allow users to run inference on trained neural networks through Tensorflow.", "start_char_idx": 0, "end_char_idx": 952, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ea4a482-450e-4b9e-99eb-dd4453e2f14d": {"__data__": {"id_": "8ea4a482-450e-4b9e-99eb-dd4453e2f14d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\266.txt", "file_name": "266.txt", "file_type": "text/plain", "file_size": 980, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4782ab0-30f2-416f-9324-abf42622fcf8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\266.txt", "file_name": "266.txt", "file_type": "text/plain", "file_size": 980, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2f494ee08414e7134d3f5ecde5b8e9c2adbeb6d85f687e705e9c7d72d557b2b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "723ed975-1add-4c2d-a413-14d22565705a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\265.txt", "file_name": "265.txt", "file_type": "text/plain", "file_size": 961, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7354f70a49653144acdd682f9db3f03491596bc6099eb8b40078a2e7bc63503f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b547964c-2e71-40cf-9855-cbe8afc06e55", "node_type": "1", "metadata": {}, "hash": "7f6a811006b31aa1c05e1e1e3a7aa1f6ddedd0c98aee85566d05ae3cd561ccca", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: xai\r\nlink: https://github.com/EthicalML/xai\r\ndescription: XAI is a Machine Learning library that is designed with AI explainability in.its core. XAI contains various tools that enable for analysis and evaluation.of data and models. The XAI library is maintained by The Institute for Ethical.AI & ML, and it was developed based on the 8 principles for Responsible.Machine Learning. We see the challenge of explainability as more than just an algorithmic.challenge, which requires a combination of data science best practices with.domain-specific knowledge. The XAI library is designed to empower machine.learning engineers and relevant domain experts to analyse the end-to-end.solution and identify discrepancies that may result in sub-optimal performance.relative to the objectives required. More broadly, the XAI library is designed.using the 3-steps of explainable machine learning, which involve 1) data.analysis, 2) model evaluation, and 3) production monitoring.", "start_char_idx": 0, "end_char_idx": 978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b547964c-2e71-40cf-9855-cbe8afc06e55": {"__data__": {"id_": "b547964c-2e71-40cf-9855-cbe8afc06e55", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\267.txt", "file_name": "267.txt", "file_type": "text/plain", "file_size": 92, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f799979-c08b-4342-bc19-8dea7255a769", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\267.txt", "file_name": "267.txt", "file_type": "text/plain", "file_size": 92, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "849a39c1afee9e9b620c4ae6d851a0a79bd93f59dd9271297f840340f3dcd127", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ea4a482-450e-4b9e-99eb-dd4453e2f14d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\266.txt", "file_name": "266.txt", "file_type": "text/plain", "file_size": 980, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0baf43c929de3256431c65af1287d8e33e8981bf4016479a10f75e0bde1de00e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "527bfde7-d7c4-49a1-ab33-a428273daf2f", "node_type": "1", "metadata": {}, "hash": "5e7e594f3f58e2608f77ef7d881ffacca01ff3ef4b3f0f12f198baa875c688e3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: treeinterpreter\r\nlink: https://github.com/andosa/treeinterpreter\r\ndescription:", "start_char_idx": 0, "end_char_idx": 89, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "527bfde7-d7c4-49a1-ab33-a428273daf2f": {"__data__": {"id_": "527bfde7-d7c4-49a1-ab33-a428273daf2f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\268.txt", "file_name": "268.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea472040-355b-407c-b0dd-021b5ccaae9a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\268.txt", "file_name": "268.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "69f88645a840b60897a992fe514a08347e4817276909d73d85f86dbadf1fa267", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b547964c-2e71-40cf-9855-cbe8afc06e55", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\267.txt", "file_name": "267.txt", "file_type": "text/plain", "file_size": 92, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9744c2d28a615e7453456b0797f183763cd39c51fa42e8d5467aa4d8a925a688", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23737fb5-fee8-4ede-b4b8-59910f8bc373", "node_type": "1", "metadata": {}, "hash": "ae03a3c5f96bed99547351f1b1541894fbdbf65612a69e36d397ffef2de0270e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: woe\r\nlink: https://github.com/boredbird/woe\r\ndescription:", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23737fb5-fee8-4ede-b4b8-59910f8bc373": {"__data__": {"id_": "23737fb5-fee8-4ede-b4b8-59910f8bc373", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\269.txt", "file_name": "269.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21abde37-b7f3-4706-bd70-608bd96f42ad", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\269.txt", "file_name": "269.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ff2487aa3e7c6212e4c5394d1db38c0347115f32c9708b32801013d559a0ebcc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "527bfde7-d7c4-49a1-ab33-a428273daf2f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\268.txt", "file_name": "268.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b031f54f7e3e59ccc3e2490952da22a8d5fff93035b8e1e6a6bb7b5eccf58ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78abc551-2069-4142-9e8e-67ac0033ebdc", "node_type": "1", "metadata": {}, "hash": "4cd75bfd739ea8c6832828324ef95f0013c753aa904c397621427ae9d859365d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: themis-ml\r\nlink: https://github.com/cosmicBboy/themis-ml\r\ndescription: `Themis-ml` is a Python library built on top of `pandas` and `sklearn` that implements fairness-aware machine learning algorithms. Its features include measuring and mitigating discrimination, preprocessing, model estimation, and postprocessing. It also provides utility functions for loading freely available datasets from a variety of sources. This library defines fairness as the inverse of discrimination and provides different techniques to ensure fairness in machine learning predictions. The source code is hosted on GitHub, and the latest released version can be installed with `conda` or `pip`. For more information, visit the official documentation and the references for the discrimination discovery and fairness-aware methods implemented in `themis-ml`.", "start_char_idx": 0, "end_char_idx": 847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78abc551-2069-4142-9e8e-67ac0033ebdc": {"__data__": {"id_": "78abc551-2069-4142-9e8e-67ac0033ebdc", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\27.txt", "file_name": "27.txt", "file_type": "text/plain", "file_size": 585, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc13cfb7-e4b7-48d7-a96e-fe1551a66467", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\27.txt", "file_name": "27.txt", "file_type": "text/plain", "file_size": 585, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b14dd679d6cf02c5295e708263ff404df2f4d7b9187c538c9af4bdbadda31a65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23737fb5-fee8-4ede-b4b8-59910f8bc373", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\269.txt", "file_name": "269.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "582638d66e42b2316d925a0aacf952c8f76906e67acb054ed4e7ca634593e3a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d34af291-fa2c-44c6-82eb-48f0b90a7f4e", "node_type": "1", "metadata": {}, "hash": "5c542b666b17e0a682c00cf5d405fb0e60b46a005ffb4cdbcf44ade4feca96e7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: deeplake\r\nlink: https://github.com/activeloopai/deeplake\r\ndescription: Introducing Data 2.0, powered by Hub. The fastest way to store, access & manage datasets with version-control for PyTorch/TensorFlow. Works locally or on any cloud. Scalable data pipelines. What is Hub for? Features Getting Started Access public data. Fast. Train a model Create a local dataset Upload your dataset and access it from anywhere in 3 simple steps Documentation Tutorial Notebooks Use Cases Why Hub specifically? Community Examples README Badge Usage Tracking Disclaimers Acknowledgement.", "start_char_idx": 0, "end_char_idx": 583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d34af291-fa2c-44c6-82eb-48f0b90a7f4e": {"__data__": {"id_": "d34af291-fa2c-44c6-82eb-48f0b90a7f4e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\270.txt", "file_name": "270.txt", "file_type": "text/plain", "file_size": 761, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02145cbd-934f-4faf-93d3-383d83cc3fa4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\270.txt", "file_name": "270.txt", "file_type": "text/plain", "file_size": 761, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "72d72d4d7b715dfd37d0e28ced96efa7da674a84228d9e49596e88a61bdf3ca8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78abc551-2069-4142-9e8e-67ac0033ebdc", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\27.txt", "file_name": "27.txt", "file_type": "text/plain", "file_size": 585, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d6617094e0d40eabcf3684e1657dfbf0d16d8c4e5382003d3fb87400eb9d4869", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "655a484a-ed22-4625-85f7-ba57c6accb8d", "node_type": "1", "metadata": {}, "hash": "fb5a36c6a1f12f61e7fb3c3f6e574ef8c3f5eb68763b0c7327cc26bcfe79e086", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: Themis\r\nlink: https://github.com/LASER-UMASS/Themis\r\ndescription: Themis\u2122 is a testing-based approach for measuring discrimination in a software system. For the best explanation of the underlying problem Themis\u2122 solves, Themis\u2122 algorithms, and an evaluation of Themis\u2122, read our paper Fairness Testing: Testing Software for Discrimination. This work won an ACM Distinguished Paper Award at the ESEC/FSE 2017 Conference. Themis\u2122 is distributed only to end users and for academic use only. Themis\u2122 can also be licensed for commercial use through the University of Massachusetts Amherst. To obtain a license, contact Thomas Ferguson (tferguson@umass.edu) and Robert MacWright (macwright@umass.edu), and copy Yuriy Brun (brun@cs.umass.edu).", "start_char_idx": 0, "end_char_idx": 747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "655a484a-ed22-4625-85f7-ba57c6accb8d": {"__data__": {"id_": "655a484a-ed22-4625-85f7-ba57c6accb8d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\271.txt", "file_name": "271.txt", "file_type": "text/plain", "file_size": 658, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8664dcc4-e923-484f-bcee-cfda6f77f0a9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\271.txt", "file_name": "271.txt", "file_type": "text/plain", "file_size": 658, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5483662b36fbf32d0f96ee16424d33ab8e4e5aa9a7eefa941a594b3e153eb6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d34af291-fa2c-44c6-82eb-48f0b90a7f4e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\270.txt", "file_name": "270.txt", "file_type": "text/plain", "file_size": 761, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "585e792eaae6f6c28342bd763fafaaed89364e1f4f3aac377fffeed3172f41cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca92d59f-ac8e-478c-8b18-ef6ae4f9d309", "node_type": "1", "metadata": {}, "hash": "9d2c3bedda44e52dba7e568f2d8ebc248266acb647ca7f787f7e73b048dae939", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: model-analysis\r\nlink: https://github.com/tensorflow/model-analysis\r\ndescription: _TensorFlow Model Analysis_ (TFMA) is a library for evaluating TensorFlow models. It allows users to evaluate their models on large amounts of data in a distributed manner, using the same metrics defined in their trainer. These metrics can be computed over different slices of data and visualized in Jupyter notebooks. The recommended way to install TFMA is using the PyPI package, but it can also be installed from source. TFMA has notable dependencies like TensorFlow, Apache Beam, and Apache Arrow. For more information on using TFMA, see the get started guide.", "start_char_idx": 0, "end_char_idx": 656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca92d59f-ac8e-478c-8b18-ef6ae4f9d309": {"__data__": {"id_": "ca92d59f-ac8e-478c-8b18-ef6ae4f9d309", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce26248e-e970-4ba2-8d9b-c5a97084f5de", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ddaa3c5570a901cda2ebf36fee3525fd426bf679bf5ada09f6c4714670382a6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "655a484a-ed22-4625-85f7-ba57c6accb8d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\271.txt", "file_name": "271.txt", "file_type": "text/plain", "file_size": 658, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "06f3d2d842269c751c1146ad71030f649b55b79c0c104ad8740fbe6f36e45915", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576a3288-4706-49d4-8706-0ddc6a170fa0", "node_type": "1", "metadata": {}, "hash": "5b16a4d27948a5b5b6f5ee45ab9e8cedaf3b0a21b8b9d56f64fcf18a397bc3c2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lucid\r\nlink: https://github.com/tensorflow/lucid\r\ndescription: **Lucid is a collection of infrastructure and tools for research in neural network interpretability.** We're not currently supporting tensorflow 2! If you'd like to use lucid in colab which defaults to tensorflow 2, add `%tensorflow_version 1.x` magic to a cell before you import tensorflow: **Lucid is research code, not production code. We provide no guarantee it will work for your use case. Lucid is maintained by volunteers who are unable to provide significant technical support.** Start visualizing neural networks _**with no setup**_. The following notebooks run right from your browser, thanks to Colaboratory. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud. You can run the notebooks on your local machine, too. Clone the repository and find them in the `notebooks` subfolder.", "start_char_idx": 0, "end_char_idx": 910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "576a3288-4706-49d4-8706-0ddc6a170fa0": {"__data__": {"id_": "576a3288-4706-49d4-8706-0ddc6a170fa0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce26248e-e970-4ba2-8d9b-c5a97084f5de", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ddaa3c5570a901cda2ebf36fee3525fd426bf679bf5ada09f6c4714670382a6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca92d59f-ac8e-478c-8b18-ef6ae4f9d309", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3cc5370e51c40de2a0b8d91223107576570bfdb22d81368b3b52296711c0694b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d04c16a7-c570-4492-98b5-d13820af0251", "node_type": "1", "metadata": {}, "hash": "371d65486444e3a53451fe318cc77b161c77fa1a695142f27b99fcc4129dcbc9", "class_name": "RelatedNodeInfo"}}, "text": "** We're not currently supporting tensorflow 2! If you'd like to use lucid in colab which defaults to tensorflow 2, add `%tensorflow_version 1.x` magic to a cell before you import tensorflow: **Lucid is research code, not production code. We provide no guarantee it will work for your use case. Lucid is maintained by volunteers who are unable to provide significant technical support.** Start visualizing neural networks _**with no setup**_. The following notebooks run right from your browser, thanks to Colaboratory. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud. You can run the notebooks on your local machine, too. Clone the repository and find them in the `notebooks` subfolder. You will need to run a local instance of the Jupyter notebook environment to execute them.", "start_char_idx": 174, "end_char_idx": 1001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d04c16a7-c570-4492-98b5-d13820af0251": {"__data__": {"id_": "d04c16a7-c570-4492-98b5-d13820af0251", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce26248e-e970-4ba2-8d9b-c5a97084f5de", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ddaa3c5570a901cda2ebf36fee3525fd426bf679bf5ada09f6c4714670382a6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "576a3288-4706-49d4-8706-0ddc6a170fa0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ffd38e9ffcf00dd50800ec1f95e3173ccc6817e2d25283c3dded88f9521176ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "656f8edd-bc45-4ad6-a095-1c80b30338e9", "node_type": "1", "metadata": {}, "hash": "7cd22d01d203ada9119fa2eb1c5aa63845c1d3914035e7dad9bfebd2fb0ff1bb", "class_name": "RelatedNodeInfo"}}, "text": "** We're not currently supporting tensorflow 2! If you'd like to use lucid in colab which defaults to tensorflow 2, add `%tensorflow_version 1.x` magic to a cell before you import tensorflow: **Lucid is research code, not production code. We provide no guarantee it will work for your use case. Lucid is maintained by volunteers who are unable to provide significant technical support.** Start visualizing neural networks _**with no setup**_. The following notebooks run right from your browser, thanks to Colaboratory. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud. You can run the notebooks on your local machine, too. Clone the repository and find them in the `notebooks` subfolder. You will need to run a local instance of the Jupyter notebook environment to execute them. The lucid notebooks include Tutorial Notebooks, Feature Visualization Notebooks, Building Blocks Notebooks, Differentiable Image Parameterizations Notebooks, Activation Atlas Notebooks, and Miscellaneous Notebooks.", "start_char_idx": 174, "end_char_idx": 1216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "656f8edd-bc45-4ad6-a095-1c80b30338e9": {"__data__": {"id_": "656f8edd-bc45-4ad6-a095-1c80b30338e9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\273.txt", "file_name": "273.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea3dfee1-4caf-4a42-a3ae-770e5e05bbc2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\273.txt", "file_name": "273.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "80a0874f8852dfbd103cbe93e577e2d79bb90134a4eeec8c9c046ccec761b21a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d04c16a7-c570-4492-98b5-d13820af0251", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f3964b892efd9a00f6b6131a6b0d693557ee696bbebfe6627bfa6beba0ce70f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e87cf59-bf8b-4d35-9b9a-e785182844b5", "node_type": "1", "metadata": {}, "hash": "b840b1ae04190793d401874f5aa6c4caa2724cefb4542daea765817a2a1c2bc4", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: skater\r\nlink: https://github.com/oracle/skater\r\ndescription: \"Skater is a unified framework to enable Model Interpretation for all forms of model to help one build an Interpretable machine learning system often needed for real world use-cases (** we are actively working towards to enabling faithful interpretability for all forms models). It is an open source python library designed to demystify the learned structures of a black box model both globally (inference on the basis of a complete data set) and locally (inference about an individual prediction). The project was started as a research idea to find ways to enable better interpretability (preferably human interpretability) to predictive \"black boxes\" both for researchers and practitioners. The project is still in beta phase.\"", "start_char_idx": 0, "end_char_idx": 801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e87cf59-bf8b-4d35-9b9a-e785182844b5": {"__data__": {"id_": "5e87cf59-bf8b-4d35-9b9a-e785182844b5", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\274.txt", "file_name": "274.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83e97aec-bd7b-4da0-acac-b3a27a37e031", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\274.txt", "file_name": "274.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6b5f02fc39ee3a3de3dabbaf38d0a895c6e99985c9369a137a9c81fd00e214a4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "656f8edd-bc45-4ad6-a095-1c80b30338e9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\273.txt", "file_name": "273.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "debe80949f4cab3b410518713bf8037443c2ed58d8f373ec5e485b66ed33e2fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fd501c7-b082-464c-8d24-49f44e45c9ee", "node_type": "1", "metadata": {}, "hash": "6dca802b366473f22c62570414f19e0daafeceed62edab5b26fabf38ef3c0df8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: shap\r\nlink: https://github.com/slundberg/shap\r\ndescription: **SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations).", "start_char_idx": 0, "end_char_idx": 374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fd501c7-b082-464c-8d24-49f44e45c9ee": {"__data__": {"id_": "6fd501c7-b082-464c-8d24-49f44e45c9ee", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\275.txt", "file_name": "275.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c77cd49-6487-41fb-a280-74d979e8a698", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\275.txt", "file_name": "275.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e45f5246da13b2979177ada02c824a508572e21d859bcba86da866e553861df2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e87cf59-bf8b-4d35-9b9a-e785182844b5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\274.txt", "file_name": "274.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "aee8a6597426543559e5c5df1035f6d112e36ca6267510c5e3577fa533212143", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9112d35e-cd82-46e3-b2e4-1ba6c83d0727", "node_type": "1", "metadata": {}, "hash": "27adea8ea3a3a64160ba7a661c3c2a2d6c7895ab2463dae8a1034fd7395958d0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: shapash\r\nlink: https://github.com/MAIF/shapash\r\ndescription: **Shapash** is a Python library which aims to make machine learning interpretable and understandable by everyone. It provides several types of visualization that display explicit labels that everyone can understand. Data Scientists can understand their models easily and share their results. End users can understand the decision proposed by a model using a summary of the most influential criteria. Shapash also contributes to data science auditing by displaying useful information about any model and data in a unique report. Shapash is intended to work with Python versions 3.8 to 3.10. The installation can be done with pip. In order to generate the Shapash Report, some extra requirements are needed. Shapash offers many tutorials to allow you to easily get started with it.", "start_char_idx": 0, "end_char_idx": 851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9112d35e-cd82-46e3-b2e4-1ba6c83d0727": {"__data__": {"id_": "9112d35e-cd82-46e3-b2e4-1ba6c83d0727", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\276.txt", "file_name": "276.txt", "file_type": "text/plain", "file_size": 91, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0da429c1-bb29-42a2-b88c-7aed00f6d1de", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\276.txt", "file_name": "276.txt", "file_type": "text/plain", "file_size": 91, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "220926062da081fb49016ed801d5426109b3363c2dc0aa617dda61fd2c636869", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fd501c7-b082-464c-8d24-49f44e45c9ee", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\275.txt", "file_name": "275.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f7b6169a7f5c2764699c8a7dc9d984ed99e6bae0b95b1463974eb53b5a39fc3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6033f94-ced0-4d14-a8d2-ad05b4264bc3", "node_type": "1", "metadata": {}, "hash": "03aa9d49a32841bef76b08cbd57f8fbc1063b43f91a59004a0b971c15d888377", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: responsibly\r\nlink: https://github.com/ResponsiblyAI/responsibly\r\ndescription:", "start_char_idx": 0, "end_char_idx": 88, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6033f94-ced0-4d14-a8d2-ad05b4264bc3": {"__data__": {"id_": "a6033f94-ced0-4d14-a8d2-ad05b4264bc3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\277.txt", "file_name": "277.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9fdb3ae-ad98-4985-a026-e07b64492b74", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\277.txt", "file_name": "277.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6513d6dec4f1ce787af77713f6378c02d8551ce09b8915870b97a258a75fc1e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9112d35e-cd82-46e3-b2e4-1ba6c83d0727", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\276.txt", "file_name": "276.txt", "file_type": "text/plain", "file_size": 91, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "352f8be7edd7575772743b00fbb814298ca35c437bd2423f90c3fa7fac42e33f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e31d398-fdb0-440e-85ed-7f841f82c8f7", "node_type": "1", "metadata": {}, "hash": "048902fca439e3c05dc890501fa44f7408c30a0f3ab82a7669741d3f8f0aacf6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: rcnn\r\nlink: https://github.com/taolei87/rcnn\r\ndescription: This repo contains Theano implementations of popular neural network components and optimization methods, including the RCNN (Recurrent Convolutional Neural Network). It also includes the implementation of three different models: 1) Neural question retrieval for community-based QA, 2) Sentiment analysis / document classification, and 3) Rationalizing neural predictions. The RCNN model is described in paper [2], with datasets and pre-trained word vectors available for download. The implementation of the RCNN in this repo requires Theano >= 0.7, Python >= 2.7, and Numpy.", "start_char_idx": 0, "end_char_idx": 644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e31d398-fdb0-440e-85ed-7f841f82c8f7": {"__data__": {"id_": "7e31d398-fdb0-440e-85ed-7f841f82c8f7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\278.txt", "file_name": "278.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "884a6e5e-8476-4039-8546-7d19b636f3c5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\278.txt", "file_name": "278.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a8bf71379be56ca52653033fcd603f68565951ee4f2a14509b67924bcdf2ab34", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6033f94-ced0-4d14-a8d2-ad05b4264bc3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\277.txt", "file_name": "277.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "564857695d258cb7e4e870684d958b47d485599e8c6524c3351d5a97ab0f0d24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1dd32eb-446a-4033-a773-ce6e50f810fe", "node_type": "1", "metadata": {}, "hash": "bdd12cacd82f05c2a73d7081d7c7cbeaf1941a61fd767b0abbc9ed89d08989fb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: netron\r\nlink: https://github.com/lutzroeder/netron\r\ndescription: Netron is a viewer for neural network, deep learning and machine learning models. It supports a wide range of frameworks including ONNX, TensorFlow Lite, Caffe, Keras, Darknet, PaddlePaddle, ncnn, MNN, Core ML, RKNN, MXNet, MindSpore Lite, TNN, Barracuda, Tengine, CNTK, TensorFlow.js, Caffe2 and UFF, with experimental support for PyTorch, TensorFlow, TorchScript, OpenVINO, Torch, Vitis AI, kmodel, Arm NN, BigDL, Chainer, Deeplearning4j, MediaPipe, MegEngine, ML.NET and scikit-learn. There are several ways to use Netron including downloading the appropriate installation file for your platform, running a server with `pip install netron` or opening the browser version.", "start_char_idx": 0, "end_char_idx": 750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1dd32eb-446a-4033-a773-ce6e50f810fe": {"__data__": {"id_": "d1dd32eb-446a-4033-a773-ce6e50f810fe", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\279.txt", "file_name": "279.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b85444e2-1908-4b33-97c6-fffa69287381", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\279.txt", "file_name": "279.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e925d8a5560a9f0b93b4b2e80e61cd49bfdd75dd2746a194ff14d32649b65f0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e31d398-fdb0-440e-85ed-7f841f82c8f7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\278.txt", "file_name": "278.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2d947881f15dddd75d5e26cc23fd5cf3a743e157ff294339b1f394217865f659", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdfb1451-9c27-42b5-87ea-2d0ab8eed262", "node_type": "1", "metadata": {}, "hash": "af042bd99dd9f91b49fa2d217109a6ec5d4f79466e26032bcf4186177ad3fa9a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mindsdb\r\nlink: https://github.com/mindsdb/mindsdb\r\ndescription: MindsDB is a server for artificial intelligence logic, enabling developers to ship AI powered projects from prototyping & experimentation to production in a fast & scalable way. MindsDB abstracts generative AI, large language, and other models as virtual tables on top of enterprise databases, which increases accessibility with organizations and enables development teams to use existing skills to build applications powered by AI. By taking a data-centric approach, MindsDB brings the AI process closer to the source of the data, minimizing the need to build and maintain data pipelines and ETLs, speeding up the time to deployment, and reducing complexity. MindsDB has an active and helpful community and offers features like database integrations, quickstart guides, and documentation for support.", "start_char_idx": 0, "end_char_idx": 876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdfb1451-9c27-42b5-87ea-2d0ab8eed262": {"__data__": {"id_": "bdfb1451-9c27-42b5-87ea-2d0ab8eed262", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\28.txt", "file_name": "28.txt", "file_type": "text/plain", "file_size": 914, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14263c8f-90e0-464c-8555-610a44d14da1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\28.txt", "file_name": "28.txt", "file_type": "text/plain", "file_size": 914, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "59c75bcc4833e0728c8f5fa2f1f20b26f77f2a068bf91e13aed4cf77fa0a78ff", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1dd32eb-446a-4033-a773-ce6e50f810fe", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\279.txt", "file_name": "279.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8518f40f644ea7af9489c24eb43df1fb49e5b8202788c21783f07ef15a7ddfab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58ae4b66-3599-4009-a72c-6b466942acdf", "node_type": "1", "metadata": {}, "hash": "d140769ce09e0366865f9bc5acca03f93d47dd7ff07c4b9b6c5d0650407724d0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: terminusdb\r\nlink: https://github.com/terminusdb/terminusdb\r\ndescription: TerminusDB is a distributed database with a collaboration model. It is designed to be like git, but for data. The building blocks of the model are - TerminusDB allows you to link JSON documents in a knowledge graph through a.document API. TerminusDB is available as a standalone server, or you can use our headless content and knowledge management system TerminusCMS. TerminusDB 11 features a new Rustified storage backend that reduces storage overhead and latency, improves search performance, and simplifies interchange. TerminusDB 11 also comes with some exciting features to make building easier and faster. The easiest way to install TerminusDB as a developer is by using the Snap. For deployments, you can install as a Docker Container. More information about installation and usage is available on the TerminusDB website.", "start_char_idx": 0, "end_char_idx": 912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58ae4b66-3599-4009-a72c-6b466942acdf": {"__data__": {"id_": "58ae4b66-3599-4009-a72c-6b466942acdf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\280.txt", "file_name": "280.txt", "file_type": "text/plain", "file_size": 843, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2bc1ebb-5c27-4528-ab24-5d371fd4d681", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\280.txt", "file_name": "280.txt", "file_type": "text/plain", "file_size": 843, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "09b681efb7745a467bd285d27e04be04824caa19ed2c85811f12a8372cbca547", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdfb1451-9c27-42b5-87ea-2d0ab8eed262", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\28.txt", "file_name": "28.txt", "file_type": "text/plain", "file_size": 914, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b3f9424b5f1155f8916c249433133af41011d83c9dc893316cc825fb053d52b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7cea8ce-08eb-4014-b482-3430ae73c40c", "node_type": "1", "metadata": {}, "hash": "097632500b41157eee13e13677c750c2bc7111fc5d4b93aa90ba2d8f35be6e77", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lime\r\nlink: https://github.com/marcotcr/lime\r\ndescription: This project is about explaining what machine learning classifiers (or models) are doing. At the moment, we support explaining individual predictions for text classifiers or classifiers that act on tables (numpy arrays of numerical or categorical data) or images, with a package called lime (short for local interpretable model-agnostic explanations). Lime is based on the work presented in this paper (bibtex here for citation). Lime is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a probability for each class. Support for scikit-learn classifiers is built-in. The lime package is on PyPI. Simply run, or clone the repository and run.", "start_char_idx": 0, "end_char_idx": 841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7cea8ce-08eb-4014-b482-3430ae73c40c": {"__data__": {"id_": "f7cea8ce-08eb-4014-b482-3430ae73c40c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40c1e146-3641-495a-87fc-df10d4045174", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4cd8957832a22e757bcf337201a0c4bcd8abec7607dee137a52ebb8cdd122259", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58ae4b66-3599-4009-a72c-6b466942acdf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\280.txt", "file_name": "280.txt", "file_type": "text/plain", "file_size": 843, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "88f73840fbc13477d96e611aeb1b9ce1fe90efa268e90added46000b3c7041ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7f89d06-8a09-4ca1-aeaf-e265e00c7bca", "node_type": "1", "metadata": {}, "hash": "a1bbd7ba0cba755d72e82902a02053c89fe7f4671b62b9802b744595674f5395", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lofo-importance\r\nlink: https://github.com/aerdem4/lofo-importance\r\ndescription: LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric. LOFO first evaluates the performance of the model with all the input features included, then iteratively removes one feature at a time, retrains the model, and evaluates its performance on a validation set. The mean and standard deviation (across the folds) of the importance of each feature is then reported. LOFO Importance has several advantages compared to other importance types and can be installed using pip install lofo-importance. Examples of LOFO Importance include Microsoft Malware Prediction Competition and Kaggle's TReNDS Competition.", "start_char_idx": 0, "end_char_idx": 940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7f89d06-8a09-4ca1-aeaf-e265e00c7bca": {"__data__": {"id_": "e7f89d06-8a09-4ca1-aeaf-e265e00c7bca", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40c1e146-3641-495a-87fc-df10d4045174", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4cd8957832a22e757bcf337201a0c4bcd8abec7607dee137a52ebb8cdd122259", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7cea8ce-08eb-4014-b482-3430ae73c40c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b6e48c5940763bfdc09b48d3dc0ea2b5d2f36bfd1ec7a02d65c8a508b2b1889f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5166aef1-3aea-448a-9814-83a8f5493185", "node_type": "1", "metadata": {}, "hash": "418469348dfde0de20b27ffc444f5d2185835cda659dc0f3eae22a811292062b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lofo-importance\r\nlink: https://github.com/aerdem4/lofo-importance\r\ndescription: LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric. LOFO first evaluates the performance of the model with all the input features included, then iteratively removes one feature at a time, retrains the model, and evaluates its performance on a validation set. The mean and standard deviation (across the folds) of the importance of each feature is then reported. LOFO Importance has several advantages compared to other importance types and can be installed using pip install lofo-importance. Examples of LOFO Importance include Microsoft Malware Prediction Competition and Kaggle's TReNDS Competition. For those who find running the LOFO Importance package too time-costly, Fast LOFO (FLOFO) is recommended as a faster alternative.", "start_char_idx": 0, "end_char_idx": 1070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5166aef1-3aea-448a-9814-83a8f5493185": {"__data__": {"id_": "5166aef1-3aea-448a-9814-83a8f5493185", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\282.txt", "file_name": "282.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67e1f39b-a8fc-4a1d-9564-c2704fc50f83", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\282.txt", "file_name": "282.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "32b802337d327828f8ffc6550dda0a1169a2a0c7a18bef53981e08f000978bde", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7f89d06-8a09-4ca1-aeaf-e265e00c7bca", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "250ea9ba17b8593a363eabf41d5a29cb44395c7885e68879105d7e71aa064de5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3aca073e-96bf-435d-8e74-d173f9d5ca15", "node_type": "1", "metadata": {}, "hash": "3a4b0679ffeca7813302e30f7bd65097e03e75777460dfd6cb04cf4f96b674e9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lightwood\r\nlink: https://github.com/mindsdb/lightwood\r\ndescription: Lightwood is like Legos for Machine Learning. A Pytorch based framework that breaks down machine learning problems into smaller blocks that can be glued together seamlessly with one objective: * Make it so simple that you can build predictive models with as little as one line of code. Learn more from the Lightwood's docs. You can install Lightwood from pip: pip3 install lightwood. Given the simple sensor_data.csv let's predict sensor3 values. from lightwood import Predictor. You can now predict what _sensor3_ value will be. prediction = sensor3_predictor.predict(when={'sensor1':1, 'sensor2':-1}). You can also try Lightwood in Google Colab: ! Thanks for your interest. There are many ways to contribute to this project. Please, check out our Contribution guide.", "start_char_idx": 0, "end_char_idx": 847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aca073e-96bf-435d-8e74-d173f9d5ca15": {"__data__": {"id_": "3aca073e-96bf-435d-8e74-d173f9d5ca15", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\283.txt", "file_name": "283.txt", "file_type": "text/plain", "file_size": 505, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8a7ae6a-91f7-4efb-8c9c-8922385a814b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\283.txt", "file_name": "283.txt", "file_type": "text/plain", "file_size": 505, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2fd3f9cbc3432904cc726f998face8f24ecf3d42cfa966db66b383b447423fc1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5166aef1-3aea-448a-9814-83a8f5493185", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\282.txt", "file_name": "282.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "65ffc0eb3fd8795ce47b449dc06b0434031f6225638815523220d9760c84ed76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb82a763-a146-44b4-9edb-210134feef54", "node_type": "1", "metadata": {}, "hash": "480530f420142b9dfd055525e9bec7ed5c27ac116c4746a5f6654a9da70f7581", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: L2X\r\nlink: https://github.com/Jianbo-Lab/L2X\r\ndescription: The L2X tool allows for automating workflows, hosting and managing packages, finding and fixing vulnerabilities, creating instant dev environments, writing better code with AI, and collaborating outside of code. Additionally, it requires the installation of Tensorflow 1.2.1 or higher and Keras 2.0 or higher, and the replication of experiments can be done through the code provided on Github with the citation of the original paper.", "start_char_idx": 0, "end_char_idx": 503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb82a763-a146-44b4-9edb-210134feef54": {"__data__": {"id_": "bb82a763-a146-44b4-9edb-210134feef54", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "93dc00553822211cecdb68f72a9fdad84ab34b7221998bcb479c88fca8b40e8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aca073e-96bf-435d-8e74-d173f9d5ca15", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\283.txt", "file_name": "283.txt", "file_type": "text/plain", "file_size": 505, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8407732da6faf83f448b997c548c8f78a1c1537b57ecf8f72c16cbe56f0ca912", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "377a4c7d-dbf6-4dc4-ae4b-11918c037062", "node_type": "1", "metadata": {}, "hash": "5c0a1c7e4c2003e8a368be8a4082621369ad9fe9faefdf12f1c3f3e54bd92e3a", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lightly\r\nlink: https://github.com/lightly-ai/lightly\r\ndescription: Lightly is a computer vision framework for self-supervised learning. We, at Lightly, are passionate engineers who want to make deep learning more efficient. That's why - together with our community - we want to popularize the use of self-supervised methods to understand and curate raw image data. Our solution can be applied before any data annotation step and the learned representations can be used to visualize and analyze datasets. This allows to select the best set of samples for model training through advanced filtering. Lightly offers features like hosted and managed packages, finding and fixing vulnerabilities, instant dev environments, writing better code with AI, collaborating outside of code, funding open source developers, and GitHub community articles. Lightly is compatible with PyTorch and PyTorch Lightning v2.0+, and the vision transformer based models require Torchvision v0.12+.", "start_char_idx": 0, "end_char_idx": 982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "377a4c7d-dbf6-4dc4-ae4b-11918c037062": {"__data__": {"id_": "377a4c7d-dbf6-4dc4-ae4b-11918c037062", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "93dc00553822211cecdb68f72a9fdad84ab34b7221998bcb479c88fca8b40e8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb82a763-a146-44b4-9edb-210134feef54", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "75a3434b12e35fd5981bffb36b83d10b4d447246f2ca3a04624543137eb8aabc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87f260b9-5a4b-4692-a875-606efce54f02", "node_type": "1", "metadata": {}, "hash": "4181443f52f915f6295d87567053a2bb2685775341334a9eb9a8aba831c85a1d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lightly\r\nlink: https://github.com/lightly-ai/lightly\r\ndescription: Lightly is a computer vision framework for self-supervised learning. We, at Lightly, are passionate engineers who want to make deep learning more efficient. That's why - together with our community - we want to popularize the use of self-supervised methods to understand and curate raw image data. Our solution can be applied before any data annotation step and the learned representations can be used to visualize and analyze datasets. This allows to select the best set of samples for model training through advanced filtering. Lightly offers features like hosted and managed packages, finding and fixing vulnerabilities, instant dev environments, writing better code with AI, collaborating outside of code, funding open source developers, and GitHub community articles. Lightly is compatible with PyTorch and PyTorch Lightning v2.0+, and the vision transformer based models require Torchvision v0.12+. Lightly in Action allows for the use of the latest self-supervised learning methods in a modular way using the full power of PyTorch, and experimenting with different backbones, models, and loss functions.", "start_char_idx": 0, "end_char_idx": 1188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87f260b9-5a4b-4692-a875-606efce54f02": {"__data__": {"id_": "87f260b9-5a4b-4692-a875-606efce54f02", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "93dc00553822211cecdb68f72a9fdad84ab34b7221998bcb479c88fca8b40e8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "377a4c7d-dbf6-4dc4-ae4b-11918c037062", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81c1c0b87c8b12c8d188d556793489ec7099fa79c717490c52878e1c5d4c5b03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ae5ef6f-8505-4454-a4ab-d4ae4bb89511", "node_type": "1", "metadata": {}, "hash": "2fd8afd94771e70e8170eecb6230ae225862b45a206f1601df39de1ba1af4240", "class_name": "RelatedNodeInfo"}}, "text": "That's why - together with our community - we want to popularize the use of self-supervised methods to understand and curate raw image data. Our solution can be applied before any data annotation step and the learned representations can be used to visualize and analyze datasets. This allows to select the best set of samples for model training through advanced filtering. Lightly offers features like hosted and managed packages, finding and fixing vulnerabilities, instant dev environments, writing better code with AI, collaborating outside of code, funding open source developers, and GitHub community articles. Lightly is compatible with PyTorch and PyTorch Lightning v2.0+, and the vision transformer based models require Torchvision v0.12+. Lightly in Action allows for the use of the latest self-supervised learning methods in a modular way using the full power of PyTorch, and experimenting with different backbones, models, and loss functions. Implemented models and their performance on various datasets behind hyperparameters are not tuned for maximum accuracy.", "start_char_idx": 235, "end_char_idx": 1308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ae5ef6f-8505-4454-a4ab-d4ae4bb89511": {"__data__": {"id_": "7ae5ef6f-8505-4454-a4ab-d4ae4bb89511", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "93dc00553822211cecdb68f72a9fdad84ab34b7221998bcb479c88fca8b40e8f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87f260b9-5a4b-4692-a875-606efce54f02", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9f042a424b339c1a27c24ec18185d66ed217a9e85d24749c6208f243387d0ff3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cee8624d-44b8-4b31-9025-1ed16e48d090", "node_type": "1", "metadata": {}, "hash": "188aa6ea0d2e3127c89167711e1592c568d3e89bc34985a5100bba5b1c539967", "class_name": "RelatedNodeInfo"}}, "text": "Our solution can be applied before any data annotation step and the learned representations can be used to visualize and analyze datasets. This allows to select the best set of samples for model training through advanced filtering. Lightly offers features like hosted and managed packages, finding and fixing vulnerabilities, instant dev environments, writing better code with AI, collaborating outside of code, funding open source developers, and GitHub community articles. Lightly is compatible with PyTorch and PyTorch Lightning v2.0+, and the vision transformer based models require Torchvision v0.12+. Lightly in Action allows for the use of the latest self-supervised learning methods in a modular way using the full power of PyTorch, and experimenting with different backbones, models, and loss functions. Implemented models and their performance on various datasets behind hyperparameters are not tuned for maximum accuracy. For detailed results and more info about the benchmarks, visit the website.", "start_char_idx": 376, "end_char_idx": 1384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cee8624d-44b8-4b31-9025-1ed16e48d090": {"__data__": {"id_": "cee8624d-44b8-4b31-9025-1ed16e48d090", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\285.txt", "file_name": "285.txt", "file_type": "text/plain", "file_size": 673, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f7267bc-a13a-43cf-9ef7-db2c9ff1c252", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\285.txt", "file_name": "285.txt", "file_type": "text/plain", "file_size": 673, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "999a4e65b9a300191334640f358f0e3834bf9b7bd12a9e7cbade228ae8c607c9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ae5ef6f-8505-4454-a4ab-d4ae4bb89511", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4541f511731e6dcc4b91537767778839928254b89381a9c4f47990f34fe964fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5073d26-4061-4667-87a2-954e6ec492bb", "node_type": "1", "metadata": {}, "hash": "67e44662cbe114f6ac0e579c93c5e9de792e48f324af060ade0337f7576a0023", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: keras-vis\r\nlink: https://github.com/raghakot/keras-vis\r\ndescription: Keras Visualization Toolkit (keras-vis) is a high-level toolkit for visualizing and debugging trained Keras neural net models. It offers various supported visualizations, including Conv filter visualization, Dense layer visualization, and Attention Maps. The toolkit is compatible with both Theano and TensorFlow backends and supports N-dimensional image inputs. It also generalizes all visualizations as energy minimization problems with a clean, extendable interface. The library requires Keras >2.0 and various examples of the supported visualizations can be found in the examples folder.", "start_char_idx": 0, "end_char_idx": 671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5073d26-4061-4667-87a2-954e6ec492bb": {"__data__": {"id_": "a5073d26-4061-4667-87a2-954e6ec492bb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\286.txt", "file_name": "286.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61e4a964-0740-48d8-8268-f714df1b37f6", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\286.txt", "file_name": "286.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2d67929e79d608f47bd4b3ef631679d12c1f389dcbd91a193d9bd41fc1f335a9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cee8624d-44b8-4b31-9025-1ed16e48d090", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\285.txt", "file_name": "285.txt", "file_type": "text/plain", "file_size": 673, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "92c9bf68eeece4394fb11b521142148a28ad19f3f95e6960a961b43dfa97aa3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb4f1aee-5c59-4b42-a2dd-c2e15e16eb0e", "node_type": "1", "metadata": {}, "hash": "4cae1bfeef09a07c35fb9fff0fe46c73fcb3c575c2cc7c1acd0441e60347de8f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: Integrated-Gradients\r\nlink: https://github.com/ankurtaly/Integrated-Gradients\r\ndescription: Integrated Gradients is a method used to attribute the prediction of a deep network to its input features, applicable to a variety of deep models. It requires no modification to the original network, is simple to implement, and has various applications from aiding developers in debugging to giving users some transparency into the reason for a network's prediction. The method was introduced in a paper titled \"Axiomatic Attribution for Deep Networks\" by Mukund Sundararajan, Ankur Taly, and Qiqi Yan in 2017, and code for implementing integrated gradients for networks with image inputs is available in a GitHub repository.", "start_char_idx": 0, "end_char_idx": 728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb4f1aee-5c59-4b42-a2dd-c2e15e16eb0e": {"__data__": {"id_": "bb4f1aee-5c59-4b42-a2dd-c2e15e16eb0e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\287.txt", "file_name": "287.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd75d9cc-0703-4376-9260-1172239c346c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\287.txt", "file_name": "287.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5749bec9973c14ea6c9cbfd11a86aeae1ec3cf6f8be2e9fb7f5508cfc8ba3b80", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5073d26-4061-4667-87a2-954e6ec492bb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\286.txt", "file_name": "286.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "cf65312fbabd1eee4d8c051f98a230f7f500cd5c0612dcd3c0eed3e75c5dd5e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f29e0489-f5b6-40a1-80a0-75959ada2449", "node_type": "1", "metadata": {}, "hash": "d8363f9c1efc9d2eff5e4aaf63534e94195468aa01e6e16d7589f5c5029da45e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AIF360\r\nlink: https://github.com/Trusted-AI/AIF360\r\ndescription: The AI Fairness 360 toolkit is an extensible open-source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. The AI Fairness 360 package is available in both Python and R. The package includes supported bias mitigation algorithms, supported fairness metrics, and an interactive experience for data scientists. The library is still in development and encourages contributions of metrics, explainers, and debiasing algorithms. Installation instructions are provided for Conda, pip, and manual installation. Examples and troubleshooting steps are also available.", "start_char_idx": 0, "end_char_idx": 750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f29e0489-f5b6-40a1-80a0-75959ada2449": {"__data__": {"id_": "f29e0489-f5b6-40a1-80a0-75959ada2449", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\288.txt", "file_name": "288.txt", "file_type": "text/plain", "file_size": 1035, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "498780bd-084c-436b-b101-114bd40fc00f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\288.txt", "file_name": "288.txt", "file_type": "text/plain", "file_size": 1035, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7948ab4976af7d1aa4f0f3ee9565974c6c7fa08f7929613b2c053fb60c75ea98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb4f1aee-5c59-4b42-a2dd-c2e15e16eb0e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\287.txt", "file_name": "287.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f9e275fc6f9d7b7df51bd810ec9698254e3c890614db9bd9596036fe41ba15cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "123f2acf-5b38-41a4-bad2-0e38e247eb1a", "node_type": "1", "metadata": {}, "hash": "712fad619e7db1c035bff51dde84d5eed9cbc96c07d09152dfcf91a11c4ada67", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: innvestigate\r\nlink: https://github.com/albermax/innvestigate\r\ndescription: iNNvestigate is a library that provides a common interface and out-of-the-box implementation for many analysis methods aimed at making analyzing neural networks' predictions easy. It contains implementations for several methods such as Saliency, Deconvnet, GuidedBackprop, SmoothGrad, IntegratedGradients, LRP, PatternNet, and PatternAttribution. If you use this code, please star the repository and cite the following paper. iNNvestigate is based on Keras and TensorFlow 2 and can be installed with the following commands. Please note that iNNvestigate currently requires disabling TF2's eager execution. To use the example scripts and notebooks, one additionally needs to install the package matplotlib. The library's tests can be executed via `pytest`. The iNNvestigate library contains implementations for various methods. Checkout the available documentation to learn more about this library and how to contribute or add your analysis method.", "start_char_idx": 0, "end_char_idx": 1033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "123f2acf-5b38-41a4-bad2-0e38e247eb1a": {"__data__": {"id_": "123f2acf-5b38-41a4-bad2-0e38e247eb1a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8869a8af3cba498debf5c32acdf00306ab13778f90dbb889c620bafdebf21815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f29e0489-f5b6-40a1-80a0-75959ada2449", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\288.txt", "file_name": "288.txt", "file_type": "text/plain", "file_size": 1035, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7059ad47683fb6fa142279db8fd9f941aa6f5e7d49767821bd2ceed3b30330ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa9b1d42-6863-4bea-92c3-459afb3e14c9", "node_type": "1", "metadata": {}, "hash": "03075834d94d6d7733997c95c61014246808440e7e902777cde7f477eade443b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AIX360\r\nlink: https://github.com/Trusted-AI/AIX360\r\ndescription: The AI Explainability 360 toolkit is an open-source library that supports interpretability and explainability of datasets and machine learning models. The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics. The AI Explainability 360 interactive experience provides a gentle introduction to the concepts and capabilities. To help, they have created some guidance material and a chart that can be consulted. This library is still in development and they encourage you to contribute your explainability algorithms, metrics, and use cases. AIX360 requires specific versions of many Python packages which may conflict with other projects on your system. The supported configurations are laid out, and a virtual environment manager is strongly recommended to ensure dependencies may be installed safely. The setup and installation instructions for AIX360 are provided.", "start_char_idx": 0, "end_char_idx": 1055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa9b1d42-6863-4bea-92c3-459afb3e14c9": {"__data__": {"id_": "fa9b1d42-6863-4bea-92c3-459afb3e14c9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8869a8af3cba498debf5c32acdf00306ab13778f90dbb889c620bafdebf21815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "123f2acf-5b38-41a4-bad2-0e38e247eb1a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c188fb98fe97c55c53b24b890dd16504489fd31db17eab2fff1c64066a7a51ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f92f839f-0a59-4333-baa4-c896467827f7", "node_type": "1", "metadata": {}, "hash": "2ff6e43443d93d61430da9c1385702d4d813a408085186dd9eeec926c5eaca36", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AIX360\r\nlink: https://github.com/Trusted-AI/AIX360\r\ndescription: The AI Explainability 360 toolkit is an open-source library that supports interpretability and explainability of datasets and machine learning models. The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics. The AI Explainability 360 interactive experience provides a gentle introduction to the concepts and capabilities. To help, they have created some guidance material and a chart that can be consulted. This library is still in development and they encourage you to contribute your explainability algorithms, metrics, and use cases. AIX360 requires specific versions of many Python packages which may conflict with other projects on your system. The supported configurations are laid out, and a virtual environment manager is strongly recommended to ensure dependencies may be installed safely. The setup and installation instructions for AIX360 are provided. The examples directory contains a diverse collection of Jupyter notebooks that use AI Explainability 360 in various ways.", "start_char_idx": 0, "end_char_idx": 1177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f92f839f-0a59-4333-baa4-c896467827f7": {"__data__": {"id_": "f92f839f-0a59-4333-baa4-c896467827f7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8869a8af3cba498debf5c32acdf00306ab13778f90dbb889c620bafdebf21815", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa9b1d42-6863-4bea-92c3-459afb3e14c9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e97d60bd39af665911f8080fcb0ce38f2425b4e6cea3cbb742124eb0b55b7e95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26479a0b-868b-4989-aeb6-009f32b9a5c0", "node_type": "1", "metadata": {}, "hash": "7bf8a89dbd6779e101c43185e7a6681ed9c58438a84456439c28b8c0ac8a821b", "class_name": "RelatedNodeInfo"}}, "text": "The AI Explainability 360 Python package includes a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics. The AI Explainability 360 interactive experience provides a gentle introduction to the concepts and capabilities. To help, they have created some guidance material and a chart that can be consulted. This library is still in development and they encourage you to contribute your explainability algorithms, metrics, and use cases. AIX360 requires specific versions of many Python packages which may conflict with other projects on your system. The supported configurations are laid out, and a virtual environment manager is strongly recommended to ensure dependencies may be installed safely. The setup and installation instructions for AIX360 are provided. The examples directory contains a diverse collection of Jupyter notebooks that use AI Explainability 360 in various ways. If you are using AI Explainability 360 for your work, we encourage you to cite the following paper.", "start_char_idx": 227, "end_char_idx": 1277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26479a0b-868b-4989-aeb6-009f32b9a5c0": {"__data__": {"id_": "26479a0b-868b-4989-aeb6-009f32b9a5c0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63ad6d6e-f431-4dd2-834c-381c4698680c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1a12668fe53308b69d53e029af1483824aba4eee1781583db2a61114db0a1c28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f92f839f-0a59-4333-baa4-c896467827f7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "325b7215340aa4b11eb6954080a2df2d7ea709187023e164b28e8a668b8cf201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d77deb0a-cdbb-4438-bc81-2bba3f6396f7", "node_type": "1", "metadata": {}, "hash": "1d7b811ca66582a77d43f9c78529d0024477b6ec65201f70dd123f7466cf6f37", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: facets\r\nlink: https://github.com/PAIR-code/facets\r\ndescription: The facets project contains two visualizations for understanding and analyzing machine learning datasets: Facets Overview and Facets Dive. The visualizations are implemented as Polymer web components, backed by Typescript code and can be easily embedded into Jupyter notebooks or webpages. Overview gives a high-level view of one or more data sets. It produces a visual feature-by-feature statistical analysis, and can also be used to compare statistics across two or more data sets. The tool can process both numeric and string features, including multiple instances of a number or string per feature. Dive is a tool for interactively exploring up to tens of thousands of multidimensional data points, allowing users to seamlessly switch between a high-level overview and low-level details. Details about Overview usage can be found in its README. Details about Dive usage can be found in its README. Using Facets in Google Colabratory and Jupyter notebooks can be seen in this notebook.", "start_char_idx": 0, "end_char_idx": 1063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d77deb0a-cdbb-4438-bc81-2bba3f6396f7": {"__data__": {"id_": "d77deb0a-cdbb-4438-bc81-2bba3f6396f7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63ad6d6e-f431-4dd2-834c-381c4698680c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1a12668fe53308b69d53e029af1483824aba4eee1781583db2a61114db0a1c28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26479a0b-868b-4989-aeb6-009f32b9a5c0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "29f85eb41503dff4b710b540629bde5f960e93adb070fe67caae39f5605c2292", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb507157-ddc1-43b6-b3c5-db02cc20adfd", "node_type": "1", "metadata": {}, "hash": "35ea1d5358eff5e9019e5b189c61b85edd729b6fa37a1d8e0083c85be2a4836a", "class_name": "RelatedNodeInfo"}}, "text": "The visualizations are implemented as Polymer web components, backed by Typescript code and can be easily embedded into Jupyter notebooks or webpages. Overview gives a high-level view of one or more data sets. It produces a visual feature-by-feature statistical analysis, and can also be used to compare statistics across two or more data sets. The tool can process both numeric and string features, including multiple instances of a number or string per feature. Dive is a tool for interactively exploring up to tens of thousands of multidimensional data points, allowing users to seamlessly switch between a high-level overview and low-level details. Details about Overview usage can be found in its README. Details about Dive usage can be found in its README. Using Facets in Google Colabratory and Jupyter notebooks can be seen in this notebook. Both Facets visualizations make use of HTML imports.", "start_char_idx": 214, "end_char_idx": 1116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb507157-ddc1-43b6-b3c5-db02cc20adfd": {"__data__": {"id_": "bb507157-ddc1-43b6-b3c5-db02cc20adfd", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63ad6d6e-f431-4dd2-834c-381c4698680c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1a12668fe53308b69d53e029af1483824aba4eee1781583db2a61114db0a1c28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d77deb0a-cdbb-4438-bc81-2bba3f6396f7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1fcdcfe83e0a729467924367a3f13558e2d59a906b372f64f7145093cc7b71e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c3f7ee5-af53-4668-9ff7-cf2cbbc7e7ed", "node_type": "1", "metadata": {}, "hash": "c66aa4ac6da196ba5bf3ba34c7dfce41c7b4f08d29bb66352afce1d46a203d30", "class_name": "RelatedNodeInfo"}}, "text": "The visualizations are implemented as Polymer web components, backed by Typescript code and can be easily embedded into Jupyter notebooks or webpages. Overview gives a high-level view of one or more data sets. It produces a visual feature-by-feature statistical analysis, and can also be used to compare statistics across two or more data sets. The tool can process both numeric and string features, including multiple instances of a number or string per feature. Dive is a tool for interactively exploring up to tens of thousands of multidimensional data points, allowing users to seamlessly switch between a high-level overview and low-level details. Details about Overview usage can be found in its README. Details about Dive usage can be found in its README. Using Facets in Google Colabratory and Jupyter notebooks can be seen in this notebook. Both Facets visualizations make use of HTML imports. So in order to use them, you must first load the appropriate polyfill, through `<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>`, as shown in the demo notebooks in this repo.", "start_char_idx": 214, "end_char_idx": 1350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c3f7ee5-af53-4668-9ff7-cf2cbbc7e7ed": {"__data__": {"id_": "7c3f7ee5-af53-4668-9ff7-cf2cbbc7e7ed", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63ad6d6e-f431-4dd2-834c-381c4698680c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1a12668fe53308b69d53e029af1483824aba4eee1781583db2a61114db0a1c28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb507157-ddc1-43b6-b3c5-db02cc20adfd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f8212963f2175e2be12326b5018d9864a69c6b04860ef61f2a1d4fd56ef929f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9074351a-a1d1-41be-bb1a-579f59bf5e81", "node_type": "1", "metadata": {}, "hash": "ce70da51a1b79ff03579d338097f7604154b96b3b6c1d04421c6dbb0a9d62c20", "class_name": "RelatedNodeInfo"}}, "text": "It produces a visual feature-by-feature statistical analysis, and can also be used to compare statistics across two or more data sets. The tool can process both numeric and string features, including multiple instances of a number or string per feature. Dive is a tool for interactively exploring up to tens of thousands of multidimensional data points, allowing users to seamlessly switch between a high-level overview and low-level details. Details about Overview usage can be found in its README. Details about Dive usage can be found in its README. Using Facets in Google Colabratory and Jupyter notebooks can be seen in this notebook. Both Facets visualizations make use of HTML imports. So in order to use them, you must first load the appropriate polyfill, through `<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>`, as shown in the demo notebooks in this repo. Note that for using Facets Overview in a Jupyter notebook, there are two considerations.", "start_char_idx": 424, "end_char_idx": 1439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9074351a-a1d1-41be-bb1a-579f59bf5e81": {"__data__": {"id_": "9074351a-a1d1-41be-bb1a-579f59bf5e81", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63ad6d6e-f431-4dd2-834c-381c4698680c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1a12668fe53308b69d53e029af1483824aba4eee1781583db2a61114db0a1c28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c3f7ee5-af53-4668-9ff7-cf2cbbc7e7ed", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "86548cb8ebc1add610095137f853432a15abad6f48419cc65a8d93e3b09e18e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad2adaaa-9363-4636-8d8e-3f32488e03b2", "node_type": "1", "metadata": {}, "hash": "c6b0b3cdbf6f52b02705ed7a2a33c5c013af2092703075fcdd34c533fb5a689e", "class_name": "RelatedNodeInfo"}}, "text": "The tool can process both numeric and string features, including multiple instances of a number or string per feature. Dive is a tool for interactively exploring up to tens of thousands of multidimensional data points, allowing users to seamlessly switch between a high-level overview and low-level details. Details about Overview usage can be found in its README. Details about Dive usage can be found in its README. Using Facets in Google Colabratory and Jupyter notebooks can be seen in this notebook. Both Facets visualizations make use of HTML imports. So in order to use them, you must first load the appropriate polyfill, through `<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>`, as shown in the demo notebooks in this repo. Note that for using Facets Overview in a Jupyter notebook, there are two considerations. The Facets visualizations currently work only in Chrome - Issue 9.", "start_char_idx": 559, "end_char_idx": 1506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad2adaaa-9363-4636-8d8e-3f32488e03b2": {"__data__": {"id_": "ad2adaaa-9363-4636-8d8e-3f32488e03b2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\290.txt", "file_name": "290.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7b811d0-431d-4178-aaf5-69971b5cef81", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\290.txt", "file_name": "290.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dd309516bd8b90874087cadaefa58b342151c56f387fe70490827dee64e5ea7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9074351a-a1d1-41be-bb1a-579f59bf5e81", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "49c3342c583c84f253d78054d958abaeccf72947e0eb36634c07674d7bc12140", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c4db018-701d-4b13-9c02-d4f26681f132", "node_type": "1", "metadata": {}, "hash": "ce82a773f3a217fd132240562aadb8b1cea9f17e58938e5bada7ef837856baef", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: fairness-comparison\r\nlink: https://github.com/algofairness/fairness-comparison\r\ndescription: The repository is meant to facilitate the benchmarking of fairness aware machine learning algorithms. The associated paper is a comparative study of fairness-enhancing interventions in machine learning by Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P. Hamilton, and Derek Roth. To install this software, run: `$ pip3 install fairness`.", "start_char_idx": 0, "end_char_idx": 485, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c4db018-701d-4b13-9c02-d4f26681f132": {"__data__": {"id_": "0c4db018-701d-4b13-9c02-d4f26681f132", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\291.txt", "file_name": "291.txt", "file_type": "text/plain", "file_size": 841, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98b7a51e-d10e-4478-bcf1-11a2c20fb61b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\291.txt", "file_name": "291.txt", "file_type": "text/plain", "file_size": 841, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "98a4b3b5d824e9bcc4b5e5deeb1e8f7554be3daf662176dc6d345fbec806f3cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad2adaaa-9363-4636-8d8e-3f32488e03b2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\290.txt", "file_name": "290.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b2ac39a61d413071a669e2647784acb977e4abcddcb85dfa410558533473ff7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d7522ab-208b-4313-af5c-7bb15e9d2242", "node_type": "1", "metadata": {}, "hash": "9352fbd657bcb42b71f8c396ee87805afb615aba2c1e66301070f32466fd57a5", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: GEBI\r\nlink: https://github.com/AgaMiko/GEBI\r\ndescription: The Global Explanations for Bias Identification (GEBI) is a proposed attention-based post-hoc explanation framework that allows for the detection and identification of bias in data. The framework involves several steps for data analysis, including the computation of attention maps for a particular class of samples, normalization and preprocessing of input samples and attention maps, reduction of dimensionality, and spectral clustering. Additionally, GEBI provides a bias insertion algorithm that evaluates the presence of unwanted bias in the dataset. The paper validates the framework on a skin lesion dataset and confirms the influence of black frames on deep learning models, showing that bias detection is an important step in improving the robustness of models.", "start_char_idx": 0, "end_char_idx": 839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d7522ab-208b-4313-af5c-7bb15e9d2242": {"__data__": {"id_": "6d7522ab-208b-4313-af5c-7bb15e9d2242", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\292.txt", "file_name": "292.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad0d3a70-5a2a-4f0a-8714-24ea680cd55d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\292.txt", "file_name": "292.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "95979698ff9d6e1117d216e86f290d1fb528e044d243cb6085a9931b43749984", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c4db018-701d-4b13-9c02-d4f26681f132", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\291.txt", "file_name": "291.txt", "file_type": "text/plain", "file_size": 841, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8aa64aea3b1be53cadc8b554b451931b0ac194f3ec4f0c38b619cb9ea991646a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "861f1f8b-77a1-49f7-9af4-1fc2d4f8090e", "node_type": "1", "metadata": {}, "hash": "4a0fd7fadf60834b9b9fd5dd84bd4095be0e7b91f77ce7baa9319c827c4cde65", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: fairml\r\nlink: https://github.com/adebayoj/fairml\r\ndescription: FairML is a python toolbox for auditing black-box predictive models, specifically for quantifying the relative significance of the model\u2019s inputs in order to assess the model\u2019s fairness (or discriminatory extent). This project addresses the potential for unintentional discrimination when deploying predictive models to determine access to services such as credit, insurance, and employment. The toolbox leverages model compression and four input ranking algorithms to quantify a model\u2019s relative predictive dependence on its inputs, making it easier for analysts to audit cumbersome predictive models that are difficult to interpret. The FairML package can be installed via pip, or by cloning the repository. An example model is provided for demonstration purposes.", "start_char_idx": 0, "end_char_idx": 840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "861f1f8b-77a1-49f7-9af4-1fc2d4f8090e": {"__data__": {"id_": "861f1f8b-77a1-49f7-9af4-1fc2d4f8090e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\293.txt", "file_name": "293.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "228b53cf-a254-4aec-87a0-a7d8de7477a1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\293.txt", "file_name": "293.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fdb941dac91520ccee66da8d36a0d5f5d544090245d252bd542dd7cf98823c16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d7522ab-208b-4313-af5c-7bb15e9d2242", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\292.txt", "file_name": "292.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0daaf4048cd88dce96b1bf1c0d0a87c21b6f2bed808f0acab125b80f12038c5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "babcb2dc-0fcb-4374-a043-58409b0ca870", "node_type": "1", "metadata": {}, "hash": "8fb55e070c31e22bc9d877b826b5f3e56989aec7f710dc7053677248c492a370", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: eli5\r\nlink: https://github.com/TeamHG-Memex/eli5\r\ndescription:", "start_char_idx": 0, "end_char_idx": 73, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "babcb2dc-0fcb-4374-a043-58409b0ca870": {"__data__": {"id_": "babcb2dc-0fcb-4374-a043-58409b0ca870", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2461ef18-ddee-46ea-8a3c-09b960d4b6b4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f4484c7cb872959fe6a70666595c0b0fc33129e477acaa3b4d3d4951722cf097", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "861f1f8b-77a1-49f7-9af4-1fc2d4f8090e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\293.txt", "file_name": "293.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "18e92bc31a255f506e82d22db0488b6fbc05c50c99d70e1797c89fcbed6a6674", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fbd0573-143a-4e00-b824-8cb012b46335", "node_type": "1", "metadata": {}, "hash": "758a8660084acaf880f64b75f027d2c843fce2cc7adff01556bd9cdd97d91e85", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: deep-visualization-toolbox\r\nlink: https://github.com/yosinski/deep-visualization-toolbox\r\ndescription: The Deep Visualization Toolbox is a tool that allows for the generation of neuron-by-neuron visualizations using regularized optimization, for which the code is provided in this repository. It offers two main sets of features: running images forward and backward through the network to visualize activations and compute derivatives of any unit with respect to any other unit, and computing three types of per-unit visualizations (max image, deconv of max image, and activation maximization via regularized optimization) that are computed outside of the toolbox and loaded into it as jpgs. The toolbox comes with the default caffenet-yos model weights and pre-computed visualizations, but it also allows for visualization of custom models. To install the toolbox, users need to compile the deconv-deep-vis-toolbox branch of caffe and install prerequisites (python-opencv, scipy, and scikit-image), as well as download and configure the Deep Visualization Toolbox code.", "start_char_idx": 0, "end_char_idx": 1081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fbd0573-143a-4e00-b824-8cb012b46335": {"__data__": {"id_": "2fbd0573-143a-4e00-b824-8cb012b46335", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2461ef18-ddee-46ea-8a3c-09b960d4b6b4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f4484c7cb872959fe6a70666595c0b0fc33129e477acaa3b4d3d4951722cf097", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "babcb2dc-0fcb-4374-a043-58409b0ca870", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "58e74cc95b665685e70b724289b2571732c635434052993e5f8e57ef1819e171", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b5660a1-c47f-4e39-9201-ca9c195b791a", "node_type": "1", "metadata": {}, "hash": "120f8dee4d3db827640241d64ec089a99268e1c3063b2347d23dc30a14ad32ea", "class_name": "RelatedNodeInfo"}}, "text": "It offers two main sets of features: running images forward and backward through the network to visualize activations and compute derivatives of any unit with respect to any other unit, and computing three types of per-unit visualizations (max image, deconv of max image, and activation maximization via regularized optimization) that are computed outside of the toolbox and loaded into it as jpgs. The toolbox comes with the default caffenet-yos model weights and pre-computed visualizations, but it also allows for visualization of custom models. To install the toolbox, users need to compile the deconv-deep-vis-toolbox branch of caffe and install prerequisites (python-opencv, scipy, and scikit-image), as well as download and configure the Deep Visualization Toolbox code. Troubleshooting tips and alternative methods of running the toolbox are also provided.", "start_char_idx": 304, "end_char_idx": 1168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b5660a1-c47f-4e39-9201-ca9c195b791a": {"__data__": {"id_": "9b5660a1-c47f-4e39-9201-ca9c195b791a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\295.txt", "file_name": "295.txt", "file_type": "text/plain", "file_size": 82, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0075cbf1-8468-4518-b3fe-a27aeb62c3db", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\295.txt", "file_name": "295.txt", "file_type": "text/plain", "file_size": 82, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "18ed227899eb273c7bcf54baabb7ae977e82edc4698584ea78859b641c1268d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fbd0573-143a-4e00-b824-8cb012b46335", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c3617bcd470b7c4fd39ce22348db65173669982f8e6ee923357b3b07183cf552", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00a6d016-18e1-48c5-8e20-df850088a030", "node_type": "1", "metadata": {}, "hash": "41270f4c01de2efb219898dbade8cd09b26f9d65a5bff4d1b09bef66a5c573dd", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: deeplift\r\nlink: https://github.com/kundajelab/deeplift\r\ndescription:", "start_char_idx": 0, "end_char_idx": 79, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00a6d016-18e1-48c5-8e20-df850088a030": {"__data__": {"id_": "00a6d016-18e1-48c5-8e20-df850088a030", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\296.txt", "file_name": "296.txt", "file_type": "text/plain", "file_size": 468, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bd5f3e2-0a5c-49f8-9a8b-67b5da7d4e2c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\296.txt", "file_name": "296.txt", "file_type": "text/plain", "file_size": 468, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d04a8df338d153304bb463f8cc741e17916e9298a688a9da18de4f24216aa8be", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b5660a1-c47f-4e39-9201-ca9c195b791a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\295.txt", "file_name": "295.txt", "file_type": "text/plain", "file_size": 82, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7f0caece8061dc219ce516e18f06521a191ae967851a810d182dd79f18cd7ccd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe6f6bac-9d08-4d81-9090-b7f63c989442", "node_type": "1", "metadata": {}, "hash": "bee7c67ca27d598fa3f78e43ca563c2e43601946c0387df06c5e577e3748a7f7", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: casme\r\nlink: https://github.com/kondiz/casme\r\ndescription: This repository contains the code originally forked from the ImageNet training in PyTorch that is modified to present the performance of classifier-agnostic saliency map extraction, a practical algorithm to train a classifier-agnostic saliency mapping by simultaneously training a classifier and a saliency mapping. The method was proposed by Konrad \u017bo\u0142na, Krzysztof J. Geras and Kyunghyun Cho.", "start_char_idx": 0, "end_char_idx": 464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe6f6bac-9d08-4d81-9090-b7f63c989442": {"__data__": {"id_": "fe6f6bac-9d08-4d81-9090-b7f63c989442", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\297.txt", "file_name": "297.txt", "file_type": "text/plain", "file_size": 525, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "df301cba-68c1-432b-8dd8-bb0de0bd1872", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\297.txt", "file_name": "297.txt", "file_type": "text/plain", "file_size": 525, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "57c002346ff97d99a1c94fae4201e3613ad39b245bf232014b26e58d2bfe89b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00a6d016-18e1-48c5-8e20-df850088a030", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\296.txt", "file_name": "296.txt", "file_type": "text/plain", "file_size": 468, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ab0c9df1a78964962e907f4e1ca6d04713cdd5187a812171d53f4e8a4d13e3c9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70bef62f-fc34-413d-9105-5f9204a08c25", "node_type": "1", "metadata": {}, "hash": "19f7491ae649b4e39e0e3d06c246fd8ed145ec9f0639a325b05fb9e290307039", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ContrastiveExplanation\r\nlink: https://github.com/MarcelRobeer/ContrastiveExplanation\r\ndescription: Contrastive Explanation (Foil Trees) provides an explanation for why an instance had the current outcome rather than a targeted outcome of interest. The idea of contrastive explanations is captured in this Python package `ContrastiveExplanation`. One scientific paper was published on Contrastive Explanation / Foil Trees. Several choices can be made to tailor the explanation to your type of explanation problem.", "start_char_idx": 0, "end_char_idx": 523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70bef62f-fc34-413d-9105-5f9204a08c25": {"__data__": {"id_": "70bef62f-fc34-413d-9105-5f9204a08c25", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\298.txt", "file_name": "298.txt", "file_type": "text/plain", "file_size": 1118, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff31062d-2f43-423a-bf02-d48212ee969b", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\298.txt", "file_name": "298.txt", "file_type": "text/plain", "file_size": 1118, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8dcd32ce0ea1f9660a7a12e1a8579a04ed718adb7fca4717130cb83527a5c9b0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe6f6bac-9d08-4d81-9090-b7f63c989442", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\297.txt", "file_name": "297.txt", "file_type": "text/plain", "file_size": 525, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e4bfe66baa5e64a16b9580427a13c5a9c8e8a73976ae119630179030f700f1c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8a9e263-e33e-4ccf-bf0a-f7df175e5b84", "node_type": "1", "metadata": {}, "hash": "a4cefdaf2d295314d8afcb4b939965bc83c5894740a40742852d55471158276d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: captum\r\nlink: https://github.com/pytorch/captum\r\ndescription: Captum is a model interpretability and understanding library for PyTorch. It provides state-of-the-art algorithms, including Integrated Gradients, to provide researchers and developers with an easy way to understand which features are contributing to a model\u2019s output. Captum helps ML researchers more easily implement interpretability algorithms that can interact with PyTorch models. Captum also allows researchers to quickly benchmark their work against other existing algorithms available in the library. The primary audiences for Captum are model developers who are looking to improve their models and interpretability researchers focused on identifying algorithms that can better interpret many types of models. Captum can also be used by application engineers who are using trained models in production. Captum provides easier troubleshooting through improved model interpretability, and the potential for delivering better explanations to end users on why they\u2019re seeing a specific piece of content, such as a movie recommendation.", "start_char_idx": 0, "end_char_idx": 1112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8a9e263-e33e-4ccf-bf0a-f7df175e5b84": {"__data__": {"id_": "a8a9e263-e33e-4ccf-bf0a-f7df175e5b84", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "419d90bf6532c1a8ab759a14e90dbb716b5f3011ee56888a3fed11f2d824dfe7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70bef62f-fc34-413d-9105-5f9204a08c25", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\298.txt", "file_name": "298.txt", "file_type": "text/plain", "file_size": 1118, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c012ffb0b7d2e8b6b3eaafb9b8328bf9a5e2fa08b24adb6a638084a95987b517", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd84b1cd-4ce6-4bc5-b9f6-c8cbafddb0ac", "node_type": "1", "metadata": {}, "hash": "b73a845caeb8bd296539e01174f79bf04668b1528dfd549e2e3d800d2b3b0152", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: anchor\r\nlink: https://github.com/marcotcr/anchor\r\ndescription: Anchor Installation Examples Citation: \r\n\r\nThe Anchor package is on pypi. Simply run 'pip install anchor-exp' or clone the repository and run 'python setup.py install'. If you want to use 'AnchorTextExplainer', you have to run 'python -m spacy download en_core_web_lg'. And if you want to use BERT to perturb inputs (recommended), also install 'transformers' with 'pip install torch transformers spacy && python -m spacy download en_core_web_sm'. See notebooks folder for tutorials. Note that from version 0.0.1.0, it only works on python 3. An anchor explanation is a rule that sufficiently \u201canchors\u201d the prediction locally - such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same.", "start_char_idx": 0, "end_char_idx": 884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd84b1cd-4ce6-4bc5-b9f6-c8cbafddb0ac": {"__data__": {"id_": "bd84b1cd-4ce6-4bc5-b9f6-c8cbafddb0ac", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "419d90bf6532c1a8ab759a14e90dbb716b5f3011ee56888a3fed11f2d824dfe7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8a9e263-e33e-4ccf-bf0a-f7df175e5b84", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f3eced5deac0e7d708f30cbd89c0bffeae3962d2808f4834b91ab168939f03cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbd78def-1a86-4c3b-b87f-8059aec7b3a9", "node_type": "1", "metadata": {}, "hash": "d36f553e3a3096925b2db3289813e9b89461413a60dc4509a997e295de52cfbd", "class_name": "RelatedNodeInfo"}}, "text": "Simply run 'pip install anchor-exp' or clone the repository and run 'python setup.py install'. If you want to use 'AnchorTextExplainer', you have to run 'python -m spacy download en_core_web_lg'. And if you want to use BERT to perturb inputs (recommended), also install 'transformers' with 'pip install torch transformers spacy && python -m spacy download en_core_web_sm'. See notebooks folder for tutorials. Note that from version 0.0.1.0, it only works on python 3. An anchor explanation is a rule that sufficiently \u201canchors\u201d the prediction locally - such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same. The Anchor method is able to explain any black box classifier, with two or more classes.", "start_char_idx": 148, "end_char_idx": 973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbd78def-1a86-4c3b-b87f-8059aec7b3a9": {"__data__": {"id_": "fbd78def-1a86-4c3b-b87f-8059aec7b3a9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "419d90bf6532c1a8ab759a14e90dbb716b5f3011ee56888a3fed11f2d824dfe7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd84b1cd-4ce6-4bc5-b9f6-c8cbafddb0ac", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "82d08b3efee9a20a2a976b921b1d8c7b51a0b6efe42ab61cf6edf6d40fde691b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbc1ff14-3215-4567-8d4c-53fd3947c25a", "node_type": "1", "metadata": {}, "hash": "f1cb0bc4d8053b94981c6227354a95113956d83d9a59f4a8f1642f59f6b498fb", "class_name": "RelatedNodeInfo"}}, "text": "Simply run 'pip install anchor-exp' or clone the repository and run 'python setup.py install'. If you want to use 'AnchorTextExplainer', you have to run 'python -m spacy download en_core_web_lg'. And if you want to use BERT to perturb inputs (recommended), also install 'transformers' with 'pip install torch transformers spacy && python -m spacy download en_core_web_sm'. See notebooks folder for tutorials. Note that from version 0.0.1.0, it only works on python 3. An anchor explanation is a rule that sufficiently \u201canchors\u201d the prediction locally - such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same. The Anchor method is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a prediction (integer).", "start_char_idx": 148, "end_char_idx": 1109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbc1ff14-3215-4567-8d4c-53fd3947c25a": {"__data__": {"id_": "cbc1ff14-3215-4567-8d4c-53fd3947c25a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "419d90bf6532c1a8ab759a14e90dbb716b5f3011ee56888a3fed11f2d824dfe7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbd78def-1a86-4c3b-b87f-8059aec7b3a9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5c1b16f6209b4f2ce8b46a3378b82c7c43cf82fe84994ff0b5ead2852d36b678", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99291768-b043-47c2-ad4f-88a454c82fd2", "node_type": "1", "metadata": {}, "hash": "39ec4a42513d7a6c06a0e98b9caedb0bd4d5540622638b75ad2a2c438f26e06e", "class_name": "RelatedNodeInfo"}}, "text": "And if you want to use BERT to perturb inputs (recommended), also install 'transformers' with 'pip install torch transformers spacy && python -m spacy download en_core_web_sm'. See notebooks folder for tutorials. Note that from version 0.0.1.0, it only works on python 3. An anchor explanation is a rule that sufficiently \u201canchors\u201d the prediction locally - such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same. The Anchor method is able to explain any black box classifier, with two or more classes. All we require is that the classifier implements a function that takes in raw text or a numpy array and outputs a prediction (integer). Here is the bibtex if you want to cite this work.", "start_char_idx": 344, "end_char_idx": 1159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99291768-b043-47c2-ad4f-88a454c82fd2": {"__data__": {"id_": "99291768-b043-47c2-ad4f-88a454c82fd2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd0d0fcc-2b2e-4b73-b503-deb56f81463e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3f0353fa866c6b6ba3dcfb512881313061a4b488f207b21e84fb8149bd18a25c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbc1ff14-3215-4567-8d4c-53fd3947c25a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4b553f4540328c7b122f31449c9edc06a8af89fa9cbe570ad37391a31af53195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d88b889e-113b-46a5-b08c-30fb90880dcc", "node_type": "1", "metadata": {}, "hash": "0a5fadc0de407167a14777b72c1d177ebd268424549c8c088c5d6adf4092a914", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: gensim\r\nlink: https://github.com/RaRe-Technologies/gensim\r\ndescription: 'Gensim is a Python library for _topic modelling_ , _document indexing_ and._similarity retrieval_ with large corpora. Target audience is the _natural.language processing_ (NLP) and _information retrieval_ (IR) community. If this feature list left you scratching your head, you can first read more.about the Vector Space Model and unsupervised document analysis on Wikipedia. Many scientific algorithms can be expressed in terms of large matrix.operations (see the BLAS note above). Gensim taps into these low-level BLAS.libraries, by means of its dependency on NumPy. So while gensim-the-top-level-.code is pure Python, it actually executes highly optimized Fortran/C under the.hood, including multithreading (if your BLAS is so configured). Memory-wise, gensim makes heavy use of Python\u2019s built-in generators and.iterators for streamed data processing.", "start_char_idx": 0, "end_char_idx": 937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d88b889e-113b-46a5-b08c-30fb90880dcc": {"__data__": {"id_": "d88b889e-113b-46a5-b08c-30fb90880dcc", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd0d0fcc-2b2e-4b73-b503-deb56f81463e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3f0353fa866c6b6ba3dcfb512881313061a4b488f207b21e84fb8149bd18a25c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99291768-b043-47c2-ad4f-88a454c82fd2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "48cc196bcc4136f014f9515791315cee50b2ad7008a275b4ab2ba1709351786d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee148675-b135-4828-929e-bf31b2f79e5d", "node_type": "1", "metadata": {}, "hash": "81a4b2a2e8a269f6083733fc863d35d47a3a7da1cc408010e32ea728644b045e", "class_name": "RelatedNodeInfo"}}, "text": "Target audience is the _natural.language processing_ (NLP) and _information retrieval_ (IR) community. If this feature list left you scratching your head, you can first read more.about the Vector Space Model and unsupervised document analysis on Wikipedia. Many scientific algorithms can be expressed in terms of large matrix.operations (see the BLAS note above). Gensim taps into these low-level BLAS.libraries, by means of its dependency on NumPy. So while gensim-the-top-level-.code is pure Python, it actually executes highly optimized Fortran/C under the.hood, including multithreading (if your BLAS is so configured). Memory-wise, gensim makes heavy use of Python\u2019s built-in generators and.iterators for streamed data processing. Memory efficiency was one of gensim\u2019s.design goals, and is a central feature of gensim, rather than something bolted.on as an afterthought. For commercial support, please see Gensim sponsorship.'", "start_char_idx": 202, "end_char_idx": 1133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee148675-b135-4828-929e-bf31b2f79e5d": {"__data__": {"id_": "ee148675-b135-4828-929e-bf31b2f79e5d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\30.txt", "file_name": "30.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63428642-3509-467a-ba84-47ea8d8e92ad", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\30.txt", "file_name": "30.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6e07dfec5bd5315fd62038068d3427253c1322242e46f8738464f1e615874886", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d88b889e-113b-46a5-b08c-30fb90880dcc", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8aaa4d91c7315b4106409572942136734c8f37f15daf92c11a3afb931b530105", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb31603c-5f65-4ed5-aa47-f9e969bf9ef6", "node_type": "1", "metadata": {}, "hash": "8a1e4f206b68c7f967ed546a80afae8c41edf13b206951c2a61e8d8b5de03156", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: what-if-tool\r\nlink: https://github.com/PAIR-code/what-if-tool\r\ndescription: The What-If Tool (WIT) provides an easy-to-use interface for expanding understanding of a black-box classification or regression ML model. With the plugin, you can perform inference on a large set of examples and immediately visualize the results in a variety of ways. Additionally, examples can be edited manually or programmatically and re-run through the model in order to see the results of the changes. It contains tooling for investigating model performance and fairness over subsets of a dataset. The purpose of the tool is to give people a simple, intuitive, and powerful way to play with a trained ML model on a set of data through a visual interface with absolutely no code required. The tool can be accessed through TensorBoard or as an extension in a Jupyter or Colab notebook.", "start_char_idx": 0, "end_char_idx": 876, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb31603c-5f65-4ed5-aa47-f9e969bf9ef6": {"__data__": {"id_": "bb31603c-5f65-4ed5-aa47-f9e969bf9ef6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6313a2c7-5be6-422e-a676-771d1ba19f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee148675-b135-4828-929e-bf31b2f79e5d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\30.txt", "file_name": "30.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "385176b6cdcb7a202f1937b14dd7b7d4ecb93222d4d2527c52e37628d188d342", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a600ce7d-addd-471c-b829-f99b0c11c8e2", "node_type": "1", "metadata": {}, "hash": "ffe226070d48804272027d49013e05ca0d30611fb386ddcadb0b08728d7e6813", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: alibi\r\nlink: https://github.com/SeldonIO/alibi\r\ndescription: Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models. If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project alibi-detect. Alibi can be installed from: To install from conda-forge it is recommended to use mamba, which can be installed to the _base_ conda environment with: The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize, fit and explain steps. We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`.", "start_char_idx": 0, "end_char_idx": 903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a600ce7d-addd-471c-b829-f99b0c11c8e2": {"__data__": {"id_": "a600ce7d-addd-471c-b829-f99b0c11c8e2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6313a2c7-5be6-422e-a676-771d1ba19f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb31603c-5f65-4ed5-aa47-f9e969bf9ef6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f740fe9fcee19f54a4c7c1b3ebeafb44d04211d3d5973ff09cc844f475a887f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "099098be-9ab2-45ce-938c-0f958a7ee92c", "node_type": "1", "metadata": {}, "hash": "da17615c15837de165448537061b880162591418b4c2ac235c999d602bc8ef1b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: alibi\r\nlink: https://github.com/SeldonIO/alibi\r\ndescription: Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models. If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project alibi-detect. Alibi can be installed from: To install from conda-forge it is recommended to use mamba, which can be installed to the _base_ conda environment with: The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize, fit and explain steps. We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything related to the computed explanation.", "start_char_idx": 0, "end_char_idx": 1070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "099098be-9ab2-45ce-938c-0f958a7ee92c": {"__data__": {"id_": "099098be-9ab2-45ce-938c-0f958a7ee92c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6313a2c7-5be6-422e-a676-771d1ba19f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a600ce7d-addd-471c-b829-f99b0c11c8e2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2859de60a00e691649bcfde4d7e4847b20af7c32afc72c0030bf8c386f94836d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "740189d2-e803-49a7-b6d5-8b8d294da86b", "node_type": "1", "metadata": {}, "hash": "6422ce929fe6552ec66d32c0fbad6d8aa585dca5accad43bd92d796d26f59e36", "class_name": "RelatedNodeInfo"}}, "text": "The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models. If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project alibi-detect. Alibi can be installed from: To install from conda-forge it is recommended to use mamba, which can be installed to the _base_ conda environment with: The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize, fit and explain steps. We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed via `explanation.data['anchor']` (or `explanation.anchor`).", "start_char_idx": 174, "end_char_idx": 1200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "740189d2-e803-49a7-b6d5-8b8d294da86b": {"__data__": {"id_": "740189d2-e803-49a7-b6d5-8b8d294da86b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6313a2c7-5be6-422e-a676-771d1ba19f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "099098be-9ab2-45ce-938c-0f958a7ee92c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0a8d0604558c5d810c5f110bff1c89d408b6acbba4efdc746ea5e9435d9c6eff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c138bc0f-ab83-4dc8-9696-71cd77552027", "node_type": "1", "metadata": {}, "hash": "14009692c5bdb3da607eb35467c00dd8cdbb4e0cf3ad05452f41ab0492286a89", "class_name": "RelatedNodeInfo"}}, "text": "If you're interested in outlier detection, concept drift or adversarial instance detection, check out our sister project alibi-detect. Alibi can be installed from: To install from conda-forge it is recommended to use mamba, which can be installed to the _base_ conda environment with: The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize, fit and explain steps. We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed via `explanation.data['anchor']` (or `explanation.anchor`). The exact details of available fields varies from method to method so we encourage the reader to become familiar with the types of methods supported.", "start_char_idx": 346, "end_char_idx": 1350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c138bc0f-ab83-4dc8-9696-71cd77552027": {"__data__": {"id_": "c138bc0f-ab83-4dc8-9696-71cd77552027", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6313a2c7-5be6-422e-a676-771d1ba19f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "740189d2-e803-49a7-b6d5-8b8d294da86b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "40e38d6ddf6b856a6c236752209a9b8835d3400ccba8f29e6a992c3b8859d1c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9549336e-1d13-44b3-9425-b6211314e0ed", "node_type": "1", "metadata": {}, "hash": "a070175b10182b8c725d0daf49db66d5b6f51d0fe5211f9d393996406c65fc8d", "class_name": "RelatedNodeInfo"}}, "text": "Alibi can be installed from: To install from conda-forge it is recommended to use mamba, which can be installed to the _base_ conda environment with: The alibi explanation API takes inspiration from `scikit-learn`, consisting of distinct initialize, fit and explain steps. We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed via `explanation.data['anchor']` (or `explanation.anchor`). The exact details of available fields varies from method to method so we encourage the reader to become familiar with the types of methods supported. The following tables summarize the possible use cases for each method. These algorithms provide **instance-specific** scores measuring the model confidence for making a particular prediction.", "start_char_idx": 481, "end_char_idx": 1542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9549336e-1d13-44b3-9425-b6211314e0ed": {"__data__": {"id_": "9549336e-1d13-44b3-9425-b6211314e0ed", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6313a2c7-5be6-422e-a676-771d1ba19f96", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c71296d51cd0d005841eb80b0bcbd80758f44127083d32f67cd74fc12463820e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c138bc0f-ab83-4dc8-9696-71cd77552027", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1e8e55a5f119c50b95dacbf72977523c2a4fec40a771723df27cd42fc514a162", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc912460-0b4d-4998-8ab3-8b241449f455", "node_type": "1", "metadata": {}, "hash": "3cd398f9bcc511f76dc37e4326c7d1fd8ef530c5541545e481f666f4d6d365b7", "class_name": "RelatedNodeInfo"}}, "text": "We will use the AnchorTabular explainer to illustrate the API: The explanation returned is an `Explanation` object with attributes `meta` and `data`. `meta` is a dictionary containing the explainer metadata and any hyperparameters and `data` is a dictionary containing everything related to the computed explanation. For example, for the Anchor algorithm the explanation can be accessed via `explanation.data['anchor']` (or `explanation.anchor`). The exact details of available fields varies from method to method so we encourage the reader to become familiar with the types of methods supported. The following tables summarize the possible use cases for each method. These algorithms provide **instance-specific** scores measuring the model confidence for making a particular prediction. These algorithms provide a **distilled** view of the dataset and help construct a 1-KNN **interpretable** classifier. * Accumulated Local Effects (ALE, Apley and Zhu, 2016) If you use alibi in your research, please consider citing it.", "start_char_idx": 754, "end_char_idx": 1777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc912460-0b4d-4998-8ab3-8b241449f455": {"__data__": {"id_": "bc912460-0b4d-4998-8ab3-8b241449f455", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\301.txt", "file_name": "301.txt", "file_type": "text/plain", "file_size": 348, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "228687d9-185d-42d4-bf1b-6a0ebf05ed48", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\301.txt", "file_name": "301.txt", "file_type": "text/plain", "file_size": 348, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "302a69300e8a4b5fb2f1b0f7b664823e1a45c1effd1d81cee35d7b3f7514907a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9549336e-1d13-44b3-9425-b6211314e0ed", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "53625fb3b0ce2cd1be9bb015af33c128ff1b867b9a6981ad30d4999f151e9c48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9269b9de-9c68-497d-ba6a-bb26a2269dfe", "node_type": "1", "metadata": {}, "hash": "413b0f2bcbb1d6b4d345c58d540fb6cb2e79b37b9e3d577703ffcec3e56d4a45", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: aequitas\r\nlink: https://github.com/dssg/aequitas\r\ndescription: 'Aequitas is an open-source bias audit toolkit for data scientists, machine.learning researchers, and policymakers to audit machine learning models for.discrimination and bias, and to make informed and equitable decisions around.developing and deploying predictive tools.'", "start_char_idx": 0, "end_char_idx": 346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9269b9de-9c68-497d-ba6a-bb26a2269dfe": {"__data__": {"id_": "9269b9de-9c68-497d-ba6a-bb26a2269dfe", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\31.txt", "file_name": "31.txt", "file_type": "text/plain", "file_size": 466, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d41022e-a1c8-48c4-8925-cd58306a3947", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\31.txt", "file_name": "31.txt", "file_type": "text/plain", "file_size": 466, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d4399d9a13d44ba5cbfdc035b6d251c735a1c0189765ec9719bf1d9195591d49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc912460-0b4d-4998-8ab3-8b241449f455", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\301.txt", "file_name": "301.txt", "file_type": "text/plain", "file_size": 348, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9c0d6cb15471af98ad0f4031d1120ea6c3827f01e6dc2d92ddaa6b10f2019e95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa7af594-4220-4ce8-9c84-a5c46cb526d7", "node_type": "1", "metadata": {}, "hash": "3d4498b70f3395b1dedc4265b157e401c4098801a127d5eaa31f3a6f6bc24a40", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: interpret\r\nlink: https://github.com/interpretml/interpret\r\ndescription: InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions. Interpretability is essential for:", "start_char_idx": 0, "end_char_idx": 464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa7af594-4220-4ce8-9c84-a5c46cb526d7": {"__data__": {"id_": "aa7af594-4220-4ce8-9c84-a5c46cb526d7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\32.txt", "file_name": "32.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e6772e8-479e-497b-83ce-b5728aafc463", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\32.txt", "file_name": "32.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c1973206d69f5b3922217c3d557c27b4309ea60ed844db827a7e6541d9213a59", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9269b9de-9c68-497d-ba6a-bb26a2269dfe", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\31.txt", "file_name": "31.txt", "file_type": "text/plain", "file_size": 466, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c2f031d78a0453d07c4aab0d3edcd4111b8bc66434c704d0416a4567ea4bf720", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1086f5af-2027-46dc-9976-a84b4b4c8f43", "node_type": "1", "metadata": {}, "hash": "c0ac6d44504f3edfd42a7e17917f1291fcace9ed79c3474956a89c1cd70e60f6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: fairlearn\r\nlink: https://github.com/fairlearn/fairlearn\r\ndescription:", "start_char_idx": 0, "end_char_idx": 80, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1086f5af-2027-46dc-9976-a84b4b4c8f43": {"__data__": {"id_": "1086f5af-2027-46dc-9976-a84b4b4c8f43", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\33.txt", "file_name": "33.txt", "file_type": "text/plain", "file_size": 547, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2ef52d7-8f67-44d8-be0b-1539687c0ef2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\33.txt", "file_name": "33.txt", "file_type": "text/plain", "file_size": 547, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b233e9a2a99bd36a86411cadcb5868c9176595bc7509512b52ed213d96732d4d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa7af594-4220-4ce8-9c84-a5c46cb526d7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\32.txt", "file_name": "32.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7dc59e3807c7491158e11b26bb82c58dff5183a7f02ba6c8283276fbc9ef7387", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36fa2c74-d7bd-4a0a-8b97-5d088a6c25c6", "node_type": "1", "metadata": {}, "hash": "199b5adcc446e9e62f010e3537b53afb578a1bf73f5e285c8fa17b194d2047ec", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: jActivity2PMML\r\nlink: https://github.com/teco-kit/jActivity2PMML\r\ndescription: This package illustrates how to deploy code to create PMML models for sensor data collected via jActivity. The function \"getPMML(json_file)\" can be run by handing over a JSON file containing \"sensor\", \"label\", and \"classifier\" information. An example of this JSON file is provided. To use this package, install it with devtools by running \"install_github(\"jActivity2PMML\")\". To run the app in OpenCPU, load the library and browse to the corresponding URL.", "start_char_idx": 0, "end_char_idx": 545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36fa2c74-d7bd-4a0a-8b97-5d088a6c25c6": {"__data__": {"id_": "36fa2c74-d7bd-4a0a-8b97-5d088a6c25c6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\34.txt", "file_name": "34.txt", "file_type": "text/plain", "file_size": 935, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f4609be-679b-454d-9662-bf69d41bf921", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\34.txt", "file_name": "34.txt", "file_type": "text/plain", "file_size": 935, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bedfe39f6d67f832fcff8bae6a87a11b77e6d0eb0a4516334cf329a7203954d6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1086f5af-2027-46dc-9976-a84b4b4c8f43", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\33.txt", "file_name": "33.txt", "file_type": "text/plain", "file_size": 547, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "84ffcfc55c8d8aa94c8692f42be2a9f18671e4fda4128c3097d5289306cefdda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "786b47bf-75a5-4fc9-b93e-c51f6919985a", "node_type": "1", "metadata": {}, "hash": "787810cd3cb5bf122f215cd7dc1615a5fd070cb5a6c90b66ff6fd4bd68e3fbce", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: jActivity\r\nlink: https://github.com/teco-kit/jActivity\r\ndescription: jActivity2 is a system designed to collect different sensor data on smartphones to train classifiers for the web. It is implemented as part of the Prosperity4All project funded by the European Union. A web front-end allows web-developers to collect data for a given context with any available sensor. This data is stored in a MySQL database on the server, where it can be retrieved to create classifiers which run inside a website to detect the context. jActivity2 requires Docker and docker-compose. The available functionality includes Decision Trees as PMML models and providing training data. To integrate jActivity into your website, you can run a decision tree classifier and include the PMML model files. Additionally, jActivity is designed to be modular, enabling it to be extended with your own code for collecting data from additional sensors.", "start_char_idx": 0, "end_char_idx": 933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "786b47bf-75a5-4fc9-b93e-c51f6919985a": {"__data__": {"id_": "786b47bf-75a5-4fc9-b93e-c51f6919985a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\35.txt", "file_name": "35.txt", "file_type": "text/plain", "file_size": 490, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4597f15-99cf-435f-9a04-ed4c844c96a3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\35.txt", "file_name": "35.txt", "file_type": "text/plain", "file_size": 490, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "55ccfe3b8abb81a4b4db0925e6190725fbff93890096756e33df1ec9da234a84", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36fa2c74-d7bd-4a0a-8b97-5d088a6c25c6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\34.txt", "file_name": "34.txt", "file_type": "text/plain", "file_size": 935, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4c63b9bf3a3398298fe5c927479d6b897bb7b350c8cfa44e7cc91d26c1612eeb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de632f38-0ca3-4a6f-9b3b-b237ce14ed74", "node_type": "1", "metadata": {}, "hash": "ca014aa4bb0fbb6fae725bf0b6033424b034212ccd8ae57edbc0c127883f01af", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pmml2js\r\nlink: https://github.com/teco-kit/pmml2js\r\ndescription: PMML to Javascript (pmml2js) is a project that focuses on transforming PMML to Javascript code for running in a browser. The decision tree engine was created by artistoex and currently supports decision trees. The project provides usage examples, including binary sorting and the Iris Dataset, as well as an API for generating executable models. Tests can be run by opening ./pmml2js/tests/*.html in any browser.", "start_char_idx": 0, "end_char_idx": 488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de632f38-0ca3-4a6f-9b3b-b237ce14ed74": {"__data__": {"id_": "de632f38-0ca3-4a6f-9b3b-b237ce14ed74", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\36.txt", "file_name": "36.txt", "file_type": "text/plain", "file_size": 499, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fedf7284-8bd7-449d-8583-15c4741e9a40", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\36.txt", "file_name": "36.txt", "file_type": "text/plain", "file_size": 499, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a2935403a1da27ed3c7e7a8fb8acf0a6a69600aa756cadce3ee5763ef92adf6a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "786b47bf-75a5-4fc9-b93e-c51f6919985a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\35.txt", "file_name": "35.txt", "file_type": "text/plain", "file_size": 490, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4ed397ca53bdd49e0ad9baddd766769797269a4e0f02e1bd3c5522c6c7dd746f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08478107-c0d1-437b-8c33-b8d84b1e647f", "node_type": "1", "metadata": {}, "hash": "51cdce25f3e8c0dff015a79b18e83e5821323e33660d0965eee0922b2b64262e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: DecisionTreeRules\r\nlink: https://github.com/teco-kit/DecisionTreeRules\r\ndescription: DecisionTreeRules is a module that enables users to extract and modify the rules from a decision tree. It provides methods to extract rules from a decision tree and to calculate precision and recall for datasets other than the training dataset. The module also offers methods to cut and modify the rules of the tree. Users can contact gregor.schindler@student.kit.edu for any questions or annotations.", "start_char_idx": 0, "end_char_idx": 497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08478107-c0d1-437b-8c33-b8d84b1e647f": {"__data__": {"id_": "08478107-c0d1-437b-8c33-b8d84b1e647f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\37.txt", "file_name": "37.txt", "file_type": "text/plain", "file_size": 669, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15ea1a97-9daf-40f6-b912-518052c443f5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\37.txt", "file_name": "37.txt", "file_type": "text/plain", "file_size": 669, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "aa82850139ee89523101361ff9d2531bc8554c15ba7f064a89ad53f781d15624", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de632f38-0ca3-4a6f-9b3b-b237ce14ed74", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\36.txt", "file_name": "36.txt", "file_type": "text/plain", "file_size": 499, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3539f711eeb60d45652f383963be33a151f1adfc63f08c4d2221f8167676c340", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b889bf1-fd55-491c-ad76-7c279aa4f7c6", "node_type": "1", "metadata": {}, "hash": "a53dbbe6cac17a823e6a53eb3c79037cac33c51926eb6cf9f2de0762668607e1", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: sf-gad\r\nlink: https://github.com/teco-kit/sf-gad\r\ndescription: SFGAD is a tool for detecting anomalies in **graph** and **graph streams** with python. The framework defines a modular interface that allows full customization of the analysis process, making it possible to automate any workflow, run and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. SFGAD is available for installation at the Python package index and its source code is currently available on GitHub. The project is partially funded by the German Federal Ministry of Education and Research (BMBF).", "start_char_idx": 0, "end_char_idx": 667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b889bf1-fd55-491c-ad76-7c279aa4f7c6": {"__data__": {"id_": "6b889bf1-fd55-491c-ad76-7c279aa4f7c6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08478107-c0d1-437b-8c33-b8d84b1e647f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\37.txt", "file_name": "37.txt", "file_type": "text/plain", "file_size": 669, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8d03f292ded06b3768b780dd1a69419a5a4fd8885a5e8a7b49238c0199b98602", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1b74f2b-e4e0-4e9e-a306-aad72168e436", "node_type": "1", "metadata": {}, "hash": "a05115231afb78e11f5939d18fab4f2f1c779888154ff43e5e817e27b0fcacb2", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AutoML4TS\r\nlink: https://github.com/teco-kit/AutoML4TS\r\ndescription: AutoML4TS is a automated tool for forecasting time-series data based on classic regression models. It works well with different time-series, e.g. seasonal, non-seasonal, trend, non-trend. When using this tool, the user does not need to make choice between different models and customize it by setting its hyperparameters. This is a project of SDSC-BW. Time-series forecasting is studied in nearly all fields of science and engineering. For all forecasting tasks, model selection is a necessary step. However, from a set of available models, selecting the most adequate model for a given dataset is still a di\ufb03cult task. If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall.", "start_char_idx": 0, "end_char_idx": 841, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1b74f2b-e4e0-4e9e-a306-aad72168e436": {"__data__": {"id_": "c1b74f2b-e4e0-4e9e-a306-aad72168e436", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b889bf1-fd55-491c-ad76-7c279aa4f7c6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c3bb0be49603c3fd93cd78db75460134cd304b880c2f9d756b8a935c6411500c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bc47470-bed2-4eb9-a850-352d5923ea7d", "node_type": "1", "metadata": {}, "hash": "490dfcf28753e66819614ef3256a642a61ea8277235880edb1758ca39ff6f174", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: AutoML4TS\r\nlink: https://github.com/teco-kit/AutoML4TS\r\ndescription: AutoML4TS is a automated tool for forecasting time-series data based on classic regression models. It works well with different time-series, e.g. seasonal, non-seasonal, trend, non-trend. When using this tool, the user does not need to make choice between different models and customize it by setting its hyperparameters. This is a project of SDSC-BW. Time-series forecasting is studied in nearly all fields of science and engineering. For all forecasting tasks, model selection is a necessary step. However, from a set of available models, selecting the most adequate model for a given dataset is still a di\ufb03cult task. If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall. The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task.", "start_char_idx": 0, "end_char_idx": 1018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bc47470-bed2-4eb9-a850-352d5923ea7d": {"__data__": {"id_": "5bc47470-bed2-4eb9-a850-352d5923ea7d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1b74f2b-e4e0-4e9e-a306-aad72168e436", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6527aae322d7acab8df74960e4529dc67b1a5801108cf8e143ed64d4b88332ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fa2089b-4515-4f60-ad2e-9e409a4ad72e", "node_type": "1", "metadata": {}, "hash": "acff2896469bcdcf26a2292765168e4b10339278d84580f4d6de1877dea49f90", "class_name": "RelatedNodeInfo"}}, "text": "It works well with different time-series, e.g. seasonal, non-seasonal, trend, non-trend. When using this tool, the user does not need to make choice between different models and customize it by setting its hyperparameters. This is a project of SDSC-BW. Time-series forecasting is studied in nearly all fields of science and engineering. For all forecasting tasks, model selection is a necessary step. However, from a set of available models, selecting the most adequate model for a given dataset is still a di\ufb03cult task. If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall. The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task. The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements.", "start_char_idx": 179, "end_char_idx": 1203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fa2089b-4515-4f60-ad2e-9e409a4ad72e": {"__data__": {"id_": "5fa2089b-4515-4f60-ad2e-9e409a4ad72e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bc47470-bed2-4eb9-a850-352d5923ea7d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "236864ebeb144e4570b386d9a8a8f7f206a1173adfc24023023a13c95e321b8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de6deba2-c3f5-41c7-8a27-9cdff1a5ac7b", "node_type": "1", "metadata": {}, "hash": "2c76d75c2b21ffc83d48487204799c55dc789481851b1bd4ad0ed03ea26904e6", "class_name": "RelatedNodeInfo"}}, "text": "seasonal, non-seasonal, trend, non-trend. When using this tool, the user does not need to make choice between different models and customize it by setting its hyperparameters. This is a project of SDSC-BW. Time-series forecasting is studied in nearly all fields of science and engineering. For all forecasting tasks, model selection is a necessary step. However, from a set of available models, selecting the most adequate model for a given dataset is still a di\ufb03cult task. If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall. The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task. The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements. The Sequential Model-based Algorithm Configuration (SMAC) method showed a successful performance in a high-dimensional, structured and continuous and discrete (Categorical) hybrid parameters space.", "start_char_idx": 226, "end_char_idx": 1401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de6deba2-c3f5-41c7-8a27-9cdff1a5ac7b": {"__data__": {"id_": "de6deba2-c3f5-41c7-8a27-9cdff1a5ac7b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa2089b-4515-4f60-ad2e-9e409a4ad72e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e64e435e0379f7b1a9d2532cee0a00901bc383492a3035f19f64a72ed8fc1c12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75b63164-962c-48de-8378-246464d714ab", "node_type": "1", "metadata": {}, "hash": "6b4a32fe990a5db783a8ada278415a80e397b111832aea1171621dd4e04b65ea", "class_name": "RelatedNodeInfo"}}, "text": "This is a project of SDSC-BW. Time-series forecasting is studied in nearly all fields of science and engineering. For all forecasting tasks, model selection is a necessary step. However, from a set of available models, selecting the most adequate model for a given dataset is still a di\ufb03cult task. If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall. The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task. The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements. The Sequential Model-based Algorithm Configuration (SMAC) method showed a successful performance in a high-dimensional, structured and continuous and discrete (Categorical) hybrid parameters space. This framework, through the application of SMAC, deals with this problem in the context of forecasting.", "start_char_idx": 402, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75b63164-962c-48de-8378-246464d714ab": {"__data__": {"id_": "75b63164-962c-48de-8378-246464d714ab", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de6deba2-c3f5-41c7-8a27-9cdff1a5ac7b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "06dcaa2f2ee15453927e93cb3964cbb38dad3acc50c9481bb2a245a9893cf631", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2907a7dc-a527-4850-bc63-db7a68f6abd9", "node_type": "1", "metadata": {}, "hash": "8a02993d83cfa8397695b7bafa67818298f75271e22e9cdd740c3f1c5b94bdca", "class_name": "RelatedNodeInfo"}}, "text": "Time-series forecasting is studied in nearly all fields of science and engineering. For all forecasting tasks, model selection is a necessary step. However, from a set of available models, selecting the most adequate model for a given dataset is still a di\ufb03cult task. If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall. The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task. The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements. The Sequential Model-based Algorithm Configuration (SMAC) method showed a successful performance in a high-dimensional, structured and continuous and discrete (Categorical) hybrid parameters space. This framework, through the application of SMAC, deals with this problem in the context of forecasting. In contrast to previous works on time-series forecasting, the candidate models in my work are machine learning (ML) models from the Scikit-learn regression package.", "start_char_idx": 432, "end_char_idx": 1670, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2907a7dc-a527-4850-bc63-db7a68f6abd9": {"__data__": {"id_": "2907a7dc-a527-4850-bc63-db7a68f6abd9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75b63164-962c-48de-8378-246464d714ab", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "238819c9a79260d0e77d4f6e2d9471132edab730f4a27c141c0b54bded6c20e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37d3d69e-55cf-4f4e-b4db-238afc02b8b9", "node_type": "1", "metadata": {}, "hash": "78da174ed30600049d8009c1e633b2ba3a0d6c1d320f325bdbdbcf5ad75637d1", "class_name": "RelatedNodeInfo"}}, "text": "If the hyper-parameters of candidate models are taken into consideration, there would be an enormous number of possible alternatives overall. The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task. The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements. The Sequential Model-based Algorithm Configuration (SMAC) method showed a successful performance in a high-dimensional, structured and continuous and discrete (Categorical) hybrid parameters space. This framework, through the application of SMAC, deals with this problem in the context of forecasting. In contrast to previous works on time-series forecasting, the candidate models in my work are machine learning (ML) models from the Scikit-learn regression package. In order to improve the performance of ML models, the package TSFRESH [reference] is integrated in the framework, which extracts comprehensive and well-established features from time-series.", "start_char_idx": 700, "end_char_idx": 1861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37d3d69e-55cf-4f4e-b4db-238afc02b8b9": {"__data__": {"id_": "37d3d69e-55cf-4f4e-b4db-238afc02b8b9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2907a7dc-a527-4850-bc63-db7a68f6abd9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7f7cec0c7b55070075f14dbcb6114229c81b82ad8322c2d5345040bc7fc2dbac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fbb8bb7-fe2a-4b31-85b4-1c544de51090", "node_type": "1", "metadata": {}, "hash": "de57361705691fa6927c36efbfd2aaaa95dc38e65448da2426edb8be4e14a4a7", "class_name": "RelatedNodeInfo"}}, "text": "The wide range of forecasting applications led to growing demand for a system which can automatically select a good model and simultaneously set good parameters for a new task. The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements. The Sequential Model-based Algorithm Configuration (SMAC) method showed a successful performance in a high-dimensional, structured and continuous and discrete (Categorical) hybrid parameters space. This framework, through the application of SMAC, deals with this problem in the context of forecasting. In contrast to previous works on time-series forecasting, the candidate models in my work are machine learning (ML) models from the Scikit-learn regression package. In order to improve the performance of ML models, the package TSFRESH [reference] is integrated in the framework, which extracts comprehensive and well-established features from time-series. In order to improve the efficiency of the optimization process, a meta-learning method are supported to generate configurations with the likelihood of performing for a new task.", "start_char_idx": 842, "end_char_idx": 2039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fbb8bb7-fe2a-4b31-85b4-1c544de51090": {"__data__": {"id_": "2fbb8bb7-fe2a-4b31-85b4-1c544de51090", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d82a2a6b-826d-4e9e-815f-2e66499598bd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9dd51b44d5616e968910659e91a46e1e398f6b980d1f913d7af7908db8501730", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37d3d69e-55cf-4f4e-b4db-238afc02b8b9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b177e730882df76889d3f6ccbfb4dfedb191905bf9f1f4fbeb4d98c56dd890d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3039edf2-acef-4655-b867-bb6228b5042e", "node_type": "1", "metadata": {}, "hash": "77b6486be110d1af88c3000153fbef07457d9796f8ebc7dd22a52828107d9b25", "class_name": "RelatedNodeInfo"}}, "text": "The combined algorithm selection and hyper-parameter optimization problem was dubbed CASH. Recently, automated approaches for solving this problem have led to substantial improvements. The Sequential Model-based Algorithm Configuration (SMAC) method showed a successful performance in a high-dimensional, structured and continuous and discrete (Categorical) hybrid parameters space. This framework, through the application of SMAC, deals with this problem in the context of forecasting. In contrast to previous works on time-series forecasting, the candidate models in my work are machine learning (ML) models from the Scikit-learn regression package. In order to improve the performance of ML models, the package TSFRESH [reference] is integrated in the framework, which extracts comprehensive and well-established features from time-series. In order to improve the efficiency of the optimization process, a meta-learning method are supported to generate configurations with the likelihood of performing for a new task. This initialization method yields slight improvements to expedite the optimization.", "start_char_idx": 1019, "end_char_idx": 2123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3039edf2-acef-4655-b867-bb6228b5042e": {"__data__": {"id_": "3039edf2-acef-4655-b867-bb6228b5042e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\39.txt", "file_name": "39.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a49a291e-81cf-44f8-ab6a-29ed0d9e2a7e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\39.txt", "file_name": "39.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bfee21c9963d8d00c18e5283582ef863843353903a5bf46e4e0237117b2fd11e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fbb8bb7-fe2a-4b31-85b4-1c544de51090", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c21c5e5f21c0f4e3686f7ab2bb6a28b1daf58a7f9207545c5d8d7f4aee32914c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f7eb92b-6b71-45b2-a50f-364f6993b98d", "node_type": "1", "metadata": {}, "hash": "c36aa2a9d835249c778128ebdbf26eb77ea567b7d3c913ae60a6b0a68f8213aa", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: explorer\r\nlink: https://github.com/teco-kit/explorer\r\ndescription: Explorer is a React based tool for collaborative, browser-based time series data annotation that supports managing and interacting with datasets, videos, labels, users and has 2FA to improve compliance. Clone with `--recurse-submodules`-flag to automatically initialize and update each submodule.", "start_char_idx": 0, "end_char_idx": 374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f7eb92b-6b71-45b2-a50f-364f6993b98d": {"__data__": {"id_": "4f7eb92b-6b71-45b2-a50f-364f6993b98d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3039edf2-acef-4655-b867-bb6228b5042e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\39.txt", "file_name": "39.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "29c37eb01b0d438a279d9ef00cdb2c14dbbf81d83b03c5bbd6fca6177d8df531", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01649a08-dd5b-495f-aa60-8a461196d3a3", "node_type": "1", "metadata": {}, "hash": "2a14afa0feb2de0ddba9e7c7c8dcabb8f73f82d3c760d0473a35e3962ee6fee6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: edge-ml\r\nlink: https://github.com/edge-ml/edge-ml\r\ndescription: \"edge-ml is an embedded-first machine learning framework that helps developers build models faster and more robustly with an open-source toolchain for embedded machine learning. With a few simple steps edge-ml lets you record data, label samples, train models and deploy validated embedded machine learning directly on the edge. edge-ml requires minimal initialization and supports upload in real-time as well as in bulk from the edge. Pre-recorded data can also be drag-and-dropped as CSV files to the edge-ml cloud storage. Models are generated using AutoML therefore requiring minimal user configuration. The models are optimized for resource-constrained embedded chips based on hardware-aware neural network training. If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started.", "start_char_idx": 0, "end_char_idx": 995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01649a08-dd5b-495f-aa60-8a461196d3a3": {"__data__": {"id_": "01649a08-dd5b-495f-aa60-8a461196d3a3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f7eb92b-6b71-45b2-a50f-364f6993b98d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "78037930b800bf84c3d3ea4596d2a2a936521881b337ff4e566b2293eaf257ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06344451-03dd-4161-8b01-059e32e1ad43", "node_type": "1", "metadata": {}, "hash": "aaefea5bfdaeeb6621c4829bea49a8361faff4f35adb5259114cdf66f7b747cf", "class_name": "RelatedNodeInfo"}}, "text": "With a few simple steps edge-ml lets you record data, label samples, train models and deploy validated embedded machine learning directly on the edge. edge-ml requires minimal initialization and supports upload in real-time as well as in bulk from the edge. Pre-recorded data can also be drag-and-dropped as CSV files to the edge-ml cloud storage. Models are generated using AutoML therefore requiring minimal user configuration. The models are optimized for resource-constrained embedded chips based on hardware-aware neural network training. If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started. A growing number of organizations support and use _edge-ml_ in their workflows. A comprehensive installation and setup guide is provided in the wiki. If you want to contribute, this is the recommended way to install and run edge-ml.", "start_char_idx": 253, "end_char_idx": 1228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06344451-03dd-4161-8b01-059e32e1ad43": {"__data__": {"id_": "06344451-03dd-4161-8b01-059e32e1ad43", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01649a08-dd5b-495f-aa60-8a461196d3a3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7e0d284fe3aa6432225aa5dda2339551db8758bfb1783e7dbd1cb603c792db79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c99e8179-bdca-4610-95d2-69e7ec922d5d", "node_type": "1", "metadata": {}, "hash": "eb3ddb2bfe2026db4f5ded182ae4d9ee042f29a1f9925eaad5a8679844668870", "class_name": "RelatedNodeInfo"}}, "text": "edge-ml requires minimal initialization and supports upload in real-time as well as in bulk from the edge. Pre-recorded data can also be drag-and-dropped as CSV files to the edge-ml cloud storage. Models are generated using AutoML therefore requiring minimal user configuration. The models are optimized for resource-constrained embedded chips based on hardware-aware neural network training. If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started. A growing number of organizations support and use _edge-ml_ in their workflows. A comprehensive installation and setup guide is provided in the wiki. If you want to contribute, this is the recommended way to install and run edge-ml. Installing with Docker is also possible, but it's rather meant to be a quick and easy way to run edge-ml, not as development setup.", "start_char_idx": 404, "end_char_idx": 1360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c99e8179-bdca-4610-95d2-69e7ec922d5d": {"__data__": {"id_": "c99e8179-bdca-4610-95d2-69e7ec922d5d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06344451-03dd-4161-8b01-059e32e1ad43", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f06718b913d66e2bc7b9fa051a5f3c9a07d85e0d08429022f0dffd4c02c032bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05d4e123-c41d-495f-b941-c0978c7d90be", "node_type": "1", "metadata": {}, "hash": "1a94d5d1e38defedcc7fe3ba4b82721df2309d34baaf5d6fb40ac24a0e09dba1", "class_name": "RelatedNodeInfo"}}, "text": "Pre-recorded data can also be drag-and-dropped as CSV files to the edge-ml cloud storage. Models are generated using AutoML therefore requiring minimal user configuration. The models are optimized for resource-constrained embedded chips based on hardware-aware neural network training. If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started. A growing number of organizations support and use _edge-ml_ in their workflows. A comprehensive installation and setup guide is provided in the wiki. If you want to contribute, this is the recommended way to install and run edge-ml. Installing with Docker is also possible, but it's rather meant to be a quick and easy way to run edge-ml, not as development setup. Clone with `--recurse-submodules`-flag to automatically initialize and update each submodule.", "start_char_idx": 511, "end_char_idx": 1454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05d4e123-c41d-495f-b941-c0978c7d90be": {"__data__": {"id_": "05d4e123-c41d-495f-b941-c0978c7d90be", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c99e8179-bdca-4610-95d2-69e7ec922d5d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f5eeb1eb57ab205d7b7ab27e790dc5182164e74bb81b63bc70e746b88ae76ed1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ddad4dc-19f2-48c5-819b-34739d83325a", "node_type": "1", "metadata": {}, "hash": "e0afd7fa5685a6a8922fc7269c1528b5f38dc0631fa3d822dbfd34b02d5728e1", "class_name": "RelatedNodeInfo"}}, "text": "Models are generated using AutoML therefore requiring minimal user configuration. The models are optimized for resource-constrained embedded chips based on hardware-aware neural network training. If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started. A growing number of organizations support and use _edge-ml_ in their workflows. A comprehensive installation and setup guide is provided in the wiki. If you want to contribute, this is the recommended way to install and run edge-ml. Installing with Docker is also possible, but it's rather meant to be a quick and easy way to run edge-ml, not as development setup. Clone with `--recurse-submodules`-flag to automatically initialize and update each submodule. Run the following command to build the images and to pull the mongoDB image. Services are now available.", "start_char_idx": 601, "end_char_idx": 1559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ddad4dc-19f2-48c5-819b-34739d83325a": {"__data__": {"id_": "4ddad4dc-19f2-48c5-819b-34739d83325a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05d4e123-c41d-495f-b941-c0978c7d90be", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "973ae8157009270d18ab3b074ded370a181a956fbcfded3262e593fceb7261c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16010fc3-becf-43c1-b25d-100b868973fb", "node_type": "1", "metadata": {}, "hash": "48e0e3c7e0030f6844557a18516c1d66e3c1bbad5531921b841098c8fe1d5d69", "class_name": "RelatedNodeInfo"}}, "text": "The models are optimized for resource-constrained embedded chips based on hardware-aware neural network training. If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started. A growing number of organizations support and use _edge-ml_ in their workflows. A comprehensive installation and setup guide is provided in the wiki. If you want to contribute, this is the recommended way to install and run edge-ml. Installing with Docker is also possible, but it's rather meant to be a quick and easy way to run edge-ml, not as development setup. Clone with `--recurse-submodules`-flag to automatically initialize and update each submodule. Run the following command to build the images and to pull the mongoDB image. Services are now available. Run the following command to automatically pull and start the latest version of the dockerized application from Docker Hub.", "start_char_idx": 683, "end_char_idx": 1683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16010fc3-becf-43c1-b25d-100b868973fb": {"__data__": {"id_": "16010fc3-becf-43c1-b25d-100b868973fb", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a999894d-7b98-4e77-b962-13f1a9ed7ddc", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d53288e758e0ddbc68aa5f7d2073fdf7c06611f5f9824d5af0913c1c53688e65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ddad4dc-19f2-48c5-819b-34739d83325a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "54d5eb0e25b80e6dde8d392ab7dd0f9614252711a887d3bbefd332f96d08c5a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f16d9244-dcc6-412b-ae05-a0f8b79c18ff", "node_type": "1", "metadata": {}, "hash": "78d4d684f7c22ac9cfa0cb9981b95b7a4b75814fc4dfaa674552c3c5ed82640c", "class_name": "RelatedNodeInfo"}}, "text": "If you are interested in using _edge-ml_, please go to app.edge-ml.org and register! Please visit our wiki for a comprehensive overview of edge-ml and some excellent tutorials on how to get started. A growing number of organizations support and use _edge-ml_ in their workflows. A comprehensive installation and setup guide is provided in the wiki. If you want to contribute, this is the recommended way to install and run edge-ml. Installing with Docker is also possible, but it's rather meant to be a quick and easy way to run edge-ml, not as development setup. Clone with `--recurse-submodules`-flag to automatically initialize and update each submodule. Run the following command to build the images and to pull the mongoDB image. Services are now available. Run the following command to automatically pull and start the latest version of the dockerized application from Docker Hub. You can now easily debug the different components. Changes can be committed in each sub-repository and pushed from there directly.\"", "start_char_idx": 797, "end_char_idx": 1815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f16d9244-dcc6-412b-ae05-a0f8b79c18ff": {"__data__": {"id_": "f16d9244-dcc6-412b-ae05-a0f8b79c18ff", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16010fc3-becf-43c1-b25d-100b868973fb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a0b0c220c8cede6d1b5ecc6f5c65cc1c649968c7edc3e93eb2cb70200702fdbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba3d462e-0434-43a1-959d-7b7bf1ee7491", "node_type": "1", "metadata": {}, "hash": "01882b70539d731fa0af6f8a3555e34f2d9b7674acf5383ab114e708e9cbbeff", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: veri\r\nlink: https://github.com/bgokden/veri\r\ndescription: Veri is a Distributed Feature Store optimized for Search and Recommendation tasks. Feature Label store allows storing features as keys and labels as values. Querying values is only possible with knn using features. Veri also supports creating sub sample spaces of data by default. Veri works as a cluster that can hold a Vector Space with fixed dimension and allows easy querying of k nearest neighbour search queries and also querying a sample space to be used in a machine learning algorithm. Veri is currently in Beta Stage. Veri means data in Turkish. Veri is not a regular database, instead it is purely designed to be used in machine learning. It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm.", "start_char_idx": 0, "end_char_idx": 974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba3d462e-0434-43a1-959d-7b7bf1ee7491": {"__data__": {"id_": "ba3d462e-0434-43a1-959d-7b7bf1ee7491", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f16d9244-dcc6-412b-ae05-a0f8b79c18ff", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5ed841ae8f630d828930f8bfb7a8fce1d5f56067a8288edd94e7631c87da04be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be2af7a5-883e-40fb-830e-eb0635b0f400", "node_type": "1", "metadata": {}, "hash": "bb3f4f8c6eecec7f30517fb646b066da64240ec37cbd30ef294d87830e26bd77", "class_name": "RelatedNodeInfo"}}, "text": "Feature Label store allows storing features as keys and labels as values. Querying values is only possible with knn using features. Veri also supports creating sub sample spaces of data by default. Veri works as a cluster that can hold a Vector Space with fixed dimension and allows easy querying of k nearest neighbour search queries and also querying a sample space to be used in a machine learning algorithm. Veri is currently in Beta Stage. Veri means data in Turkish. Veri is not a regular database, instead it is purely designed to be used in machine learning. It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available.", "start_char_idx": 152, "end_char_idx": 1162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be2af7a5-883e-40fb-830e-eb0635b0f400": {"__data__": {"id_": "be2af7a5-883e-40fb-830e-eb0635b0f400", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba3d462e-0434-43a1-959d-7b7bf1ee7491", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6000420b63b5ef9dff9bfcec6725b5920248c71cff0271c9211e9c516845bf49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2a6678a-3dd6-4775-aa08-c51ae83b4734", "node_type": "1", "metadata": {}, "hash": "80e50c7ebe77fa9d6b59786d57850e2fd99ed94d00ad04258750931cce243ceb", "class_name": "RelatedNodeInfo"}}, "text": "Querying values is only possible with knn using features. Veri also supports creating sub sample spaces of data by default. Veri works as a cluster that can hold a Vector Space with fixed dimension and allows easy querying of k nearest neighbour search queries and also querying a sample space to be used in a machine learning algorithm. Veri is currently in Beta Stage. Veri means data in Turkish. Veri is not a regular database, instead it is purely designed to be used in machine learning. It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale.", "start_char_idx": 226, "end_char_idx": 1259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2a6678a-3dd6-4775-aa08-c51ae83b4734": {"__data__": {"id_": "d2a6678a-3dd6-4775-aa08-c51ae83b4734", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be2af7a5-883e-40fb-830e-eb0635b0f400", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c9bf4880866f6f3d94464b6c628f9903230588e1e06a298f7711c24fbd3438b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f46ba24-327d-413d-a6a0-fc13f01f61f4", "node_type": "1", "metadata": {}, "hash": "6a7d5c48bf6ae88981a42503eff558083acb498797285d6d3456001d07e6b5ba", "class_name": "RelatedNodeInfo"}}, "text": "Veri works as a cluster that can hold a Vector Space with fixed dimension and allows easy querying of k nearest neighbour search queries and also querying a sample space to be used in a machine learning algorithm. Veri is currently in Beta Stage. Veri means data in Turkish. Veri is not a regular database, instead it is purely designed to be used in machine learning. It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space.", "start_char_idx": 350, "end_char_idx": 1396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f46ba24-327d-413d-a6a0-fc13f01f61f4": {"__data__": {"id_": "6f46ba24-327d-413d-a6a0-fc13f01f61f4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2a6678a-3dd6-4775-aa08-c51ae83b4734", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "243ea7888c1e781043e38e4afcb9e318b2a2e2872b0eb9aec50672ec92f4c440", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce70a585-6e18-4f30-b5be-ea101e738c13", "node_type": "1", "metadata": {}, "hash": "b0b08a4b9991b91b82e9092b339b6c40a832bb49bcd1264d4e63f9c220f07837", "class_name": "RelatedNodeInfo"}}, "text": "Veri is currently in Beta Stage. Veri means data in Turkish. Veri is not a regular database, instead it is purely designed to be used in machine learning. It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance).", "start_char_idx": 564, "end_char_idx": 1522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce70a585-6e18-4f30-b5be-ea101e738c13": {"__data__": {"id_": "ce70a585-6e18-4f30-b5be-ea101e738c13", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f46ba24-327d-413d-a6a0-fc13f01f61f4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a8c297806f3555b0216bedd8679f5512dd2f21eba6dce61a26297cdca62a100c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8ed2b9c-5b6b-48d7-b4e9-cee69dd75c61", "node_type": "1", "metadata": {}, "hash": "9827d17026edca04b4420b51f5c993260c9a79ddd9c1266480856ba77e28fa7c", "class_name": "RelatedNodeInfo"}}, "text": "Veri means data in Turkish. Veri is not a regular database, instead it is purely designed to be used in machine learning. It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough.", "start_char_idx": 597, "end_char_idx": 1624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8ed2b9c-5b6b-48d7-b4e9-cee69dd75c61": {"__data__": {"id_": "b8ed2b9c-5b6b-48d7-b4e9-cee69dd75c61", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce70a585-6e18-4f30-b5be-ea101e738c13", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "15a65b65e92e9f87fc5365addc06c3f4a6e73e5cc8b9a36b6605fa6d0852f475", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e78a53d3-a097-42a3-9d11-8e7d6d4539a6", "node_type": "1", "metadata": {}, "hash": "cb56d1a514671a244be7e32385f31cb6f58f3638c1fdd03004f43f678f693a6f", "class_name": "RelatedNodeInfo"}}, "text": "It does not give any guarantee of responding with the same result every time. In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result.", "start_char_idx": 719, "end_char_idx": 1731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e78a53d3-a097-42a3-9d11-8e7d6d4539a6": {"__data__": {"id_": "e78a53d3-a097-42a3-9d11-8e7d6d4539a6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8ed2b9c-5b6b-48d7-b4e9-cee69dd75c61", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f566f64d8d54dd2a0537ec50412271f89c29f1ef3d448a98e78aae21921eca5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1eafa26-9e1e-4623-a04b-ede8deee9484", "node_type": "1", "metadata": {}, "hash": "5619a657f70a8650f9ba27823030e81553d49c7cd73cc11574f3698450b7fb9b", "class_name": "RelatedNodeInfo"}}, "text": "In machine learning, data scientist usually convert data into a feature label vector space, when a space is ready it is almost always about writing and optimising the algorithm. I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning.", "start_char_idx": 797, "end_char_idx": 1806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1eafa26-9e1e-4623-a04b-ede8deee9484": {"__data__": {"id_": "e1eafa26-9e1e-4623-a04b-ede8deee9484", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e78a53d3-a097-42a3-9d11-8e7d6d4539a6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "629b07c553af9d3f8c0fea8b2369f801b3b8ac2a552d2c937355eb859ddc1f56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8b5a20a-fa76-4984-b285-fe5fd39a4ed6", "node_type": "1", "metadata": {}, "hash": "6549aec86ce7fd2a55533d132e5a043df103bf22458ad126962c37e74a966234", "class_name": "RelatedNodeInfo"}}, "text": "I have worked in different roles as a Data Engineer, Data scientist and a Software Developer. In many projects, I wanted a scalable approach to vector space search which is not available. I wanted to optimise the data ingestion and data querying into one tool. Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return.", "start_char_idx": 975, "end_char_idx": 2079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8b5a20a-fa76-4984-b285-fe5fd39a4ed6": {"__data__": {"id_": "b8b5a20a-fa76-4984-b285-fe5fd39a4ed6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1eafa26-9e1e-4623-a04b-ede8deee9484", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b76a8722f7715cf6afaf964a0baa4567c8cb0df403cd708a25a8d9c949b1366b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e75b966d-b1ee-49ae-abd0-1f58622f3e37", "node_type": "1", "metadata": {}, "hash": "a0e7e8ad1727c4d1bd9e5edd93e71fee76fbe6fd3f709a3869f650b345bc186a", "class_name": "RelatedNodeInfo"}}, "text": "Veri is meant to scale. Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return. If a search with the same id received, query is rejected to avoid infinite recursions.", "start_char_idx": 1236, "end_char_idx": 2166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e75b966d-b1ee-49ae-abd0-1f58622f3e37": {"__data__": {"id_": "e75b966d-b1ee-49ae-abd0-1f58622f3e37", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8b5a20a-fa76-4984-b285-fe5fd39a4ed6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "487ae9fbd24a397f828ecff540d31d85fd5c7df9084193a112312a8a64de01e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa391b96-75df-4892-9890-35d1308fdfe8", "node_type": "1", "metadata": {}, "hash": "20930cf542d59ad0989836a0574be3397ecb0314eb5cb6bb8f4d3122f41f461b", "class_name": "RelatedNodeInfo"}}, "text": "Each Veri instance tries to synchronise its data with other peers and keep a statistically identical subset of the general vector space. Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return. If a search with the same id received, query is rejected to avoid infinite recursions. This behaviour will be replaced with cached results and checking timeout.", "start_char_idx": 1260, "end_char_idx": 2240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa391b96-75df-4892-9890-35d1308fdfe8": {"__data__": {"id_": "fa391b96-75df-4892-9890-35d1308fdfe8", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e75b966d-b1ee-49ae-abd0-1f58622f3e37", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ce11ecdb77314325d1d04be540f9dc9c8d0b614d7ffe1726a8264ad67cc5199f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d351b4fa-bb02-43e3-a9a7-bed30990573f", "node_type": "1", "metadata": {}, "hash": "8e112723db74ccd1df8b0366ff1acbe849a6210a3ed40e64a02e8f03d447ff3c", "class_name": "RelatedNodeInfo"}}, "text": "Veri keeps the average (Center) and a histogram of distribution of data with the distance to the center (Euclidean Distance). Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return. If a search with the same id received, query is rejected to avoid infinite recursions. This behaviour will be replaced with cached results and checking timeout. Every knn query has a timeout and timeout defines the precision of the result. User can trade the precision for time.", "start_char_idx": 1397, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d351b4fa-bb02-43e3-a9a7-bed30990573f": {"__data__": {"id_": "d351b4fa-bb02-43e3-a9a7-bed30990573f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa391b96-75df-4892-9890-35d1308fdfe8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "24eab01c299fac1f4f1652940327c348e7b9623a65f7e2e0c53dfbb038849ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37fb48ae-48d8-4988-b14a-664f0745d96f", "node_type": "1", "metadata": {}, "hash": "712e52d13966b651ce3d9ee866c79b2ecc88bd1393470df21c2f4a1b370aa667", "class_name": "RelatedNodeInfo"}}, "text": "Every instance continue, exchanging data as long as their average and histogram are not close enough. Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return. If a search with the same id received, query is rejected to avoid infinite recursions. This behaviour will be replaced with cached results and checking timeout. Every knn query has a timeout and timeout defines the precision of the result. User can trade the precision for time. In production users usually want a predictable response time. Since every Veri instance keeps a statistically identical in most classification case you will get the same result.", "start_char_idx": 1523, "end_char_idx": 2536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37fb48ae-48d8-4988-b14a-664f0745d96f": {"__data__": {"id_": "37fb48ae-48d8-4988-b14a-664f0745d96f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d351b4fa-bb02-43e3-a9a7-bed30990573f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "63a7f75851c727b3c6756b1acf6f1f19da87fe3f0721b4cde282d014a94f02e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75378eb4-acd8-4eeb-a183-65f23b9637cc", "node_type": "1", "metadata": {}, "hash": "b13995723f6ab0e266c3d070ed241f980ac9bc99ddb04b4dfbd3e7e7c6811b9e", "class_name": "RelatedNodeInfo"}}, "text": "Veri internally has an internal key-value store, but it also queries its neighbours and merges the result. It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return. If a search with the same id received, query is rejected to avoid infinite recursions. This behaviour will be replaced with cached results and checking timeout. Every knn query has a timeout and timeout defines the precision of the result. User can trade the precision for time. In production users usually want a predictable response time. Since every Veri instance keeps a statistically identical in most classification case you will get the same result. Veri replicates the data to its peers periodically and data is persisted to the disk for crashes.", "start_char_idx": 1625, "end_char_idx": 2634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75378eb4-acd8-4eeb-a183-65f23b9637cc": {"__data__": {"id_": "75378eb4-acd8-4eeb-a183-65f23b9637cc", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ade05e-ea05-4c4a-923e-714df5e6b162", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81e428dbe869f7a19ce763a98e3be62ea89cae21b4e1bda83b28d310bf95f05b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37fb48ae-48d8-4988-b14a-664f0745d96f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "726d1b1002ee51dcdb488148a3d309263bfb077c6f4715c95b50f0c72879e01e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c42224a6-c112-4000-b057-66731c528d69", "node_type": "1", "metadata": {}, "hash": "4c9d7f1731cccaa3bbb2ea753454cd0c398440833f274dc20775e75ba0509e11", "class_name": "RelatedNodeInfo"}}, "text": "It is very similar to map-reduce process done on the fly without planning. When a knn query is stated, veri creates a unique hash, Starts a timer, Then do a local knn search locally, Then calls its peers to do the same with a smaller timeout, Merges results into a map, Waits for timeout and then do a refine process on the result map, and return. If a search with the same id received, query is rejected to avoid infinite recursions. This behaviour will be replaced with cached results and checking timeout. Every knn query has a timeout and timeout defines the precision of the result. User can trade the precision for time. In production users usually want a predictable response time. Since every Veri instance keeps a statistically identical in most classification case you will get the same result. Veri replicates the data to its peers periodically and data is persisted to the disk for crashes. Veri uses badger internally. Many functions are made possible thanks to badger.", "start_char_idx": 1732, "end_char_idx": 2714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c42224a6-c112-4000-b057-66731c528d69": {"__data__": {"id_": "c42224a6-c112-4000-b057-66731c528d69", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\41.txt", "file_name": "41.txt", "file_type": "text/plain", "file_size": 358, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af4db40e-101d-4953-9ef1-9bae0a723f27", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\41.txt", "file_name": "41.txt", "file_type": "text/plain", "file_size": 358, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3a15fe6a5f65f9bdcbdf76aae79ccf2209c6d7989f53f0b2f8f60d06c9b4527a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75378eb4-acd8-4eeb-a183-65f23b9637cc", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ac6f471969060ef90fed9aceb209e6912eed45cef85c752fcd4123e489811985", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82fca539-d612-4664-b565-99467442cf4f", "node_type": "1", "metadata": {}, "hash": "b52447bbb327f38a36bbca015b8ce18c02b658275daf30f31400e4bb96c20fdf", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ivory\r\nlink: https://github.com/antony-a1/ivory\r\ndescription: '_ivory_ defines a specification for how to store feature data and provides a.set of tools for querying it. It does not provide any tooling for producing.feature data in the first place. All ivory commands run as MapReduce jobs so.it assumed that feature data is maintained on HDFS.'", "start_char_idx": 0, "end_char_idx": 356, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82fca539-d612-4664-b565-99467442cf4f": {"__data__": {"id_": "82fca539-d612-4664-b565-99467442cf4f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\42.txt", "file_name": "42.txt", "file_type": "text/plain", "file_size": 856, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "40130f56-0714-4ad9-9a60-30154ed89569", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\42.txt", "file_name": "42.txt", "file_type": "text/plain", "file_size": 856, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6b738309e249e8627f7857e81e7ff2210a1f4aff75a0a6de69e959d1bc74d7ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c42224a6-c112-4000-b057-66731c528d69", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\41.txt", "file_name": "41.txt", "file_type": "text/plain", "file_size": 358, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ed36a18ac1001c375322c58738964b9ce52c26ebfc854aeae6efe6df0e0b4eea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3eee76be-6875-4ed1-92a4-1ae0f8799f3c", "node_type": "1", "metadata": {}, "hash": "c133dc1cfcbb7757125dd2f8a9918a38d0ef07b560dff89d6e8e5531e804e2f6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: hopsworks\r\nlink: https://github.com/logicalclocks/hopsworks\r\ndescription: Hopsworks is a modular platform that provides a Python-centric Feature Store and MLOps capabilities. It brings collaboration for ML teams by offering a secure, governed platform for developing, managing, and sharing ML assets such as features, models, training data, batch scoring data, logs, and more. Hopsworks can be accessed through a serverless app, Azure/AWS/GCP as a managed platform, or by following the installation instructions to install it on any Linux-based virtual machine. The platform also provides development tools for Data Science and supports project-based multi-tenancy, team collaboration, versioning, lineage, and provenance. Hopsworks is available under the AGPL-V3 license and aims to be the most complete and modular ML platform in the market.", "start_char_idx": 0, "end_char_idx": 854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3eee76be-6875-4ed1-92a4-1ae0f8799f3c": {"__data__": {"id_": "3eee76be-6875-4ed1-92a4-1ae0f8799f3c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\43.txt", "file_name": "43.txt", "file_type": "text/plain", "file_size": 615, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25a12674-4683-44b2-90a8-fc3ca4814280", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\43.txt", "file_name": "43.txt", "file_type": "text/plain", "file_size": 615, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "135a65166921f206504ffb58b07f634d2671c853c3ec9ce0423184a53fe2aa39", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82fca539-d612-4664-b565-99467442cf4f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\42.txt", "file_name": "42.txt", "file_type": "text/plain", "file_size": 856, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7c91a55e59375039ba6102a416d7dfdea03ea47e74625fc79863d80355ee3b2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "072c5eb8-d4c3-4cf3-9220-02b33f813271", "node_type": "1", "metadata": {}, "hash": "b3c40fe7818a4562df2b7d908e4fd02786e7c948e9d669209c02a94f8ea66b57", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: butterfree\r\nlink: https://github.com/quintoandar/butterfree\r\ndescription: Butterfree is a tool for building feature stores. Transform your raw data into beautiful features. This library supports Python version 3.7+ and is meant to provide tools for building ETL pipelines for Feature Stores using Apache Spark. The library is centered on the following concepts: To understand the main concepts of Feature Store modeling and library main features, you can check Butterfree's Documentation, which is hosted by Read the Docs. To learn how to use Butterfree in practice, see Butterfree's notebook examples.", "start_char_idx": 0, "end_char_idx": 613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "072c5eb8-d4c3-4cf3-9220-02b33f813271": {"__data__": {"id_": "072c5eb8-d4c3-4cf3-9220-02b33f813271", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\44.txt", "file_name": "44.txt", "file_type": "text/plain", "file_size": 943, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3cab912f-730d-4daa-82b6-c812d016f809", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\44.txt", "file_name": "44.txt", "file_type": "text/plain", "file_size": 943, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "958eb1b2e66cbbccc8ec4d1c848c0d30f146c9f1455afe45c1c690975290cbb3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3eee76be-6875-4ed1-92a4-1ae0f8799f3c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\43.txt", "file_name": "43.txt", "file_type": "text/plain", "file_size": 615, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fd1d3252e19bbdcd71957371b8c266dad95935387b886c6a1ec8aa68f7c36cca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a47dd41e-f7f4-409e-96a0-1c74f339b9f9", "node_type": "1", "metadata": {}, "hash": "39f4e8eccde8b4305821729e46846a718f0cecd2e9985da628ab7d3aa6281291", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: feast\r\nlink: https://github.com/feast-dev/feast\r\ndescription: Feast ( **Fea** ture **St** ore) is an open source feature store for machine learning. It is the fastest path to manage existing infrastructure to productionize analytic data for model training and online inference. Feast allows ML platform teams to automate any workflow, host and manage packages, find and fix vulnerabilities, create instant dev environments, write better code with AI, and collaborate outside of code. In this article, we will walk you through the steps to install Feast, create a feature repository, register your feature definitions, and set up your feature store. We will also cover how to explore your data in the web UI, build a training dataset, load feature values into your online store, and read online features at low latency. Additionally, we will provide important resources and information about the functionality and roadmap of Feast.", "start_char_idx": 0, "end_char_idx": 941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a47dd41e-f7f4-409e-96a0-1c74f339b9f9": {"__data__": {"id_": "a47dd41e-f7f4-409e-96a0-1c74f339b9f9", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\45.txt", "file_name": "45.txt", "file_type": "text/plain", "file_size": 740, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "723f36c7-767e-4956-986c-7c26a50a3958", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\45.txt", "file_name": "45.txt", "file_type": "text/plain", "file_size": 740, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "457a6ff746c4df5f99e2059afd1c675b2bf84f5302005991b257f9267a19bf0b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "072c5eb8-d4c3-4cf3-9220-02b33f813271", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\44.txt", "file_name": "44.txt", "file_type": "text/plain", "file_size": 943, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f7524df609de8abd10568bb53694eb12df54d72ff93297941dbaefa6835c19a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d275ded3-2c1c-48de-813e-a9f529828db1", "node_type": "1", "metadata": {}, "hash": "e439b700faf57eea0eea7eae26e06a6f13fd60b9edbb7f1d4f2199c0f18a8e2b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mljar-supervised\r\nlink: https://github.com/mljar/mljar-supervised\r\ndescription: The `mljar-supervised` is an Automated Machine Learning Python package that works with tabular data. It is designed to save time for a data scientist. It abstracts the common way to preprocess the data, construct the machine learning models, and perform hyper-parameters tuning to find the best model. It is not a black-box as you can see exactly how the ML pipeline is constructed (with a detailed Markdown report for each ML model). The `mljar-supervised` will help you with: automatic documentation, the AutoML report, the decision tree report, and the LightGBM report. It has four built-in modes of work: Explain, Perform, Compete, and Optuna.", "start_char_idx": 0, "end_char_idx": 738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d275ded3-2c1c-48de-813e-a9f529828db1": {"__data__": {"id_": "d275ded3-2c1c-48de-813e-a9f529828db1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\46.txt", "file_name": "46.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "501117bb-59b3-4f0e-8c4b-d9fa547ef48e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\46.txt", "file_name": "46.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2133ae3d6b3c70bd954b1a915a2d26d775a6a6220c433a001fb8c4cdfa06dfbd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a47dd41e-f7f4-409e-96a0-1c74f339b9f9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\45.txt", "file_name": "45.txt", "file_type": "text/plain", "file_size": 740, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "564a9cd67124219f4c1a22fb852f0f31bebfe324df3c2b5a4e68926c0ef0ec95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "602a8439-a01d-48cc-ab4d-44216bade9d0", "node_type": "1", "metadata": {}, "hash": "cf615dd1ad9030ff7793f640d2016f9f662b6ae768bbba62f6bc191e9290a960", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: tsfresh\r\nlink: https://github.com/blue-yonder/tsfresh\r\ndescription:", "start_char_idx": 0, "end_char_idx": 78, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "602a8439-a01d-48cc-ab4d-44216bade9d0": {"__data__": {"id_": "602a8439-a01d-48cc-ab4d-44216bade9d0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\47.txt", "file_name": "47.txt", "file_type": "text/plain", "file_size": 712, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a20f4ac-f1fa-441a-8512-1f1fccb73a5d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\47.txt", "file_name": "47.txt", "file_type": "text/plain", "file_size": 712, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2e9b32cb777ed6afb752bde8fe2843ccdfca1601bbe1bf38b5559a2f792083b2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d275ded3-2c1c-48de-813e-a9f529828db1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\46.txt", "file_name": "46.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "adbc2c37b52a5515689deea9f18bbda80a23d3aacdd6d810245d1259df9118da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2932a65-667a-4131-ac06-896b1377cda7", "node_type": "1", "metadata": {}, "hash": "acb0db344007468e21221e454fbf41adfd66a6745418513a0d54eb75890d0789", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: tpot\r\nlink: https://github.com/EpistasisLab/tpot\r\ndescription: **TPOT** (Tree-based Pipeline Optimization Tool) is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data. Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there. TPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.", "start_char_idx": 0, "end_char_idx": 710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2932a65-667a-4131-ac06-896b1377cda7": {"__data__": {"id_": "f2932a65-667a-4131-ac06-896b1377cda7", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\48.txt", "file_name": "48.txt", "file_type": "text/plain", "file_size": 639, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "906298b4-363f-467d-bb6a-e30c9f7a047d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\48.txt", "file_name": "48.txt", "file_type": "text/plain", "file_size": 639, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d7471b6cf69a89a333dd3b11ee88dbdc64ff657e2162525521be64ab0b1f53a6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "602a8439-a01d-48cc-ab4d-44216bade9d0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\47.txt", "file_name": "47.txt", "file_type": "text/plain", "file_size": 712, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e78181f144427aa2ab3f0afc25e051596a0a143d05d813638df8c22f972e9c99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95070a48-5cbc-4949-be75-e520eec6a29c", "node_type": "1", "metadata": {}, "hash": "bf0c25184141e8f5663266606de618f8b56f42062a728918e93b66d5ce3b26eb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: sklearn-deap\r\nlink: https://github.com/rsteca/sklearn-deap\r\ndescription: sklearn-deap is a library that allows you to use evolutionary algorithms instead of gridsearch in scikit-learn, reducing the time required to find the best parameters for your estimator. It evolves only the combinations that give the best results. It's implemented using the deap library and can be installed using pip: \"pip install sklearn-deap\" or by cloning the repo and typing \"python setup.py install\" on your shell. An ipython notebook comparing EvolutionaryAlgorithmSearchCV against GridSearchCV and RandomizedSearchCV is available as an example.", "start_char_idx": 0, "end_char_idx": 637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95070a48-5cbc-4949-be75-e520eec6a29c": {"__data__": {"id_": "95070a48-5cbc-4949-be75-e520eec6a29c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\49.txt", "file_name": "49.txt", "file_type": "text/plain", "file_size": 648, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fcb5c77-561a-4bf9-a03a-b9960b0a6e8f", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\49.txt", "file_name": "49.txt", "file_type": "text/plain", "file_size": 648, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2d3d6f29858fa406d747a480b3052d247a65f58824e3a49580761b5521a12c48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2932a65-667a-4131-ac06-896b1377cda7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\48.txt", "file_name": "48.txt", "file_type": "text/plain", "file_size": 639, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "380490b380a5a4564028b14fbcaa08fe891ef20099da9d3ca2c0d9dffaaa99c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90f77b72-e08a-492e-9f57-54a84f7ae51e", "node_type": "1", "metadata": {}, "hash": "58096daca599cb9e4611849f269ca06f137cb5ceb7eaa1024d2c234eb3a0aaab", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: keras-tuner\r\nlink: https://github.com/keras-team/keras-tuner\r\ndescription: KerasTuner is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search. Easily configure your search space with a define-by-run syntax, then leverage one of the available search algorithms to find the best hyperparameter values for your models. KerasTuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms. KerasTuner requires Python 3.8+ and TensorFlow 2.0+.", "start_char_idx": 0, "end_char_idx": 646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90f77b72-e08a-492e-9f57-54a84f7ae51e": {"__data__": {"id_": "90f77b72-e08a-492e-9f57-54a84f7ae51e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\5.txt", "file_name": "5.txt", "file_type": "text/plain", "file_size": 808, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c122559-c8be-416d-a1c5-36181ac1ee65", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\5.txt", "file_name": "5.txt", "file_type": "text/plain", "file_size": 808, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "3d444884a69cc93d386a7d418205b4f998c0e3c4cf01e1348e47104baddcb55e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95070a48-5cbc-4949-be75-e520eec6a29c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\49.txt", "file_name": "49.txt", "file_type": "text/plain", "file_size": 648, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f1cd957f7d21e88b75049862028c0d717b170ac936b915cb17c1e84576b27d8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9705f8f0-5fbb-43b5-9cd6-1765d1a5fe80", "node_type": "1", "metadata": {}, "hash": "b825821535e4fc2dad4bcca3146144b4e42192a4f215a3619e54e265c1653da6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: DALEX\r\nlink: https://github.com/ModelOriented/DALEX\r\ndescription: `DALEX` is a package that helps to explain the behavior of any model and explore how complex models are working. It creates a wrapper around a predictive model with the `explain()` function and empowers you to compare wrapped models with a collection of local and global explainers. The main philosophy behind `DALEX` explanations is explained in the Explanatory Model Analysis e-book. This package is part of the DrWhy.AI universe and is available in R on CRAN and in Python on PyPI and conda-forge. Additionally, there is an extension package called DALEXtra that provides easy-to-use `explain_*()` functions for models created in popular libraries like `scikit-learn`, `keras`, `H2O`, `tidymodels`, `xgboost`, `mlr` or `mlr3`.", "start_char_idx": 0, "end_char_idx": 806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9705f8f0-5fbb-43b5-9cd6-1765d1a5fe80": {"__data__": {"id_": "9705f8f0-5fbb-43b5-9cd6-1765d1a5fe80", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\50.txt", "file_name": "50.txt", "file_type": "text/plain", "file_size": 1076, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "122baf94-a4e4-4a13-b77c-03ff283b931c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\50.txt", "file_name": "50.txt", "file_type": "text/plain", "file_size": 1076, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dc92cce79cd9ca4af08bd374f48551019e11a2bb6662d6a095f7de09f97305e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90f77b72-e08a-492e-9f57-54a84f7ae51e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\5.txt", "file_name": "5.txt", "file_type": "text/plain", "file_size": 808, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8a566fb85272ee03d7f84b76c3ce9d487753252885fcdfae965c86a9ae78b31a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "760e0fe7-7e00-4f44-a628-453262461e91", "node_type": "1", "metadata": {}, "hash": "35dd36b22477f7d536a7eb799d7f5224ae8afa9378487c72b4e6c1532549cf5f", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: feature_engine\r\nlink: https://github.com/feature-engine/feature_engine\r\ndescription: Feature-engine is a Python library with multiple transformers to engineer and select features for use in machine learning models. Feature-engine's transformers follow Scikit-learn's functionality with fit() and transform() methods to learn the transforming parameters from the data and then transform it. Current Feature-engine's transformers include functionality for imputation methods, encoding methods, discretisation methods, outlier handling methods, variable transformation methods, variable creation, feature selection, datetime, time series, preprocessing, and wrappers. For installation, it can be downloaded from PyPI using pip or cloned from GitHub. Example usage and documentation can be found in their Jupyter Notebook Gallery or documentation, which is hosted on Read the Docs. The library is licensed under BSD 3-Clause, and you can sponsor them to support further their mission to democratize machine learning and programming tools through open-source software.", "start_char_idx": 0, "end_char_idx": 1074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "760e0fe7-7e00-4f44-a628-453262461e91": {"__data__": {"id_": "760e0fe7-7e00-4f44-a628-453262461e91", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\51.txt", "file_name": "51.txt", "file_type": "text/plain", "file_size": 805, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94880efd-8fbb-469f-9dca-545dd3277949", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\51.txt", "file_name": "51.txt", "file_type": "text/plain", "file_size": 805, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "303610a8b64d4b60607b27741810cba966dd19f5cacf2f50ca5696521f1c37df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9705f8f0-5fbb-43b5-9cd6-1765d1a5fe80", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\50.txt", "file_name": "50.txt", "file_type": "text/plain", "file_size": 1076, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5b8a620594bcf783d998027efc907053e8fd52f69dabc2e68f75d7bf7dd75209", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b167d900-2bfd-4dd4-a2d3-e7cf546af900", "node_type": "1", "metadata": {}, "hash": "94cacc133ae20fa055a204475650ebf6524d6cf10688d66d137a0840608758bc", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: auto_ml\r\nlink: https://github.com/ClimbsRocks/auto_ml\r\ndescription: Auto_ml is a project that automates the entire machine learning process. With this project, it is super easy to use for analytics and getting real-time predictions in production. It can automate any workflow, host and manage packages, find and fix vulnerabilities, and provide instant dev environments. Moreover, with AI, you can write better code. Auto_ml also allows you to collaborate outside of code, fund open source developers, and find GitHub community articles. It offers deep learning with TensorFlow & Keras, XGBoost, LightGBM, CatBoost for 3rd party packages. Feature Responses Classification and Categorical Ensembling are also included in the project. For installation, you can simply run `pip install auto_ml`.", "start_char_idx": 0, "end_char_idx": 803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b167d900-2bfd-4dd4-a2d3-e7cf546af900": {"__data__": {"id_": "b167d900-2bfd-4dd4-a2d3-e7cf546af900", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08354a60-7e6d-40d4-a8cb-37270b552ef1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b8b1535cd0cb583f70d0a1cddd24f9eda60ab1be50684cba0b12da77c1097c07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "760e0fe7-7e00-4f44-a628-453262461e91", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\51.txt", "file_name": "51.txt", "file_type": "text/plain", "file_size": 805, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f75b3ba58e2f29bcdfc4d9e2ce9db5726e62740245507338733afd6ff961a244", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0624af0-9600-4752-8d8f-82a1d8da62c1", "node_type": "1", "metadata": {}, "hash": "8882a998943a1b485164d11e9b426760c673190d084d75bc3bb90a48b9a0bcc0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: automl-gs\r\nlink: https://github.com/minimaxir/automl-gs\r\ndescription: Automl-gs is an AutoML tool which offers a _zero code/model definition interface_ to getting an optimized model and data transformation pipeline in multiple popular ML/DL frameworks, with the philosophy that you don't need to know any modern data preprocessing and machine learning engineering techniques to create a powerful prediction workflow. The cost of computing many different models and hyperparameters is much lower than the opportunity cost of a data scientist's time. The models generated by automl-gs are intended to give a strong baseline for solving a given problem, with the resulting code easily tweakable to improve from the baseline. Currently, automl-gs supports the generation of models for regression and classification problems using several Python frameworks. The automl-gs package can be installed via pip and a corresponding ML/DL framework may need to be installed as well. After installation, it can be used directly from the command line or invoked directly from Python.", "start_char_idx": 0, "end_char_idx": 1079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0624af0-9600-4752-8d8f-82a1d8da62c1": {"__data__": {"id_": "c0624af0-9600-4752-8d8f-82a1d8da62c1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08354a60-7e6d-40d4-a8cb-37270b552ef1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b8b1535cd0cb583f70d0a1cddd24f9eda60ab1be50684cba0b12da77c1097c07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b167d900-2bfd-4dd4-a2d3-e7cf546af900", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d60e876710d524802741e2cf72a63312193df28658b0b66fe4b2383475dea23a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be6cf76b-2076-444f-9022-f723ef281940", "node_type": "1", "metadata": {}, "hash": "c9294a57a3fccfb0d2bfe9ca7fd4ae815954373b41fe51f3d0575169df215e58", "class_name": "RelatedNodeInfo"}}, "text": "The cost of computing many different models and hyperparameters is much lower than the opportunity cost of a data scientist's time. The models generated by automl-gs are intended to give a strong baseline for solving a given problem, with the resulting code easily tweakable to improve from the baseline. Currently, automl-gs supports the generation of models for regression and classification problems using several Python frameworks. The automl-gs package can be installed via pip and a corresponding ML/DL framework may need to be installed as well. After installation, it can be used directly from the command line or invoked directly from Python. The output of the automl-gs training is a trained model with Python code pipelines, allowing for easy integration into any prediction workflow. Automl-gs generates raw Python code using Jinja templates and trains a model using the generated code in a subprocess, repeating the process using different hyperparameters until it finds the best model.", "start_char_idx": 428, "end_char_idx": 1427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be6cf76b-2076-444f-9022-f723ef281940": {"__data__": {"id_": "be6cf76b-2076-444f-9022-f723ef281940", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\53.txt", "file_name": "53.txt", "file_type": "text/plain", "file_size": 891, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9c3b170-0bbd-4182-be76-ff3b15d7cc91", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\53.txt", "file_name": "53.txt", "file_type": "text/plain", "file_size": 891, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "61ad3404ba01e84c798a07a14a89ee0614c00832bb14f8e86e904ccf222a3e23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0624af0-9600-4752-8d8f-82a1d8da62c1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "210448b6f0c0c8f32ec5e18b1fdea8bd35b084bdb25bd75936dbd3dcca6c2879", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eacfa080-5667-4bdb-8293-a56c66005fe0", "node_type": "1", "metadata": {}, "hash": "ee9b314ceb47a361f758f4659507c23d0c9f4cc0b779339c3b4fab63fb5b29d8", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: autogluon\r\nlink: https://github.com/autogluon/autogluon\r\ndescription: AutoGluon is a machine learning automation framework that enables users to easily achieve strong predictive performance in their applications. With just a few lines of code, users can train and deploy high-accuracy machine learning and deep learning models on image, text, time series, and tabular data. AutoGluon also provides tools for hyperparameter optimization, such as ASHA, Hyperband, Bayesian Optimization, and BOHB, which have moved to the standalone package syn-tune. For installation, learning with tabular and time series data, and upcoming features and releases, refer to the AutoGluon website. The framework has scientific publications, hands-on tutorials, and cloud training/deployment resources available. AutoGluon is licensed under the Apache 2.0 License and is open for code contributions.", "start_char_idx": 0, "end_char_idx": 889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eacfa080-5667-4bdb-8293-a56c66005fe0": {"__data__": {"id_": "eacfa080-5667-4bdb-8293-a56c66005fe0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\54.txt", "file_name": "54.txt", "file_type": "text/plain", "file_size": 606, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af821be8-f438-40d9-b136-b205dbf07270", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\54.txt", "file_name": "54.txt", "file_type": "text/plain", "file_size": 606, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "985334fb42c32f299a736bbaa337d4b42819d4adfe30a268f2c880d210e37eb0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be6cf76b-2076-444f-9022-f723ef281940", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\53.txt", "file_name": "53.txt", "file_type": "text/plain", "file_size": 891, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "95eb7388ad82f14675da71b1792481f188a6809f3f044432af9b0627e823132a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b3f876-9d79-434d-8359-db4fbf1bea18", "node_type": "1", "metadata": {}, "hash": "50771498068ba74e02445d85232607c786049fc99334378c513a3ec27805c952", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: data-validation\r\nlink: https://github.com/tensorflow/data-validation\r\ndescription: TF Data Validation is a library for exploring and validating machine learning data. To get started with TFDV, you can install it using the PyPI package or nightly packages. If you want to build TFDV from source, there are two methods: building with Docker or building from source. Before installing TFDV, you need to set up some prerequisites. TensorFlow, Apache Beam, and Apache Arrow are notable dependencies. For any questions, you can direct them to Stack Overflow using the tensorflow-data-validation tag.", "start_char_idx": 0, "end_char_idx": 604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10b3f876-9d79-434d-8359-db4fbf1bea18": {"__data__": {"id_": "10b3f876-9d79-434d-8359-db4fbf1bea18", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\55.txt", "file_name": "55.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "add56e7b-15f4-4e48-b9a3-e63af372e931", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\55.txt", "file_name": "55.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d88f794adda618211141cb04741e1f6c76ea48849203e7027d129fa98cef6ce4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eacfa080-5667-4bdb-8293-a56c66005fe0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\54.txt", "file_name": "54.txt", "file_type": "text/plain", "file_size": 606, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fcd3a7985c6902892f43520f0177b8a6cb7561e4b58d874e4070ea62714e291d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21aebf4d-30f3-43f4-8800-e4adb871a059", "node_type": "1", "metadata": {}, "hash": "e194c9ba44f75e48539547812bfe5f87d21c5e0919b3062304f6d52f96362bf6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: SUOD\r\nlink: https://github.com/yzhao062/SUOD\r\ndescription:", "start_char_idx": 0, "end_char_idx": 69, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21aebf4d-30f3-43f4-8800-e4adb871a059": {"__data__": {"id_": "21aebf4d-30f3-43f4-8800-e4adb871a059", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\56.txt", "file_name": "56.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25decb75-1742-45d0-b05f-f433d285fc43", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\56.txt", "file_name": "56.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a7a93dbb961bae4ef587511aa829d151203da666c5e6cf6c7ddbf08871355ab3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b3f876-9d79-434d-8359-db4fbf1bea18", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\55.txt", "file_name": "55.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "655290440a75ff5db769d97b1dd42ee2e52d8168674ec2efdb31dd4c1a37c185", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4543bc8a-81e6-4642-9d42-34357ca621b2", "node_type": "1", "metadata": {}, "hash": "f55f7b1692c6b0fbe1acb2140ce4cbc17e4c226aebef593766d0d4305c8cf4d6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: outlier-exposure\r\nlink: https://github.com/hendrycks/outlier-exposure\r\ndescription: Outlier Exposure (OE) is a method for improving anomaly detection performance in deep learning models. Using an out-of-distribution dataset, we fine-tune a classifier so that the model learns heuristics to distinguish anomalies and in-distribution samples. Crucially, these heuristics generalize to new distributions. Unlike ODIN, OE does not require a model per OOD dataset and does not require tuning on \"validation\" examples from the OOD dataset in order to work. This repository contains a subset of the calibration and multiclass classification experiments. Please consult the paper for the full results and method descriptions.", "start_char_idx": 0, "end_char_idx": 728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4543bc8a-81e6-4642-9d42-34357ca621b2": {"__data__": {"id_": "4543bc8a-81e6-4642-9d42-34357ca621b2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\57.txt", "file_name": "57.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4e2410e-cb0f-48de-b8b4-0bb4a201f854", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\57.txt", "file_name": "57.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "be634d91f4609790bbe447f6714e8b842019a61f488cf6de19b83d4f0437bc2d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21aebf4d-30f3-43f4-8800-e4adb871a059", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\56.txt", "file_name": "56.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2ff311751c5c5c8aa59cba159637a2c39d117f1f7f48c077c8499179b4ac9d2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdfe54d9-4e53-4637-8812-d5c5457aa426", "node_type": "1", "metadata": {}, "hash": "80b269b787ffc44fea5a0bd11f27269f2b39b72851434547f26cc4f24e707f41", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pyod\r\nlink: https://github.com/yzhao062/pyod\r\ndescription:", "start_char_idx": 0, "end_char_idx": 69, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdfe54d9-4e53-4637-8812-d5c5457aa426": {"__data__": {"id_": "cdfe54d9-4e53-4637-8812-d5c5457aa426", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\58.txt", "file_name": "58.txt", "file_type": "text/plain", "file_size": 811, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7b8cb34-e822-4487-aa4e-f0530099b8b4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\58.txt", "file_name": "58.txt", "file_type": "text/plain", "file_size": 811, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9d7cee1f3dba4be1c3ade8b890803de770c0bbaff124c4a3e7165c6fbe2f30d5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4543bc8a-81e6-4642-9d42-34357ca621b2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\57.txt", "file_name": "57.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "00649f81871a99d27d27f9ffc34a0fa1c9df2109dce66a12a5e219cda585104e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7974f9a4-5225-497e-b66f-9cb7668ef9e6", "node_type": "1", "metadata": {}, "hash": "b2e0586f26c3f2e7a4d13bb3eeac6f234a686ddfbbfbcbb08b79af2753f9d5aa", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: deequ\r\nlink: https://github.com/awslabs/deequ\r\ndescription: Deequ is a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets. It works on tabular data and is designed to work with very large datasets that typically live in a distributed filesystem or a data warehouse. The main entry point for defining how you expect your data to look is the VerificationSuite from which you can add Checks that define constraints on attributes of the data. Deequ translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Our library contains much more functionality than what we showed in the basic example. If you would like to reference this package in a research paper, please cite the paper describing it.", "start_char_idx": 0, "end_char_idx": 809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7974f9a4-5225-497e-b66f-9cb7668ef9e6": {"__data__": {"id_": "7974f9a4-5225-497e-b66f-9cb7668ef9e6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61dd2dff-57e9-42e1-b727-cadc5badbd9a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "da41435fb06df8252de4604f1ea36f457e52536d1877de487fd7c30efe1deb25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdfe54d9-4e53-4637-8812-d5c5457aa426", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\58.txt", "file_name": "58.txt", "file_type": "text/plain", "file_size": 811, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "63d992f3efb6d9f60765058ab9fd66dbf9ab25d5a621b4966111b63f5e85f649", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c2525a9-8b6e-40dc-85de-1d8e056c7a21", "node_type": "1", "metadata": {}, "hash": "3e1deb8252213c0ed035bf637efe76e68f7fc7091cf793651294529f9cd7b947", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: adtk\r\nlink: https://github.com/arundo/adtk\r\ndescription: Introducing the Anomaly Detection Toolkit (ADTK), a Python package for unsupervised/rule-based time series anomaly detection. With ADTK, users can automate workflows, host/manage packages, find/fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and access GitHub community articles. ADTK offers a set of common detectors, transformers, and aggregators with unified APIs, as well as pipe classes that connect them into models. The package also provides functions to process and visualize time series and anomaly events. It is recommended to install the most recent stable release of ADTK from PyPI, but users can also install from source code for the latest, though unstable, version. Pull requests are welcome, and more detailed examples of each module of ADTK can be found in the Examples section of the documentation or an interactive demo notebook.", "start_char_idx": 0, "end_char_idx": 1007, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c2525a9-8b6e-40dc-85de-1d8e056c7a21": {"__data__": {"id_": "3c2525a9-8b6e-40dc-85de-1d8e056c7a21", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61dd2dff-57e9-42e1-b727-cadc5badbd9a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "da41435fb06df8252de4604f1ea36f457e52536d1877de487fd7c30efe1deb25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7974f9a4-5225-497e-b66f-9cb7668ef9e6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9ea3729d4da871a8a8c0f13f4f1a4bebd416b5d3c11cd18bab6885cfd55f69b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8edde0a3-71da-4dd3-826d-021ad09369f6", "node_type": "1", "metadata": {}, "hash": "d5c874b6261f4262a179c0199dd9021acd86ab7c76a9d602593705d9a2d9fa8b", "class_name": "RelatedNodeInfo"}}, "text": "With ADTK, users can automate workflows, host/manage packages, find/fix vulnerabilities, create instant dev environments, write better code with AI, collaborate outside of code, fund open source developers, and access GitHub community articles. ADTK offers a set of common detectors, transformers, and aggregators with unified APIs, as well as pipe classes that connect them into models. The package also provides functions to process and visualize time series and anomaly events. It is recommended to install the most recent stable release of ADTK from PyPI, but users can also install from source code for the latest, though unstable, version. Pull requests are welcome, and more detailed examples of each module of ADTK can be found in the Examples section of the documentation or an interactive demo notebook. ADTK is licensed under the Mozilla Public License 2.0 (MPL 2.0).", "start_char_idx": 194, "end_char_idx": 1072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8edde0a3-71da-4dd3-826d-021ad09369f6": {"__data__": {"id_": "8edde0a3-71da-4dd3-826d-021ad09369f6", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\6.txt", "file_name": "6.txt", "file_type": "text/plain", "file_size": 821, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "48947c14-e046-4c7e-965b-ae53e9da43ef", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\6.txt", "file_name": "6.txt", "file_type": "text/plain", "file_size": 821, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "26a7c7b637af7dc8fc7487a074f81ecfb1be7e6f84dccf899dd107d77c60e5e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c2525a9-8b6e-40dc-85de-1d8e056c7a21", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "43b822fddbf0406bf37c986b74f2ec388a29a75d60ba37448c143da10dd6c769", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a0a7bdd-02cd-4a7f-ba4f-e19f7b6ebdb1", "node_type": "1", "metadata": {}, "hash": "688e67a92940ae18d2866fea6a40917f0e340925f7560ab10304161c254ab2ff", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: TFL2uC\r\nlink: https://github.com/konegen/TFL2uC\r\ndescription: \"The TensorFlow Lite to microcontroller (TFL2uC) tool automates the optimization of TensorFlow models and conversion to a TensorFlow Lite C++ model for easy integration into a C++ project. The optimization algorithms pruning and quantization can be applied to reduce the memory requirements of the model and speed up inference. TFL2uC also provides a GUI for easy use. The tool was developed and tested on Windows 10 using Python v.3.7.10 and pip v.21.0.1, with all other required frameworks located in the requirements.txt file. To use TFL2uC, simply install the required frameworks using 'pip install -r requirements.txt'. Full installation instructions are provided in the TFL2uC Table of Contents. Code released under the Apache-2.0 License.\"", "start_char_idx": 0, "end_char_idx": 819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a0a7bdd-02cd-4a7f-ba4f-e19f7b6ebdb1": {"__data__": {"id_": "7a0a7bdd-02cd-4a7f-ba4f-e19f7b6ebdb1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\60.txt", "file_name": "60.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60f7fb2d-59b7-408d-b53b-2563e901b26a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\60.txt", "file_name": "60.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "eec07760eba1d1f2db9fc8efe505cb478e5a12ca1cf1c0140fe7842f1aed41de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8edde0a3-71da-4dd3-826d-021ad09369f6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\6.txt", "file_name": "6.txt", "file_type": "text/plain", "file_size": 821, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9d785fdf6b5ef0e54ab2cf7a3bea062a5d27d00a66c9a94f15bf9b587332ec58", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7d1dce6-73de-473f-9321-ab6e56218c85", "node_type": "1", "metadata": {}, "hash": "42c2d2f199f50e185ad7ec89a5cf7fe43751d29876a7b28fca23435691016166", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: brooklin\r\nlink: https://github.com/linkedin/brooklin\r\ndescription:", "start_char_idx": 0, "end_char_idx": 77, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7d1dce6-73de-473f-9321-ab6e56218c85": {"__data__": {"id_": "b7d1dce6-73de-473f-9321-ab6e56218c85", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\61.txt", "file_name": "61.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca6985b7-73e4-4d36-abb0-0f7e317f1196", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\61.txt", "file_name": "61.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "50e44d1766b3383114b8b0cf6fff044f1bd1608be241d2dfeffa1b0175be44bf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a0a7bdd-02cd-4a7f-ba4f-e19f7b6ebdb1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\60.txt", "file_name": "60.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "65c427ac9e97d3b778a30c45d2f4aa9568b10eedb65c6f8f90f78f3a6ce6878a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30d4c0bd-28ae-4e72-b345-09e13a88fcef", "node_type": "1", "metadata": {}, "hash": "427239f2e45f0699e58440e4ec2f84d38924ffecd11f0b9d229854b52b6697e0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: faust\r\nlink: https://github.com/robinhood/faust\r\ndescription:", "start_char_idx": 0, "end_char_idx": 72, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30d4c0bd-28ae-4e72-b345-09e13a88fcef": {"__data__": {"id_": "30d4c0bd-28ae-4e72-b345-09e13a88fcef", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\62.txt", "file_name": "62.txt", "file_type": "text/plain", "file_size": 991, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5f58532-b563-42cb-97b8-180547eae720", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\62.txt", "file_name": "62.txt", "file_type": "text/plain", "file_size": 991, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ad3e63804742bbcd9057cf53d93a20a7b73a1cee013072f5515d29d79ef1537f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7d1dce6-73de-473f-9321-ab6e56218c85", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\61.txt", "file_name": "61.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f3cf356036f4a56c6999ed1c4543f1a1d4a5815d6e6a34091dfc1b149363f984", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdc5a975-4138-44dd-8968-f7cd77d8c145", "node_type": "1", "metadata": {}, "hash": "50793c9b6913f82ecad94f01d183a31566c94c749325a3821c6da9653100c2b5", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: flink\r\nlink: https://github.com/apache/flink\r\ndescription: Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities. It is a streaming-first runtime that supports both batch processing and data streaming programs. Some of its features include elegant and fluent APIs in Java and Scala, high throughput and low event latency, fault-tolerance with exactly-once processing guarantees, and natural back-pressure in streaming programs. Flink also has libraries for Graph processing (batch), Machine Learning (batch), and Complex Event Processing (streaming), and built-in support for iterative programs (BSP) in the DataSet (batch) API. Additionally, Flink has custom memory management for efficient and robust switching between in-memory and out-of-core data processing algorithms, compatibility layers for Apache Hadoop MapReduce, and integration with YARN, HDFS, HBase, and other components of the Apache Hadoop ecosystem.", "start_char_idx": 0, "end_char_idx": 989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdc5a975-4138-44dd-8968-f7cd77d8c145": {"__data__": {"id_": "bdc5a975-4138-44dd-8968-f7cd77d8c145", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\63.txt", "file_name": "63.txt", "file_type": "text/plain", "file_size": 68, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5047b9fb-a464-4744-ad60-6223e2b36fc0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\63.txt", "file_name": "63.txt", "file_type": "text/plain", "file_size": 68, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7f4b16258bef6e82449b02b8a603c5f0eeb6d4e0e5687ea9a102d267e49bdf98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30d4c0bd-28ae-4e72-b345-09e13a88fcef", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\62.txt", "file_name": "62.txt", "file_type": "text/plain", "file_size": 991, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "490a56d30ab61b597d167ba3d3a2848fbf0518890e674ff50b6efb050007b965", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe285b24-4ca3-4c1a-80c2-fd9fca7d6d02", "node_type": "1", "metadata": {}, "hash": "3ae1b4976bf9a49c83df02d2afc65b81094f0474a4b108ee59bc6bd49b29362e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: dask\r\nlink: https://github.com/dask/dask\r\ndescription:", "start_char_idx": 0, "end_char_idx": 65, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe285b24-4ca3-4c1a-80c2-fd9fca7d6d02": {"__data__": {"id_": "fe285b24-4ca3-4c1a-80c2-fd9fca7d6d02", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\64.txt", "file_name": "64.txt", "file_type": "text/plain", "file_size": 1117, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a8bfb97-6ec5-43db-b980-1dc2c1ac6544", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\64.txt", "file_name": "64.txt", "file_type": "text/plain", "file_size": 1117, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e48299979d686f325aa401e3586b152609eecd09893a5b6ecba8f8ea5fe30306", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdc5a975-4138-44dd-8968-f7cd77d8c145", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\63.txt", "file_name": "63.txt", "file_type": "text/plain", "file_size": 68, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ebf54e3c4cbac4bdd591e26cd0f586cd72d030630060ae2c855a6880dc06732a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8cc1d8e-ef60-4ce4-99cb-683bf811dd45", "node_type": "1", "metadata": {}, "hash": "cf050a7f87bb6242e5888037f048ecdefe85165acc05db801610f18551a0ee70", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: kompute\r\nlink: https://github.com/KomputeProject/kompute\r\ndescription: Kompute is a general purpose GPU compute framework for cross vendor graphics cards. It is blazing fast, mobile-enabled, asynchronous, and optimized for advanced GPU acceleration use cases. Kompute is backed by the Linux Foundation as a hosted project by the LF AI & Data Foundation. With Kompute, users can automate any workflow, host and manage packages, find and fix vulnerabilities, and collaborate outside of code. Additionally, users can write better code with AI and fund open source developers. Kompute provides both C++ and Python interfaces, with interactive notebooks and hands-on videos available for both. Its core architecture includes asynchronous and parallel operations, mobile enablement, and more. End-to-end examples are available for both the C++ and Python packages. To get started with Kompute, users can join the Discord & Community Calls, check the documentation, read blog posts, or access examples. Kompute is also open for contributions, and debugging and testing instructions are available for developers.", "start_char_idx": 0, "end_char_idx": 1115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8cc1d8e-ef60-4ce4-99cb-683bf811dd45": {"__data__": {"id_": "e8cc1d8e-ef60-4ce4-99cb-683bf811dd45", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\65.txt", "file_name": "65.txt", "file_type": "text/plain", "file_size": 858, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1bc7ae7c-fd0b-4120-a5a1-44880b1d2f43", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\65.txt", "file_name": "65.txt", "file_type": "text/plain", "file_size": 858, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6ded3a6da1bab161764cfd863e91c256704b901cee22f908a3be8a3d9af7937b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe285b24-4ca3-4c1a-80c2-fd9fca7d6d02", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\64.txt", "file_name": "64.txt", "file_type": "text/plain", "file_size": 1117, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5510adb4ccae87b59d102366a1562e3bd3204c61063e84021e204773c437be62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "027932e1-9599-48cb-9a5c-35a493fe5cc1", "node_type": "1", "metadata": {}, "hash": "7f3baf1a3847b66c3d826ead1ad2ea3247cec52d727d395eb780c3bbef28de9c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: weld\r\nlink: https://github.com/weld-project/weld\r\ndescription: Weld is a language and runtime for improving the performance of data-intensive applications. It optimizes across libraries and functions by expressing the core computations in libraries using a common intermediate representation, and optimizing across each framework. Modern analytics applications combine multiple functions from different libraries and frameworks to build complex workflows. Even though individual functions can achieve high performance in isolation, the performance of the combined workflow is often an order of magnitude below hardware limits due to extensive data movement across the functions. Weld\u2019s take on solving this problem is to lazily build up a computation for the entire workflow, and then optimizing and evaluating it only when a result is needed.", "start_char_idx": 0, "end_char_idx": 854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "027932e1-9599-48cb-9a5c-35a493fe5cc1": {"__data__": {"id_": "027932e1-9599-48cb-9a5c-35a493fe5cc1", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\66.txt", "file_name": "66.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a310b6c-d302-4fda-ba62-15c8d623ed6e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\66.txt", "file_name": "66.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4f2e9d88224aae01959afced768d8a6ffb54760ef8c528efc9fa3ec0e3221f53", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8cc1d8e-ef60-4ce4-99cb-683bf811dd45", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\65.txt", "file_name": "65.txt", "file_type": "text/plain", "file_size": 858, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2cb2ef374b8567c01f4509510765f314a35139525218b406f306817adad76f62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebe20b30-9ae6-4253-adbc-027035bf395b", "node_type": "1", "metadata": {}, "hash": "17003b618f981a44e5dd44abbb980679eac93505cc1a742d47b69c4bbbea53db", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: numba\r\nlink: https://github.com/numba/numba\r\ndescription:", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebe20b30-9ae6-4253-adbc-027035bf395b": {"__data__": {"id_": "ebe20b30-9ae6-4253-adbc-027035bf395b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\67.txt", "file_name": "67.txt", "file_type": "text/plain", "file_size": 1059, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d872e017-c394-4b92-85c9-fed2a7d72483", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\67.txt", "file_name": "67.txt", "file_type": "text/plain", "file_size": 1059, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d19d116267b43ef228c755cb1b498bc4368b31ba156a2283457876f8c36b547d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "027932e1-9599-48cb-9a5c-35a493fe5cc1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\66.txt", "file_name": "66.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ac26338c03f8a0aa67d7f37d73ac326cabf694203872c5b1c27f76fe93fe07e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27452635-9ecc-4605-a4e8-e82428b5ff51", "node_type": "1", "metadata": {}, "hash": "747eb759a4da5e1823dfd881b4f84b36f65897dc678daeba438070deafa1a250", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: numpy-groupies\r\nlink: https://github.com/ml31415/numpy-groupies\r\ndescription: This package consists of a small library of optimized tools for doing things that can roughly be considered \"group-indexing operations\". The most prominent tool is `aggregate`, which is described in detail further down the page. `aggregate` takes an array of values, and an array giving the group number for each of those values. It then returns the sum (or mean, or std, or any,....etc.) of the values in each group. There are multiple implementations of `aggregate` provided. If you use `from.numpy_groupies import aggregate`, the best available implementation will automatically be selected. Otherwise, you can pick a specific version directly. Currently, the following implementations exist: `numpy`, `numba`, `weave`, `cython`. All implementations have the same calling syntax and produce the same outputs, to within some floating-point error. However, some implementations only support a subset of the valid inputs and will sometimes throw `NotImplementedError`.", "start_char_idx": 0, "end_char_idx": 1057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27452635-9ecc-4605-a4e8-e82428b5ff51": {"__data__": {"id_": "27452635-9ecc-4605-a4e8-e82428b5ff51", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f057991d-4465-4fcc-b9d5-4afbf680e892", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5e62ad458742ae6fac9b64bc0afab32e4cd1517b6807db946eca2741e5d1276e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebe20b30-9ae6-4253-adbc-027035bf395b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\67.txt", "file_name": "67.txt", "file_type": "text/plain", "file_size": 1059, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "64f8e43fc29f9b8360ab93b2ad3e0d381f4d62ccb207706adeb47c57f4b756e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "335aebce-6f88-4b1c-a55a-ac2b7c5bba62", "node_type": "1", "metadata": {}, "hash": "256cccd23bb3a398d3572962685a3f0b99567f757bb754c29b340cd3adeba87d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: jax\r\nlink: https://github.com/google/jax\r\ndescription: JAX is Autograd and XLA brought together for high-performance machine learning research. With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives. of derivatives. It supports reverse-mode differentiation (a.k.a..backpropagation) via `grad` as well as forward-mode differentiation, and the two can be composed arbitrarily to any order. What\u2019s new is that JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, `jit`.", "start_char_idx": 0, "end_char_idx": 894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "335aebce-6f88-4b1c-a55a-ac2b7c5bba62": {"__data__": {"id_": "335aebce-6f88-4b1c-a55a-ac2b7c5bba62", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f057991d-4465-4fcc-b9d5-4afbf680e892", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5e62ad458742ae6fac9b64bc0afab32e4cd1517b6807db946eca2741e5d1276e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27452635-9ecc-4605-a4e8-e82428b5ff51", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f1b78d99f2c952d5d9d8784ec79e0d58323ac836c0d33022e83090c519711d79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "616a6886-8d9d-4792-a214-501ce218c32f", "node_type": "1", "metadata": {}, "hash": "f391f8e8ceb1f0e1613ae474cfee57cfd465fbcfdd5ce0a34b6b2eae40116a47", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: jax\r\nlink: https://github.com/google/jax\r\ndescription: JAX is Autograd and XLA brought together for high-performance machine learning research. With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives. of derivatives. It supports reverse-mode differentiation (a.k.a..backpropagation) via `grad` as well as forward-mode differentiation, and the two can be composed arbitrarily to any order. What\u2019s new is that JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, `jit`. Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python.", "start_char_idx": 0, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "616a6886-8d9d-4792-a214-501ce218c32f": {"__data__": {"id_": "616a6886-8d9d-4792-a214-501ce218c32f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f057991d-4465-4fcc-b9d5-4afbf680e892", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5e62ad458742ae6fac9b64bc0afab32e4cd1517b6807db946eca2741e5d1276e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "335aebce-6f88-4b1c-a55a-ac2b7c5bba62", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "086c8d13bfd595e7e86ed189197705ca8c96683a7fde6b076b41771e90810867", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd3ec216-1bd5-4277-8a3b-596ce5856833", "node_type": "1", "metadata": {}, "hash": "5f5dfdc7b1b67deef3d71753efb8d974528967b63e7138a661e6f6b3f16bcf11", "class_name": "RelatedNodeInfo"}}, "text": "With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives. of derivatives. It supports reverse-mode differentiation (a.k.a..backpropagation) via `grad` as well as forward-mode differentiation, and the two can be composed arbitrarily to any order. What\u2019s new is that JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, `jit`. Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python. You can even program multiple GPUs or TPU cores at once using `pmap`, and differentiate through the whole thing.", "start_char_idx": 155, "end_char_idx": 1174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd3ec216-1bd5-4277-8a3b-596ce5856833": {"__data__": {"id_": "bd3ec216-1bd5-4277-8a3b-596ce5856833", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\69.txt", "file_name": "69.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b25d4a08-14cd-4e16-86c0-cb0ad3112787", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\69.txt", "file_name": "69.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f97835debdc007ab10976d8b1687088da18192eac0a2bb70dc5b69a3f1ecaf37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "616a6886-8d9d-4792-a214-501ce218c32f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e376f8191d33c21e3d59ef54bc1bda68e785461499c6cb72b723369ffae01319", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ecd05ea-d5b6-4472-94f7-96b40c20afac", "node_type": "1", "metadata": {}, "hash": "cf57e515d19cea6d347fab08f8b908372dbba7b13b8ef66986c62a0492452cd0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: modin\r\nlink: https://github.com/modin-project/modin\r\ndescription: Modin is a drop-in replacement for pandas that allows you to instantly speed up your workflows by scaling pandas so it uses all of your cores. It works particularly well on larger datasets, where pandas becomes painfully slow or runs out of memory. Modin can handle datasets that pandas cannot, allowing you to work with hundreds of gigabytes of data without worrying about substantial slowdowns or memory errors. Modin can be installed with pip on Linux, Windows, and MacOS, and it automatically detects which engine(s) you have installed and uses that for scheduling computation. You can also choose a specific compute engine to run on by setting the environment variable MODIN_ENGINE.", "start_char_idx": 0, "end_char_idx": 764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ecd05ea-d5b6-4472-94f7-96b40c20afac": {"__data__": {"id_": "6ecd05ea-d5b6-4472-94f7-96b40c20afac", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\7.txt", "file_name": "7.txt", "file_type": "text/plain", "file_size": 777, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f5d66d8-7b45-4e0d-a988-3ef1de75b0db", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\7.txt", "file_name": "7.txt", "file_type": "text/plain", "file_size": 777, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "105a5b7adab090d411c5d88f6f1bb2c347479de480ed65bd31c629d6ad822e98", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd3ec216-1bd5-4277-8a3b-596ce5856833", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\69.txt", "file_name": "69.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "81de5c22e52ef600e8b2ad0f9ddcc9cbd99d5c66cf935cd08508bbac37396df9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbca2355-9e03-4578-86b4-d69eff1bd261", "node_type": "1", "metadata": {}, "hash": "0c44ce1c79dd1c6c27c9a79e19a0de7ada91e9e17867a6cd3366ef4e2026e905", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: go-featureprocessing\r\nlink: https://github.com/nikolaydubina/go-featureprocessing\r\ndescription: go-featureprocessing is a fast and simple sklearn-like feature processing library for Go. It offers high performance on typical use cases, with processing times of around 100ns for a single sample. The library includes a transformer that can be serialized and de-serialized by standard Go routines, making it easy to read, update, and integrate with other tools. Additionally, go-featureprocessing offers a reflection-based version as an alternative to the go:gencode version, although this version comes with added overhead that becomes noticeable for structs with a lot of fields. For more information, including benchmarks and profiling data, see the documentation.", "start_char_idx": 0, "end_char_idx": 775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbca2355-9e03-4578-86b4-d69eff1bd261": {"__data__": {"id_": "cbca2355-9e03-4578-86b4-d69eff1bd261", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\70.txt", "file_name": "70.txt", "file_type": "text/plain", "file_size": 548, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c564ca4-ed6a-4aa7-81e9-4d09d27db662", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\70.txt", "file_name": "70.txt", "file_type": "text/plain", "file_size": 548, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "decaff9be8218d95ed5c12ef5686e8cd21e7c7089d58933146b752321fe80db6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ecd05ea-d5b6-4472-94f7-96b40c20afac", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\7.txt", "file_name": "7.txt", "file_type": "text/plain", "file_size": 777, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7707dc6d42edecd0553aed7d9deb2c2a17d534215fcd67f0925d134f415b226d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd7e5132-590f-454f-8a8e-0fa1c18fcf2b", "node_type": "1", "metadata": {}, "hash": "fb5caf45dac02f9ff27857ff6f74b28b03435b0c63dd03cb257ddaeb6f717168", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cupy\r\nlink: https://github.com/cupy/cupy\r\ndescription: CuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python. CuPy acts as a drop-in replacement to run existing NumPy/SciPy code on NVIDIA CUDA or AMD ROCm platforms. CuPy also provides access to low-level CUDA features such as RawKernels, Streams, and even CUDA Runtime APIs directly. Binary packages are available for Linux and Windows on PyPI and Conda-Forge. CuPy is being developed and maintained by Preferred Networks and community contributors.", "start_char_idx": 0, "end_char_idx": 546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd7e5132-590f-454f-8a8e-0fa1c18fcf2b": {"__data__": {"id_": "dd7e5132-590f-454f-8a8e-0fa1c18fcf2b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\71.txt", "file_name": "71.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ae318a9-5e4c-4570-ba7a-016dc0483cc9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\71.txt", "file_name": "71.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e43f715900cf469d9f215ca91dba276900a21b0c3c63446af36de18b5d6b7395", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbca2355-9e03-4578-86b4-d69eff1bd261", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\70.txt", "file_name": "70.txt", "file_type": "text/plain", "file_size": 548, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "178a28a4406533688b7206b831cd3e566545837428cd025e4aac044f4cbc1a45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19bc7068-b965-4215-964d-47d2ccffd2a3", "node_type": "1", "metadata": {}, "hash": "7fbedd94d05936fe278d66c38a1cef6f5c2150c9b6e37b96a15b0ac55321966d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: h2o-3\r\nlink: https://github.com/h2oai/h2o-3\r\ndescription:", "start_char_idx": 0, "end_char_idx": 68, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "19bc7068-b965-4215-964d-47d2ccffd2a3": {"__data__": {"id_": "19bc7068-b965-4215-964d-47d2ccffd2a3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95c64dc9-e4d0-48a0-a35f-8cbd0295e49a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "085b8fc587d3699b9a67ddc445164e5492d29f4e619ca6ae3184b683a16ca06f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd7e5132-590f-454f-8a8e-0fa1c18fcf2b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\71.txt", "file_name": "71.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "55dde9d4f8b816d5b85c637b6ab37df2d089ccc1ceec9f57fd9c9f08c6d1e15f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d7ce476-afb9-4530-8ec3-60b33d664879", "node_type": "1", "metadata": {}, "hash": "594eb939ef4c1e9ef8472c1a28f4c8b3bdc1d3a6e9fa75ec757f6e9a571d1102", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cuml\r\nlink: https://github.com/rapidsai/cuml\r\ndescription: cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects. cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn. For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents. As an example, the following Python snippet loads input and computes DBSCAN clusters, all on GPU, using cuDF. cuML also features multi-GPU and multi-node-multi-GPU operation, using Dask, for a growing list of algorithms. For additional examples, browse our complete API documentation, or check out our example walkthrough notebooks. Finally, you can find complete end-to-end examples in the notebooks-contrib repo.", "start_char_idx": 0, "end_char_idx": 986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d7ce476-afb9-4530-8ec3-60b33d664879": {"__data__": {"id_": "3d7ce476-afb9-4530-8ec3-60b33d664879", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95c64dc9-e4d0-48a0-a35f-8cbd0295e49a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "085b8fc587d3699b9a67ddc445164e5492d29f4e619ca6ae3184b683a16ca06f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19bc7068-b965-4215-964d-47d2ccffd2a3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "169808f7546eb35363d8c2e50c663aeda8f2d434f572b2f64b1dec714e7f5aab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fbcc06c-8e51-4b83-bfb1-1e97e23d06a4", "node_type": "1", "metadata": {}, "hash": "bbfa656a2ed0107b2534de1d2f61877170a126f50d29cefd00d24e16a6285e8b", "class_name": "RelatedNodeInfo"}}, "text": "cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn. For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents. As an example, the following Python snippet loads input and computes DBSCAN clusters, all on GPU, using cuDF. cuML also features multi-GPU and multi-node-multi-GPU operation, using Dask, for a growing list of algorithms. For additional examples, browse our complete API documentation, or check out our example walkthrough notebooks. Finally, you can find complete end-to-end examples in the notebooks-contrib repo. Please see the RAPIDS Release Selector for the command line to install either nightly or official release cuML packages via Conda or Docker.", "start_char_idx": 235, "end_char_idx": 1127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fbcc06c-8e51-4b83-bfb1-1e97e23d06a4": {"__data__": {"id_": "0fbcc06c-8e51-4b83-bfb1-1e97e23d06a4", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\73.txt", "file_name": "73.txt", "file_type": "text/plain", "file_size": 888, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c0b3bf09-dfcd-49f7-84b0-d4c2d938405c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\73.txt", "file_name": "73.txt", "file_type": "text/plain", "file_size": 888, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fe8e2f79aaefc9d977f3bc52eda89be29ba676713aab1521c2efb15b67acf341", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d7ce476-afb9-4530-8ec3-60b33d664879", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d1c578444d0b8b61443f9057b04274cad5b6db198740c4e4fca450d48a255536", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf5a925b-fb04-4948-bc3b-e31c0cffb13b", "node_type": "1", "metadata": {}, "hash": "bd8266e76b37b0aa8f53c6e2e1f4106ed01778b4866191852e93eae6d49b74ba", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: cudf\r\nlink: https://github.com/rapidsai/cudf\r\ndescription: Built based on the Apache Arrow columnar memory format, cuDF is a GPU.DataFrame library for loading, joining, aggregating, filtering, and otherwise.manipulating data. cuDF provides a pandas-like API that will be familiar to data engineers & data.scientists, so they can use it to easily accelerate their workflows without.going into the details of CUDA programming. For example, the following snippet downloads a CSV, then uses the GPU to parse.it into rows and columns and run calculations. For additional examples, browse our complete API documentation, or check out.our more detailed notebooks. Please see the Demo Docker Repository, choosing a tag based on the NVIDIA CUDA.version you're running. This provides a ready to run Docker container with.example notebooks and data, showcasing how you can utilize cuDF.", "start_char_idx": 0, "end_char_idx": 886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf5a925b-fb04-4948-bc3b-e31c0cffb13b": {"__data__": {"id_": "bf5a925b-fb04-4948-bc3b-e31c0cffb13b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\74.txt", "file_name": "74.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9100a7ef-bb38-4af8-b781-2a2da09b770e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\74.txt", "file_name": "74.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "98a39be0165e9586b825f1a7547809231416d010d15a13c3722ce16c76b79402", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fbcc06c-8e51-4b83-bfb1-1e97e23d06a4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\73.txt", "file_name": "73.txt", "file_type": "text/plain", "file_size": 888, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6fb77ca5597277228d90a09254c750ba8ad20c4dc8cc164bb1396e1d0f603918", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "832789b9-32b8-4748-8740-71a63a7212e3", "node_type": "1", "metadata": {}, "hash": "c8c4bddd984bf13712f7068b3ffe9ac06d5140cfd631b87d48878004526e6901", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: onnx\r\nlink: https://github.com/onnx/onnx\r\ndescription: Open Neural Network Exchange (ONNX) is an open ecosystem that empowers AI developers to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types. Currently we focus on the capabilities needed for inferencing (scoring). ONNX is widely supported and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.", "start_char_idx": 0, "end_char_idx": 797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "832789b9-32b8-4748-8740-71a63a7212e3": {"__data__": {"id_": "832789b9-32b8-4748-8740-71a63a7212e3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\75.txt", "file_name": "75.txt", "file_type": "text/plain", "file_size": 906, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80e1e671-52b4-4881-8395-75c43e1f5873", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\75.txt", "file_name": "75.txt", "file_type": "text/plain", "file_size": 906, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e79d0c611d75f3c8e5869026e2cd6ef3f3198857c60332f36c03b5c86de3c7ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf5a925b-fb04-4948-bc3b-e31c0cffb13b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\74.txt", "file_name": "74.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "581820d72ecf238b33f62cd4a5c1ad5f4a8d58bfb040e91b2a5e12a9200be272", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81b72515-dcd4-4464-bf2f-2684154095ee", "node_type": "1", "metadata": {}, "hash": "0566a8d6b2034a541f5550a4303c780cb7e6820a23620e8d4dd2f98d0d914629", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: sparklyr2pmml\r\nlink: https://github.com/jpmml/sparklyr2pmml\r\ndescription: Sparklyr2PMML is an R library that can convert Apache Spark ML pipelines to PMML. This package is essentially a thin R wrapper for the JPMML-SparkML library, and it comes with several features such as fitting a Spark ML pipeline and exporting it to a PMML file. To use Sparklyr2PMML, it must be paired with JPMML-SparkML based on the provided compatibility matrix. Furthermore, this library is licensed under the terms and conditions of the GNU Affero General Public License, Version 3.0, but it is possible to enter into a licensing agreement if you would like to use it in a proprietary software project. Finally, Sparklyr2PMML is developed and maintained by Openscoring Ltd, Estonia. If you're interested in using Java PMML API software in your company, you can reach out to info@openscoring.io for more information.", "start_char_idx": 0, "end_char_idx": 904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81b72515-dcd4-4464-bf2f-2684154095ee": {"__data__": {"id_": "81b72515-dcd4-4464-bf2f-2684154095ee", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\76.txt", "file_name": "76.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b18e20bb-2c35-4550-a71a-a0eeb94fb2f2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\76.txt", "file_name": "76.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "234fd47801327bab8aadcaa761d6801e1e1e62edf1ce63e744856b2d037054d9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "832789b9-32b8-4748-8740-71a63a7212e3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\75.txt", "file_name": "75.txt", "file_type": "text/plain", "file_size": 906, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9bc34a2d0149ac665a7bbc98807ddbb2a02eb34e5277d2d4d28c90b9e81b1da0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7648935b-7a08-4c30-a23f-d542b50f87ab", "node_type": "1", "metadata": {}, "hash": "3816c4a27c825a8e7edb01a4c12db63f6d111990179fa78ee3a807eee624dbab", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: r2pmml\r\nlink: https://github.com/jpmml/r2pmml\r\ndescription: R2PMML is an R package that allows users to convert R models to PMML. This library is a thin R wrapper around the JPMML-R library. The package offers base functionality, data pre-processing, and advanced functionality for model conversion. Additionally, it provides the option to tweak JVM configuration and to employ a custom converter class. To install R2PMML, users may use the `install.packages(\"r2pmml\")` command for a release version from CRAN or install the latest snapshot version from GitHub using the `devtools` package. For documentation and license information, please refer to the R2PMML package page.", "start_char_idx": 0, "end_char_idx": 685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7648935b-7a08-4c30-a23f-d542b50f87ab": {"__data__": {"id_": "7648935b-7a08-4c30-a23f-d542b50f87ab", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\77.txt", "file_name": "77.txt", "file_type": "text/plain", "file_size": 770, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f76d1e22-b47b-4e06-a0b2-7b42081c8e69", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\77.txt", "file_name": "77.txt", "file_type": "text/plain", "file_size": 770, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1efe45a7cfb7a8e0ed322d9eda7bab76c72cb5e48579b824bc08013cacc27eb5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81b72515-dcd4-4464-bf2f-2684154095ee", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\76.txt", "file_name": "76.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "60d653548d921ab015eb339b4a982d6df64f50839e236a4b55ea31aa8640ac73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71beb6e9-5333-428f-bea2-84a500615725", "node_type": "1", "metadata": {}, "hash": "30935f220d5cffbf3d6eb3bb4dd0bc3821b75839fc4aa24009ca813504237421", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: jpmml-sklearn\r\nlink: https://github.com/jpmml/jpmml-sklearn\r\ndescription: JPMML-SkLearn is a Java library and command-line application for converting Scikit-Learn pipelines to PMML. It supports various packages including mlxtend, category_encoders, h2o, imblearn, lightgbm, pycaret, sklego, sklearn2pmml, statsmodels, and tpot. The workflow involves loading data to a pandas.DataFrame object, creating feature engineering and selection objects, creating an Estimator object, combining the objects into a PMMLPipeline, and running the experiment. JPMML-SkLearn is licensed under the GNU Affero General Public License, but a licensing agreement is available to use it under the BSD 3-Clause License. It is developed and maintained by Openscoring Ltd, Estonia.", "start_char_idx": 0, "end_char_idx": 768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71beb6e9-5333-428f-bea2-84a500615725": {"__data__": {"id_": "71beb6e9-5333-428f-bea2-84a500615725", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\78.txt", "file_name": "78.txt", "file_type": "text/plain", "file_size": 884, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01a714d9-e060-4d29-908d-1e12c2ef9f19", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\78.txt", "file_name": "78.txt", "file_type": "text/plain", "file_size": 884, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e56fa4c6a85b2c5327565fffcaad2a3a77a92f986f7cd75cf005eefc76c12401", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7648935b-7a08-4c30-a23f-d542b50f87ab", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\77.txt", "file_name": "77.txt", "file_type": "text/plain", "file_size": 770, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a8b936102500876188e26eccf4f07846965269912864f9757200139378ada630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1591a5ac-8621-4e2f-a312-4f2beb74fa2f", "node_type": "1", "metadata": {}, "hash": "8bcc32b847acd171d12fe585e972d616d32d4ec1029c146dee5cd10091a3c027", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: pyspark2pmml\r\nlink: https://github.com/jpmml/pyspark2pmml\r\ndescription: PySpark2PMML is a Python package that allows users to convert Apache Spark ML pipelines to PMML. This package is a thin PySpark wrapper for the JPMML-SparkML library. Users can install a release version from PyPI using pip install pyspark2pmml or alternatively, install the latest snapshot version from GitHub with pip install --upgrade git+https://github.com/jpmml/pyspark2pmml.git. Pairing PySpark2PMML with JPMML-SparkML requires a specific compatibility matrix, and individual stages can be customized through conversion options. PySpark2PMML is licensed under the GNU Affero General Public License, Version 3.0, but proprietary software projects can enter into a licensing agreement to use PySpark2PMML under the BSD 3-Clause License. It is developed and maintained by Openscoring Ltd, Estonia.", "start_char_idx": 0, "end_char_idx": 882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1591a5ac-8621-4e2f-a312-4f2beb74fa2f": {"__data__": {"id_": "1591a5ac-8621-4e2f-a312-4f2beb74fa2f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\79.txt", "file_name": "79.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2230f129-480d-4a25-8cbd-5f93a11c60c1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\79.txt", "file_name": "79.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8f3fb664e5b1ed43e9c21cbac5e52c59df69cbee931c7f2ec80627ced081316e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71beb6e9-5333-428f-bea2-84a500615725", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\78.txt", "file_name": "78.txt", "file_type": "text/plain", "file_size": 884, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "626a97e285a99524d64289573d2b9bb7fcc75476c2790dd6580efae829ff1f0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab46a04b-cfe3-4df6-bbd2-fa6288e9dee0", "node_type": "1", "metadata": {}, "hash": "0795004650064cc9655806e58ef0ab8de9eefd9d98af89fb887b38988fe809c3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: MMdnn\r\nlink: https://github.com/microsoft/MMdnn\r\ndescription: MMdnn is a comprehensive and cross-framework tool to convert, visualize and diagnose deep learning (DL) models. The \"MM\" stands for model management, and \"dnn\" is the acronym of deep neural network. Major features include model conversion, guidelines to help deploy DL models to another hardware platform, and a model converter to help developers convert models between frameworks through an intermediate representation format. MMdnn provides a local visualizer to display the network architecture of a deep learning model, and there are currently tutorials available to convert models between Keras, TensorFlow, Caffe, MXNet, and more. MMdnn also provides a docker image for easy installation and use. Contributions are welcome from researchers, developers, and students looking to improve DL productivity.", "start_char_idx": 0, "end_char_idx": 880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab46a04b-cfe3-4df6-bbd2-fa6288e9dee0": {"__data__": {"id_": "ab46a04b-cfe3-4df6-bbd2-fa6288e9dee0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\8.txt", "file_name": "8.txt", "file_type": "text/plain", "file_size": 200, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2878648d-faea-4d33-984f-71bb6b8d2481", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\8.txt", "file_name": "8.txt", "file_type": "text/plain", "file_size": 200, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "deefed50d4601eaeb058f9d2a9de839354af4f2c0b8a740fb76b5c16ce0c6905", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1591a5ac-8621-4e2f-a312-4f2beb74fa2f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\79.txt", "file_name": "79.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "83dc35beb271d1aaf6e24f2db7033d6c67e798fdcdaec63f029df4fe667ebb41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8b84100-1703-4246-828f-dc6f642bf091", "node_type": "1", "metadata": {}, "hash": "455f9fb047bf2d52c307f5b7d218a9d50a361fef7d56027c8fc9d1fab0514bd9", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: featuretools\r\nlink: https://github.com/alteryx/featuretools\r\ndescription: 'Featuretools is a python library for automated feature engineering. See the.documentation for more information.'", "start_char_idx": 0, "end_char_idx": 198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8b84100-1703-4246-828f-dc6f642bf091": {"__data__": {"id_": "c8b84100-1703-4246-828f-dc6f642bf091", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\80.txt", "file_name": "80.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f7bb37f-c1b1-49d1-addc-c75f8024b6f3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\80.txt", "file_name": "80.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "10d05d905dec910fd85838a6fe58be899b0806354b1aa4a6fe2cd7caf2ccbed1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab46a04b-cfe3-4df6-bbd2-fa6288e9dee0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\8.txt", "file_name": "8.txt", "file_type": "text/plain", "file_size": 200, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9b6978f76458622284c1afb925101edcee8bf8dc5ff4ae4c53e2155ac8353fbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bd1fadb-3633-4415-984f-a37f69b7b86b", "node_type": "1", "metadata": {}, "hash": "c3e443754ffb8354a3b06b31f956bf3618f8e833f06d72103969f028d1408217", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: ray\r\nlink: https://github.com/ray-project/ray\r\ndescription:", "start_char_idx": 0, "end_char_idx": 70, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bd1fadb-3633-4415-984f-a37f69b7b86b": {"__data__": {"id_": "6bd1fadb-3633-4415-984f-a37f69b7b86b", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\81.txt", "file_name": "81.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8e4cac6-b7dc-4a85-aef4-f4f2dcbc7251", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\81.txt", "file_name": "81.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "b0fc17c0cff17bbda44d971ff18c2ddad29814e6fdf2eb349c080aa400746722", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8b84100-1703-4246-828f-dc6f642bf091", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\80.txt", "file_name": "80.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c5b8c719555ec4adc1c7cc6ea081b1e20ceb1a7c98de3ae149463ec936bfb910", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0847261c-16c9-4f84-9390-0b187f15d5b3", "node_type": "1", "metadata": {}, "hash": "100091670b830aa7b168711a4a96f0db3028902514c1940cf37de3ea4f007a8b", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: vespa\r\nlink: https://github.com/vespa-engine/vespa\r\ndescription: This is the primary repository for Vespa where all development is happening. Vespa is a platform used on a number of large internet services and apps which serve hundreds of thousands of queries from Vespa per second. Use cases such as search, recommendation, and personalization need to select a subset of data in a large corpus, evaluate machine-learned models over the selected data, organize and aggregate it, and return it, typically in less than 100 milliseconds, all while the data corpus is continuously changing. Run your own Vespa instance or deploy your Vespa applications to the cloud service. Full documentation is at https://docs.vespa.ai.", "start_char_idx": 0, "end_char_idx": 729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0847261c-16c9-4f84-9390-0b187f15d5b3": {"__data__": {"id_": "0847261c-16c9-4f84-9390-0b187f15d5b3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d19ab6d-0912-46f6-8a4d-310f45cc45cb", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bb1a8367883e15288b50c27077470b336f0beacfcdac38cf2d9dd5278ad01621", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bd1fadb-3633-4415-984f-a37f69b7b86b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\81.txt", "file_name": "81.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bb483df94ee3b589dfc97697b9eff033934eeda24246575bb729dd0824a253c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fe7a9ad-7333-4c02-b0a8-3f326e351a47", "node_type": "1", "metadata": {}, "hash": "376f7a3a6b7e487eb029ce3665a678995fca23b2d9b1d9f107a4227e3962e1c0", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lightning\r\nlink: https://github.com/Lightning-AI/lightning\r\ndescription: **The Deep Learning framework to train, deploy, and ship AI products Lightning.fast.** \r\n\r\nIntroducing Lightning, an AI framework that automates any workflow, hosts and manages packages, finds and fixes vulnerabilities, provides instant dev environments, allows collaboration outside of code, funds open source developers, and offers GitHub community articles. This lightning-fast framework helps in writing better code with AI. There are no suggested jumps to results; however, Lightning has three core packages: PyTorch Lightning, Lightning Fabric, and Lightning Apps. These packages allow for granular control over how much abstraction one wants to add over PyTorch. PyTorch Lightning trains and deploys PyTorch at scale and has over 40 advanced features designed for professional AI research at scale.", "start_char_idx": 0, "end_char_idx": 889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fe7a9ad-7333-4c02-b0a8-3f326e351a47": {"__data__": {"id_": "3fe7a9ad-7333-4c02-b0a8-3f326e351a47", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d19ab6d-0912-46f6-8a4d-310f45cc45cb", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bb1a8367883e15288b50c27077470b336f0beacfcdac38cf2d9dd5278ad01621", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0847261c-16c9-4f84-9390-0b187f15d5b3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c52335ce5241774298e2eb9af534afa7aa0ccc05c52d565a108bd8d8b2abcae1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ec304d0-3da3-4f69-8c06-81a7c0e073bf", "node_type": "1", "metadata": {}, "hash": "54452ec8aa6cac6847408eaf1bafc36372a5480b23f64c43520016d0b9d7937d", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: lightning\r\nlink: https://github.com/Lightning-AI/lightning\r\ndescription: **The Deep Learning framework to train, deploy, and ship AI products Lightning.fast.** \r\n\r\nIntroducing Lightning, an AI framework that automates any workflow, hosts and manages packages, finds and fixes vulnerabilities, provides instant dev environments, allows collaboration outside of code, funds open source developers, and offers GitHub community articles. This lightning-fast framework helps in writing better code with AI. There are no suggested jumps to results; however, Lightning has three core packages: PyTorch Lightning, Lightning Fabric, and Lightning Apps. These packages allow for granular control over how much abstraction one wants to add over PyTorch. PyTorch Lightning trains and deploys PyTorch at scale and has over 40 advanced features designed for professional AI research at scale. Lightning Fabric offers expert-level control over PyTorch training loop and scaling strategy and is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning, of any size.", "start_char_idx": 0, "end_char_idx": 1142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ec304d0-3da3-4f69-8c06-81a7c0e073bf": {"__data__": {"id_": "8ec304d0-3da3-4f69-8c06-81a7c0e073bf", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d19ab6d-0912-46f6-8a4d-310f45cc45cb", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "bb1a8367883e15288b50c27077470b336f0beacfcdac38cf2d9dd5278ad01621", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fe7a9ad-7333-4c02-b0a8-3f326e351a47", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c5264be063c2bb4a872567db87a6c6bb3d8de46414c7261b31503953ee70a322", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3acd92b5-55a2-47f0-b82f-937fa5d93cc2", "node_type": "1", "metadata": {}, "hash": "51900b51edb34d33c2bd8ea55503552ce8ecb3165738c9053c7a0b28095205e8", "class_name": "RelatedNodeInfo"}}, "text": "** \r\n\r\nIntroducing Lightning, an AI framework that automates any workflow, hosts and manages packages, finds and fixes vulnerabilities, provides instant dev environments, allows collaboration outside of code, funds open source developers, and offers GitHub community articles. This lightning-fast framework helps in writing better code with AI. There are no suggested jumps to results; however, Lightning has three core packages: PyTorch Lightning, Lightning Fabric, and Lightning Apps. These packages allow for granular control over how much abstraction one wants to add over PyTorch. PyTorch Lightning trains and deploys PyTorch at scale and has over 40 advanced features designed for professional AI research at scale. Lightning Fabric offers expert-level control over PyTorch training loop and scaling strategy and is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning, of any size. Lightning Apps allow for the building of AI products and ML workflows, removing cloud infrastructure boilerplate so users can focus on solving the research or business problems.", "start_char_idx": 168, "end_char_idx": 1320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3acd92b5-55a2-47f0-b82f-937fa5d93cc2": {"__data__": {"id_": "3acd92b5-55a2-47f0-b82f-937fa5d93cc2", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\83.txt", "file_name": "83.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52dec9ab-f3e4-4763-9cd8-d383f66aedec", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\83.txt", "file_name": "83.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1dfb7bb9121a3c95768f3315cd18305425f64b5b54c9c3e675e6fc73b3329ded", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ec304d0-3da3-4f69-8c06-81a7c0e073bf", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "840087bb7424fe692da431a77f5840b4880bf34f9ed047483516571a7cedb8c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82e7b984-c674-4519-b6f3-ec26b5ef15b0", "node_type": "1", "metadata": {}, "hash": "aaa5c0f1ea62278ae2f8ccb6f930f846a2666bd2dddbe8e11fa593862c6531b1", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: horovod\r\nlink: https://github.com/horovod/horovod\r\ndescription:", "start_char_idx": 0, "end_char_idx": 74, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82e7b984-c674-4519-b6f3-ec26b5ef15b0": {"__data__": {"id_": "82e7b984-c674-4519-b6f3-ec26b5ef15b0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\84.txt", "file_name": "84.txt", "file_type": "text/plain", "file_size": 713, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1474d637-b80e-4c6c-8a72-79247cdb139c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\84.txt", "file_name": "84.txt", "file_type": "text/plain", "file_size": 713, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "fade07b9efebb9941806db64a252cec419c8345da7f4bd41746011d99eb280b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3acd92b5-55a2-47f0-b82f-937fa5d93cc2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\83.txt", "file_name": "83.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0e2d3590e8bc6713b8ae7144faab959f34a059a752815c27859109cc8ac8ac83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adb003e6-404a-42b7-acd9-8091e0915c89", "node_type": "1", "metadata": {}, "hash": "1fd8853aa178e2dd54460802e21528c18390d7d52d9523aea29b6f4ca8c7fee3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: numpywren\r\nlink: https://github.com/Vaishaal/numpywren\r\ndescription: numpywren is a scientific computing framework built on top of the serverless execution framework pywren. numpywren forgoes the traditional mpi.computational model for scientific computing workloads. Instead of dealing.with individual machines, host names, and processor grids numpywren works on.the abstraction of \"cores\" and \"memory\". numpywren currently uses Amazon EC2.and Lambda services for computation, and use Amazon S3 as a distributed memory.abstraction. Even with this coarse abstraction, numpywren can achieve close to.peak FLOPS and network IO for difficult workloads such as matrix multiply and.cholesky decomposition.", "start_char_idx": 0, "end_char_idx": 711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adb003e6-404a-42b7-acd9-8091e0915c89": {"__data__": {"id_": "adb003e6-404a-42b7-acd9-8091e0915c89", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\85.txt", "file_name": "85.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2b741a4-1417-44b2-a4cd-1e0b040c3b87", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\85.txt", "file_name": "85.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c619e9e93c23be44eb5463d75af5dcc7c92e3354b338d9243313917e30c5b97e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82e7b984-c674-4519-b6f3-ec26b5ef15b0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\84.txt", "file_name": "84.txt", "file_type": "text/plain", "file_size": 713, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "018c7dfabd6db9ee672bbeb9eba8532812ec960a2ace2b9a6afe3dc457798a04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b8b6c74-f610-4a30-9510-b50dce0e6ad0", "node_type": "1", "metadata": {}, "hash": "0478c1006e90542be78fb5750d12c5df783359645208d812fb5eb6942ae0d6eb", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: fiber\r\nlink: https://github.com/uber/fiber\r\ndescription: Fiber is a Python distributed computing library for modern computer clusters. Originally, it was developed to power large scale parallel scientific computation projects like POET and it has been used to power similar projects within Uber. Fiber implements most of multiprocessing's API including `Process`, `SimpleQueue`, `Pool`, `Pipe`, `Manager` and it has its own extension to the multiprocessing's API to make it easy to compose large scale distributed applications. For the detailed API guild, check out here.", "start_char_idx": 0, "end_char_idx": 582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b8b6c74-f610-4a30-9510-b50dce0e6ad0": {"__data__": {"id_": "2b8b6c74-f610-4a30-9510-b50dce0e6ad0", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\86.txt", "file_name": "86.txt", "file_type": "text/plain", "file_size": 1078, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ba841022-58ec-44ca-a43b-39961be87b33", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\86.txt", "file_name": "86.txt", "file_type": "text/plain", "file_size": 1078, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "92b8e0557ebc4c00b479c5665f9134c232267c6efbc17fa3b1f3590eb84d1145", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adb003e6-404a-42b7-acd9-8091e0915c89", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\85.txt", "file_name": "85.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6a733bd00accf1d3d0cd371f8f96831c531bd453c2c52dbfdbc0705978ba5a85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "005d0d86-2d03-48a7-b96d-b0a1e3ec0464", "node_type": "1", "metadata": {}, "hash": "65443d617e27060b2301a6072cce337251dadd7f669194e773f53e9196eaa751", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: DeepSpeed\r\nlink: https://github.com/microsoft/DeepSpeed\r\ndescription: DeepSpeed is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. It offers extreme speed and scale for DL training and inference and empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales. DeepSpeed's three innovation pillars include DeepSpeed-Training, DeepSpeed-Inference, and DeepSpeed-Compression. The DeepSpeed library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for easy composition of multitude of features within a single training, inference or compression pipeline and is heavily adopted by the DL community. DeepSpeed has been used to train many different large-scale models, and it has been integrated with several different popular open-source DL frameworks.", "start_char_idx": 0, "end_char_idx": 1076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "005d0d86-2d03-48a7-b96d-b0a1e3ec0464": {"__data__": {"id_": "005d0d86-2d03-48a7-b96d-b0a1e3ec0464", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\87.txt", "file_name": "87.txt", "file_type": "text/plain", "file_size": 586, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84ef711b-781e-425b-9aa4-c5587f9b95da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\87.txt", "file_name": "87.txt", "file_type": "text/plain", "file_size": 586, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5775ef6e775d3fbe75b56710d94472f03938b66067b068872f5679e16ccdb6b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b8b6c74-f610-4a30-9510-b50dce0e6ad0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\86.txt", "file_name": "86.txt", "file_type": "text/plain", "file_size": 1078, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "d5f171abfb96ef4584b89e025f4a0cc922dd992fbb602765bdd1ed13ecfafa45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fac5f96-bcb2-49c7-8c8a-590981146891", "node_type": "1", "metadata": {}, "hash": "f81d1656b8a7703a7c776b1bfc0f9f6590d02cf4a8664141bbfb0b9d3a8c5f24", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: deap\r\nlink: https://github.com/DEAP/deap\r\ndescription: DEAP is a novel evolutionary computation framework for rapid prototyping and testing of ideas. It seeks to make algorithms explicit and data structures transparent. It works in perfect harmony with parallelisation mechanisms such as multiprocessing and SCOOP. DEAP includes the following features: Automate any workflow, Host and manage packages, Find and fix vulnerabilities, Instant dev environments, Write better code with AI, Collaborate outside of code, Fund open source developers, and GitHub community articles.", "start_char_idx": 0, "end_char_idx": 584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fac5f96-bcb2-49c7-8c8a-590981146891": {"__data__": {"id_": "2fac5f96-bcb2-49c7-8c8a-590981146891", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\88.txt", "file_name": "88.txt", "file_type": "text/plain", "file_size": 864, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e6eac33-6916-41b4-8be7-1a725f838684", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\88.txt", "file_name": "88.txt", "file_type": "text/plain", "file_size": 864, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "85168363ac67b2cfaa9800adcadd48c56ba0661d984adbdfa72dffd470b46eb7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "005d0d86-2d03-48a7-b96d-b0a1e3ec0464", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\87.txt", "file_name": "87.txt", "file_type": "text/plain", "file_size": 586, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "25b22840f8355975e1ae044b309ce5d5128918321cd087b84484f505aa1e9e20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4b5b2e3-99fd-4527-a94c-a983ed5225e3", "node_type": "1", "metadata": {}, "hash": "6bc5bbc6d2938e8cf493748be246e4e1dfd7acd2a345b95e9ebfb3a05f5ed71c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: BigDL\r\nlink: https://github.com/intel-analytics/BigDL\r\ndescription: BigDL is a distributed deep learning library for Apache Spark; with BigDL, users can write their deep learning applications as standard Spark programs, which can directly run on top of existing Spark or Hadoop clusters. BigDL provides rich deep learning support, high performance, and efficient scaling-out capabilities by leveraging Spark's lightning-fast distributed data processing framework. It is ideal to use BigDL when analyzing significant amounts of data on the same Big Data (Hadoop/Spark) cluster where the data is stored or when adding deep learning functionalities to existing Spark programs and/or workflows. It is highly recommended to use the high-level APIs provided by Analytics Zoo. If you've found BigDL useful for your project, you may cite the paper as follows.", "start_char_idx": 0, "end_char_idx": 862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4b5b2e3-99fd-4527-a94c-a983ed5225e3": {"__data__": {"id_": "c4b5b2e3-99fd-4527-a94c-a983ed5225e3", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\89.txt", "file_name": "89.txt", "file_type": "text/plain", "file_size": 393, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3ee54c0-04da-45f1-9329-437b41468049", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\89.txt", "file_name": "89.txt", "file_type": "text/plain", "file_size": 393, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "24021605fe1e4f782a5fa15fd0dc972cfab7add7387d0dd271f0bff4201e3b66", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fac5f96-bcb2-49c7-8c8a-590981146891", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\88.txt", "file_name": "88.txt", "file_type": "text/plain", "file_size": 864, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ba6cfb5eaf2272122c60d1bf09a1911e6bf65f218e9c2e31317e08f71c3ded73", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24a13d41-d222-45da-8048-f5b2359dd043", "node_type": "1", "metadata": {}, "hash": "9d989716984eb832a56a30f632cdd3281a15a0c19f15403c6ef8ad07a6f557e1", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: beam\r\nlink: https://github.com/apache/beam\r\ndescription: \"Apache Beam is a unified model for defining both batch and streaming data-parallel processing pipelines, as well as a set of language-specific SDKs for constructing pipelines and Runners for executing them on distributed processing backends, including Apache Flink, Apache Spark, Google Cloud Dataflow, and Hazelcast Jet.\"", "start_char_idx": 0, "end_char_idx": 391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24a13d41-d222-45da-8048-f5b2359dd043": {"__data__": {"id_": "24a13d41-d222-45da-8048-f5b2359dd043", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\9.txt", "file_name": "9.txt", "file_type": "text/plain", "file_size": 423, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b1fd6b4-7d4e-41fa-ad42-a9fbe68f7bf2", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\9.txt", "file_name": "9.txt", "file_type": "text/plain", "file_size": 423, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "48a9aba303061528599be8ec3eb2f1057ea4f12745a99b48fa8747dda84ff496", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4b5b2e3-99fd-4527-a94c-a983ed5225e3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\89.txt", "file_name": "89.txt", "file_type": "text/plain", "file_size": 393, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "dbc1e3f3767a789fa3549ce2de86ffab38105c120498855db6fcaa317cd32346", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c78bb835-c426-49e4-86e1-2fde5db2e31f", "node_type": "1", "metadata": {}, "hash": "c01ec4247dd5e541cf156c3b7c291d0bf80b4b31ce1ad92f1550b7a9e6bcbcc3", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: auto-sklearn\r\nlink: https://github.com/automl/auto-sklearn\r\ndescription: **auto-sklearn** is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator. Find the documentation **here**. Quick links: ## auto-sklearn in one image, ## auto-sklearn in four lines of code, ## Relevant publications. If you use auto-sklearn in scientific publications, we would appreciate citations.", "start_char_idx": 0, "end_char_idx": 421, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c78bb835-c426-49e4-86e1-2fde5db2e31f": {"__data__": {"id_": "c78bb835-c426-49e4-86e1-2fde5db2e31f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\90.txt", "file_name": "90.txt", "file_type": "text/plain", "file_size": 815, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30dcdd05-eeb3-4645-a18c-d36473030b1c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\90.txt", "file_name": "90.txt", "file_type": "text/plain", "file_size": 815, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "1a342d0fa48ec543fb223686bd1aca6f7f405ce6a5036cdb7b7336f4dbb30a51", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24a13d41-d222-45da-8048-f5b2359dd043", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\9.txt", "file_name": "9.txt", "file_type": "text/plain", "file_size": 423, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9baf7acdd9fd40e7b657933f7eb6452fe1f169d6da524840a2d501d0660e8b1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9efbbbe5-3dbd-43aa-bf38-e07d6846062e", "node_type": "1", "metadata": {}, "hash": "12ae89ac5244b5ac617e821b10332e80286dec5500e56a55a0b87f18ddf7225e", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: analytics-zoo\r\nlink: https://github.com/intel-analytics/analytics-zoo\r\ndescription: Analytics Zoo is an open source Big Data AI platform that includes features for scaling end-to-end AI to distributed Big Data, such as Orca for seamless scaling of TensorFlow and PyTorch, RayOnSpark for running Ray programs directly on Big Data clusters, BigDL Extensions for high-level Spark ML pipeline and Keras-like APIs, Chronos for scalable time series analysis using AutoML, and PPML for privacy preserving big data analysis and machine learning (experimental). More information and documentation can be found on their website, and Analytics Zoo can also be used on Google Colab without any installation. To install Analytics Zoo, it is recommended to use conda environments or the latest nightly build via pip.", "start_char_idx": 0, "end_char_idx": 813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9efbbbe5-3dbd-43aa-bf38-e07d6846062e": {"__data__": {"id_": "9efbbbe5-3dbd-43aa-bf38-e07d6846062e", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\91.txt", "file_name": "91.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1cea5fff-9f8f-46de-b7d9-43f9d831eefd", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\91.txt", "file_name": "91.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "9604473f7066d3d80b2b0da4c1c8236f34bbc31c624cbb5be983d9730630f663", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c78bb835-c426-49e4-86e1-2fde5db2e31f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\90.txt", "file_name": "90.txt", "file_type": "text/plain", "file_size": 815, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "e03b633e006472c078b6a705322212b8f4a58dae1499942b240559df82dbdff4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62895b8b-0db8-448e-bb80-6ba652d21441", "node_type": "1", "metadata": {}, "hash": "5ea93c46b5c5557dca908903f751bd85cded5b5b76fce8d4ce588e27bd96e15c", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: nuclio\r\nlink: https://github.com/nuclio/nuclio\r\ndescription: Nuclio is a high-performance \"serverless\" framework focused on data, I/O, and compute intensive workloads. It is well integrated with popular data science tools, such as Jupyter and Kubeflow; supports a variety of data and streaming sources; and supports execution over CPUs and GPUs. None of the existing cloud and open-source serverless solutions address all the desired capabilities of a serverless framework, which is why Nuclio was created to fulfill these requirements. It is an extendable open-source framework, using a modular and layered approach that supports constant addition of triggers and runtimes, with the hope that many will join the effort of developing new modules, developer tools, and platforms for Nuclio.", "start_char_idx": 0, "end_char_idx": 800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62895b8b-0db8-448e-bb80-6ba652d21441": {"__data__": {"id_": "62895b8b-0db8-448e-bb80-6ba652d21441", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\92.txt", "file_name": "92.txt", "file_type": "text/plain", "file_size": 349, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1951614-9d46-45d6-9977-1315fb47372e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\92.txt", "file_name": "92.txt", "file_type": "text/plain", "file_size": 349, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "c203c4b3ea860298aaa11b06f7d91a3c3ba11d8ab125bfa6dca80b7aaefb2ac2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9efbbbe5-3dbd-43aa-bf38-e07d6846062e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\91.txt", "file_name": "91.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "4ee7188b450dc44307157ffe110aa5820925ce7ed253f7349950db3aab99a894", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac3585b6-c617-469b-8f97-918b88a25e0d", "node_type": "1", "metadata": {}, "hash": "102aed0f359a8f2d4260cdafcd8f4fde4634a7f8b05718aa269ce96df13e7975", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: faas\r\nlink: https://github.com/openfaas/faas\r\ndescription: OpenFaaS\u00ae makes it easy for developers to deploy event-driven functions and microservices to Kubernetes without repetitive, boiler-plate coding. Package your code or an existing binary in an OCI-compatible image to get a highly scalable endpoint with auto-scaling and metrics.", "start_char_idx": 0, "end_char_idx": 346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac3585b6-c617-469b-8f97-918b88a25e0d": {"__data__": {"id_": "ac3585b6-c617-469b-8f97-918b88a25e0d", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9f7c5fa-4284-407e-b043-f602c5f22629", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7ed48a38337f6f81524bc0ed3c1bef31b55e7027a1c6be23cb52a58e608c811d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62895b8b-0db8-448e-bb80-6ba652d21441", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\92.txt", "file_name": "92.txt", "file_type": "text/plain", "file_size": 349, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "78c8ce5d08bc457aaa006e7980c63448f2464eebe3741f4d967ad037318f9cd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49d3f981-e034-4fb0-aa02-7ea418171032", "node_type": "1", "metadata": {}, "hash": "63374bf351dd78cab9c5adcea5fc5e5c16a741bfdcf18abbbb19ea223d4148b6", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: hydro-serving\r\nlink: https://github.com/Hydrospheredata/hydro-serving\r\ndescription: Hydrosphere Serving is a cluster for deploying and versioning your machine learning models in production. You can refer to our documentation to see tutorials, check out example projects, and learn about all features of Hydrosphere. There are two main ways of installing Hydrosphere. Before installing Hydrosphere Serving, please install its prerequisites. To install Hydrosphere Serving, follow the instructions below. To reach the cluster, port-forward `ui` service locally. Kubectl port-forward -n hydrosphere svc/serving-ui 8080:9090. To check installation, open http://localhost:8080/. **Note**, other installation options are described in the documentation.", "start_char_idx": 0, "end_char_idx": 757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49d3f981-e034-4fb0-aa02-7ea418171032": {"__data__": {"id_": "49d3f981-e034-4fb0-aa02-7ea418171032", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9f7c5fa-4284-407e-b043-f602c5f22629", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7ed48a38337f6f81524bc0ed3c1bef31b55e7027a1c6be23cb52a58e608c811d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac3585b6-c617-469b-8f97-918b88a25e0d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ebb83b25b943a884f10df245a0bd0b6cfb0decc8f1fc90060044f24fcde4aac9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7962e109-337f-40be-b319-4d3b447d5807", "node_type": "1", "metadata": {}, "hash": "118ad0eb3b32f687e19fdbc61a0a99b58d8add7f18e131b6a2aa3472fd36df92", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: hydro-serving\r\nlink: https://github.com/Hydrospheredata/hydro-serving\r\ndescription: Hydrosphere Serving is a cluster for deploying and versioning your machine learning models in production. You can refer to our documentation to see tutorials, check out example projects, and learn about all features of Hydrosphere. There are two main ways of installing Hydrosphere. Before installing Hydrosphere Serving, please install its prerequisites. To install Hydrosphere Serving, follow the instructions below. To reach the cluster, port-forward `ui` service locally. Kubectl port-forward -n hydrosphere svc/serving-ui 8080:9090. To check installation, open http://localhost:8080/. **Note**, other installation options are described in the documentation. Keep up to date and get Hydrosphere.io support via.![](https://camo.githubusercontent.com/19305dc0aa2cbd8d35ed469bc2ea4acffb48743675982df3788e717dd998d3ba/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230736c61636b2d253233453031453541).", "start_char_idx": 0, "end_char_idx": 1023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7962e109-337f-40be-b319-4d3b447d5807": {"__data__": {"id_": "7962e109-337f-40be-b319-4d3b447d5807", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9f7c5fa-4284-407e-b043-f602c5f22629", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7ed48a38337f6f81524bc0ed3c1bef31b55e7027a1c6be23cb52a58e608c811d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49d3f981-e034-4fb0-aa02-7ea418171032", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "01cbbe2aec327756a051b02ab49363a61e7fc70400a015d02995aaab13c16992", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d790a79-cff2-4554-8f83-919bbb75db5a", "node_type": "1", "metadata": {}, "hash": "aaf0f907ec7feb2488722f42d9a9a68e2dcad346d9413528fe60b53a0f96e840", "class_name": "RelatedNodeInfo"}}, "text": "Before installing Hydrosphere Serving, please install its prerequisites. To install Hydrosphere Serving, follow the instructions below. To reach the cluster, port-forward `ui` service locally. Kubectl port-forward -n hydrosphere svc/serving-ui 8080:9090. To check installation, open http://localhost:8080/. **Note**, other installation options are described in the documentation. Keep up to date and get Hydrosphere.io support via.![](https://camo.githubusercontent.com/19305dc0aa2cbd8d35ed469bc2ea4acffb48743675982df3788e717dd998d3ba/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230736c61636b2d253233453031453541). or contact us directly at info@hydrosphere.io.", "start_char_idx": 378, "end_char_idx": 1070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d790a79-cff2-4554-8f83-919bbb75db5a": {"__data__": {"id_": "3d790a79-cff2-4554-8f83-919bbb75db5a", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\94.txt", "file_name": "94.txt", "file_type": "text/plain", "file_size": 202, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0d3428d-f1d4-4954-9770-b1acb9f38528", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\94.txt", "file_name": "94.txt", "file_type": "text/plain", "file_size": 202, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "5a15b0f8801a8a76f60d123331a928697a9fb3f42c7e787a863f0adff338d6c8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7962e109-337f-40be-b319-4d3b447d5807", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "6e3abf23ab58f3d2fa80bc6b8072ae41d9d08315d4e22ad8589d4403b70134d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee1f883c-a7ed-404f-8177-d4360d4ac04c", "node_type": "1", "metadata": {}, "hash": "e24c896f298868b9cd25cb8b1b1410ef961fc272a1ea8c2adf509b263b9106f5", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: serving\r\nlink: https://github.com/knative/serving\r\ndescription: Knative Serving builds on Kubernetes to support deploying and serving of applications and functions as serverless containers.", "start_char_idx": 0, "end_char_idx": 200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee1f883c-a7ed-404f-8177-d4360d4ac04c": {"__data__": {"id_": "ee1f883c-a7ed-404f-8177-d4360d4ac04c", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\95.txt", "file_name": "95.txt", "file_type": "text/plain", "file_size": 962, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92f96cc5-fb08-431c-8a82-2243eba56518", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\95.txt", "file_name": "95.txt", "file_type": "text/plain", "file_size": 962, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "2a2af66ed876eca628ce2a98258eb7439f5d6d0dd35c0e55f2febe3fefdeb0c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d790a79-cff2-4554-8f83-919bbb75db5a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\94.txt", "file_name": "94.txt", "file_type": "text/plain", "file_size": 202, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "7caa9fd36fb0fb301d15e3e80619e57565d593467ac0ef0e9b77befef334b30e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae0a7dd8-543a-4a38-97e9-ef9bf190fd8f", "node_type": "1", "metadata": {}, "hash": "b92fc8b072da4174ef8047c2a9332ebdead53ed6234a7090b4b4287f2318e710", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: fission\r\nlink: https://github.com/fission/fission\r\ndescription: Fission is a fast serverless framework for Kubernetes with a focus on developer productivity and high performance. Fission operates on just the code: Docker and Kubernetes are abstracted away under normal operation, though you can use both to extend Fission if you want to. Fission is extensible to any language; the core is written in Go, and language-specific parts are isolated in something called environments. Fission maintains a pool of \"warm\" containers that each contain a small dynamic loader. When a function is first called, i.e. \"cold-started\", a running container is chosen and the function is loaded. This pool is what makes Fission fast: cold-start latencies are typically about 100msec. Building on Kubernetes also means that anything you do for operations on your Kubernetes cluster such as monitoring or log aggregation also helps with ops on your Fission deployment.", "start_char_idx": 0, "end_char_idx": 960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae0a7dd8-543a-4a38-97e9-ef9bf190fd8f": {"__data__": {"id_": "ae0a7dd8-543a-4a38-97e9-ef9bf190fd8f", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\96.txt", "file_name": "96.txt", "file_type": "text/plain", "file_size": 234, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3f87ce7-f81a-4860-9272-74316bb0e0fb", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\96.txt", "file_name": "96.txt", "file_type": "text/plain", "file_size": 234, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8bb90d4f8644c17a7427229bb096166c92cc2ccb39abf1001ad7b64d9b261aad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee1f883c-a7ed-404f-8177-d4360d4ac04c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\95.txt", "file_name": "95.txt", "file_type": "text/plain", "file_size": 962, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ef49e11b263ed4b8e803da839dd507e569278f8a7fe3db2c5d56c19d42dba5cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d22d01d8-0e3e-46d7-a880-0382d7624d37", "node_type": "1", "metadata": {}, "hash": "886463154ecebce98b00343485ad1dee9a868f694f609854e916cda45cc07fee", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: mist\r\nlink: https://github.com/Hydrospheredata/mist\r\ndescription: Hydrosphere Mist is a serverless proxy for Spark cluster. Mist provides a new functional programming framework and deployment model for Spark applications.", "start_char_idx": 0, "end_char_idx": 232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d22d01d8-0e3e-46d7-a880-0382d7624d37": {"__data__": {"id_": "d22d01d8-0e3e-46d7-a880-0382d7624d37", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51a6cfaf-27f9-47e8-9134-bb649e724485", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8a5b346cb3ad591cfdeb7e92d5b6c5e29ecdd50f7a9a09eddc52155a8ce04447", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae0a7dd8-543a-4a38-97e9-ef9bf190fd8f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\96.txt", "file_name": "96.txt", "file_type": "text/plain", "file_size": 234, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "a10705b90bb050941491631d506e6e3fa05fe6aa18b449784c4bd4bd91885bb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d94545c6-1461-4d44-9ace-e676c17bef44", "node_type": "1", "metadata": {}, "hash": "b2e7147fdab6230df171318d3846b5067e310d2dae4fe1786dc9f66437adafef", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: timescaledb\r\nlink: https://github.com/timescale/timescaledb\r\ndescription: TimescaleDB is an open-source database designed to make SQL scalable for time-series data. It is engineered from PostgreSQL and packaged as a PostgreSQL extension, providing automatic partitioning across time and space (partitioning key), as well as full SQL support. TimescaleDB scales PostgreSQL for time-series data via automatic partitioning across time and space (partitioning key), yet retains the standard PostgreSQL interface. In other words, TimescaleDB exposes what look like regular tables, but are actually only an abstraction (or a virtual view) of many individual tables comprising the actual data. This single-table view, which we call a hypertable, is comprised of many chunks, which are created by partitioning the hypertable's data in either one or two dimensions: by a time interval, and by an (optional) \"partition key\" such as device id, location, user id, etc.", "start_char_idx": 0, "end_char_idx": 967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d94545c6-1461-4d44-9ace-e676c17bef44": {"__data__": {"id_": "d94545c6-1461-4d44-9ace-e676c17bef44", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51a6cfaf-27f9-47e8-9134-bb649e724485", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8a5b346cb3ad591cfdeb7e92d5b6c5e29ecdd50f7a9a09eddc52155a8ce04447", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d22d01d8-0e3e-46d7-a880-0382d7624d37", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "f8b4e27ae0e96ba4cac77ab1dcca436ba6b0236f068949cd5e94d30023c489d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d9cb584-ffc7-4acc-ba26-4c50b4a0da62", "node_type": "1", "metadata": {}, "hash": "25af633724236abffc11e6f68bd50be07e56ba5580153b9e71d5d69f22d515c1", "class_name": "RelatedNodeInfo"}}, "text": "It is engineered from PostgreSQL and packaged as a PostgreSQL extension, providing automatic partitioning across time and space (partitioning key), as well as full SQL support. TimescaleDB scales PostgreSQL for time-series data via automatic partitioning across time and space (partitioning key), yet retains the standard PostgreSQL interface. In other words, TimescaleDB exposes what look like regular tables, but are actually only an abstraction (or a virtual view) of many individual tables comprising the actual data. This single-table view, which we call a hypertable, is comprised of many chunks, which are created by partitioning the hypertable's data in either one or two dimensions: by a time interval, and by an (optional) \"partition key\" such as device id, location, user id, etc. Virtually all user interactions with TimescaleDB are with hypertables. Creating tables and indexes, altering tables, inserting data, selecting data, etc., can (and should) all be executed on the hypertable.", "start_char_idx": 176, "end_char_idx": 1174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d9cb584-ffc7-4acc-ba26-4c50b4a0da62": {"__data__": {"id_": "5d9cb584-ffc7-4acc-ba26-4c50b4a0da62", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51a6cfaf-27f9-47e8-9134-bb649e724485", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8a5b346cb3ad591cfdeb7e92d5b6c5e29ecdd50f7a9a09eddc52155a8ce04447", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d94545c6-1461-4d44-9ace-e676c17bef44", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "540049e16659728ddcf8a42badd4866a2b10183e4e755495d209c128a63f0456", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0b732ec-9a87-4e74-a73a-933ae89d0aed", "node_type": "1", "metadata": {}, "hash": "52b2077edeef8dc2352d51ae02a522aff45648f9ace201c2ec272e5b972e7091", "class_name": "RelatedNodeInfo"}}, "text": "TimescaleDB scales PostgreSQL for time-series data via automatic partitioning across time and space (partitioning key), yet retains the standard PostgreSQL interface. In other words, TimescaleDB exposes what look like regular tables, but are actually only an abstraction (or a virtual view) of many individual tables comprising the actual data. This single-table view, which we call a hypertable, is comprised of many chunks, which are created by partitioning the hypertable's data in either one or two dimensions: by a time interval, and by an (optional) \"partition key\" such as device id, location, user id, etc. Virtually all user interactions with TimescaleDB are with hypertables. Creating tables and indexes, altering tables, inserting data, selecting data, etc., can (and should) all be executed on the hypertable. From the perspective of both use and management, TimescaleDB just looks and feels like PostgreSQL, and can be managed and queried as such.", "start_char_idx": 353, "end_char_idx": 1313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0b732ec-9a87-4e74-a73a-933ae89d0aed": {"__data__": {"id_": "e0b732ec-9a87-4e74-a73a-933ae89d0aed", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\98.txt", "file_name": "98.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a703a98c-0cf4-4245-ba39-b360db3499c1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\98.txt", "file_name": "98.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ebb09573d2ed42944b82457d64ddf524440eec14f95673103b9a0c9b631f3310", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d9cb584-ffc7-4acc-ba26-4c50b4a0da62", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "8df4a79237017956c7c033b7770dbc8ca22c3a4ffae0544aacc821d09bedd018", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03f1a526-d53f-4f12-8f1c-7806f0070334", "node_type": "1", "metadata": {}, "hash": "3c31147bbe08ff99de546b088d1c4009c5861ee73d3067a1eea7dbc20128bc97", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: openwhisk\r\nlink: https://github.com/apache/openwhisk\r\ndescription: OpenWhisk is a serverless functions platform for building cloud applications. OpenWhisk offers a rich programming model for creating serverless APIs from functions, composing functions into serverless workflows, and connecting events to functions using rules and triggers. The easiest way to start using OpenWhisk is to install the \"Standalone\" OpenWhisk stack. When the OpenWhisk stack is up, it will open your browser to a functions Playground, typically served from http://localhost:3232. To make use of all OpenWhisk features, you will need the OpenWhisk command line tool called `wsk` which you can download from https://s.apache.org/openwhisk-cli-download. OpenWhisk can also be installed on a Kubernetes cluster. Browse the documentation to learn more about OpenWhisk.", "start_char_idx": 0, "end_char_idx": 853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03f1a526-d53f-4f12-8f1c-7806f0070334": {"__data__": {"id_": "03f1a526-d53f-4f12-8f1c-7806f0070334", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\99.txt", "file_name": "99.txt", "file_type": "text/plain", "file_size": 1216, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8469d484-e5c1-4c43-a63c-540e9f6ce250", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\99.txt", "file_name": "99.txt", "file_type": "text/plain", "file_size": 1216, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "346466234d0898e02dc1587994f46ea5c7704da91e6ac2843741997c91ffeb17", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0b732ec-9a87-4e74-a73a-933ae89d0aed", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\98.txt", "file_name": "98.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "ebc8339a16ba6f32602808a74bbcad38408d59a74f66ad1fabb6b98577555d12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bed57c7-f2a4-42ea-8fbe-c9ecc91a3189", "node_type": "1", "metadata": {}, "hash": "531c345a4eff250f77ec103050a75f7aa286da530a92fc0fccf409866b26c5ed", "class_name": "RelatedNodeInfo"}}, "text": "repo_name: hops\r\nlink: https://github.com/hopshadoop/hops\r\ndescription: **Hops** ( **H** adoop **O** pen **P** latform-as-a- **S** ervice) is a next.generation distribution of Apache Hadoop with scalable, highly available,.customizable metadata. Hops consists internally of two main sub projects,.HopsFs and HopsYarn. **HopsFS** is a new implementation of the Hadoop.Filesystem (HDFS), that supports multiple stateless NameNodes, where the.metadata is stored in MySQL Cluster, an in-memory distributed database. HopsFS.enables more scalable clusters than Apache HDFS (up to ten times larger.clusters), and enables NameNode metadata to be both customized and analyzed,.because it can now be easily accessed via a SQL API. **HopsYARN** introduces a.distributed stateless Resource Manager, whose state is migrated to MySQL.Cluster.", "start_char_idx": 0, "end_char_idx": 828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bed57c7-f2a4-42ea-8fbe-c9ecc91a3189": {"__data__": {"id_": "4bed57c7-f2a4-42ea-8fbe-c9ecc91a3189", "embedding": null, "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\99.txt", "file_name": "99.txt", "file_type": "text/plain", "file_size": 1216, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8469d484-e5c1-4c43-a63c-540e9f6ce250", "node_type": "4", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\99.txt", "file_name": "99.txt", "file_type": "text/plain", "file_size": 1216, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "346466234d0898e02dc1587994f46ea5c7704da91e6ac2843741997c91ffeb17", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03f1a526-d53f-4f12-8f1c-7806f0070334", "node_type": "1", "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\99.txt", "file_name": "99.txt", "file_type": "text/plain", "file_size": 1216, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}, "hash": "0c9fc6298c30148867ae28f917ffafe9c54212fa8eaa33029293e423d548a4a8", "class_name": "RelatedNodeInfo"}}, "text": "Hops consists internally of two main sub projects,.HopsFs and HopsYarn. **HopsFS** is a new implementation of the Hadoop.Filesystem (HDFS), that supports multiple stateless NameNodes, where the.metadata is stored in MySQL Cluster, an in-memory distributed database. HopsFS.enables more scalable clusters than Apache HDFS (up to ten times larger.clusters), and enables NameNode metadata to be both customized and analyzed,.because it can now be easily accessed via a SQL API. **HopsYARN** introduces a.distributed stateless Resource Manager, whose state is migrated to MySQL.Cluster. This enables our YARN architecture to have no down-time, with.failover of a ResourceManager happening in a few seconds. Together, HopsFS and.HopsYARN enable Hadoop clusters to scale to larger volumes and higher.throughput.\r\n\r\nFor more information on Hops, including how to build software, required software, and installation, refer to the Hops Hadoop Distribution Online Documentation.", "start_char_idx": 246, "end_char_idx": 1214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"b5e292d2-a134-4a9e-86ad-69dc8cda1d3d": {"node_ids": ["ef36562f-401d-4d3f-8819-01bd317f38ac"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\0.txt", "file_name": "0.txt", "file_type": "text/plain", "file_size": 632, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a33872ec-8199-4568-8cca-87ca03333326": {"node_ids": ["c1c41615-7dba-48c5-b7bb-e0278326c20d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\1.txt", "file_name": "1.txt", "file_type": "text/plain", "file_size": 751, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "cf6106e0-f8b6-4fd9-8110-83752bade113": {"node_ids": ["879aa659-c616-4665-848f-3b89ae161684"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\10.txt", "file_name": "10.txt", "file_type": "text/plain", "file_size": 599, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "38b42cf6-0149-4170-a208-4b53d527d9d2": {"node_ids": ["a845ed96-cdae-40a0-922a-1b83f8d38709"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\100.txt", "file_name": "100.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "351b7993-f448-4ecd-8635-0288100f1c35": {"node_ids": ["df31eaea-ed63-4a41-b411-842b54ea0ec9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\101.txt", "file_name": "101.txt", "file_type": "text/plain", "file_size": 967, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "139b9d1d-b8a9-4a68-8c54-2553083d7e07": {"node_ids": ["2a9e1df7-06b5-4099-b55c-b2b8917ab711", "babaae1a-c048-45cf-90fe-99d3a6940348"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\102.txt", "file_name": "102.txt", "file_type": "text/plain", "file_size": 1056, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "aabfb308-4360-4680-b521-e061f877d376": {"node_ids": ["b6f38bfa-f98f-4f3e-80a7-ef7298e0a554"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\103.txt", "file_name": "103.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6560b13a-ac24-44e2-bafc-5ee91c0f8884": {"node_ids": ["c675754f-af3e-49e2-85b1-a5f5e8c7577f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\104.txt", "file_name": "104.txt", "file_type": "text/plain", "file_size": 942, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "cb57b4af-92f4-41d6-8bac-6e6967e6f8bc": {"node_ids": ["cecd8ee7-58b9-4a0e-98ab-0ecdb5b81a83"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\105.txt", "file_name": "105.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c97a0073-d1d9-4e6b-88e6-fceeb3e7e31e": {"node_ids": ["847a89ad-85df-4391-a2ec-f9b6a4ec35f5"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\106.txt", "file_name": "106.txt", "file_type": "text/plain", "file_size": 983, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "baeb0f1a-cc92-46a2-886f-9b3186cd23b9": {"node_ids": ["2df32ffc-1586-4d61-a892-b62ba5932c5f", "6ac77a62-188b-4ae4-8a92-951eca0ca931", "fa795666-e029-48fd-8758-1d04d7f03014"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\107.txt", "file_name": "107.txt", "file_type": "text/plain", "file_size": 1317, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0c2b9635-b7c3-4992-bcb0-b2b41675bcf4": {"node_ids": ["539dc552-ce4f-4f08-92e2-2ef8b6a339e4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\108.txt", "file_name": "108.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8751e01c-546f-45c9-a18f-9072f84ece02": {"node_ids": ["c0948931-3583-440b-ae33-cd6eded834ec"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\109.txt", "file_name": "109.txt", "file_type": "text/plain", "file_size": 973, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "96495664-a204-47b7-bbea-f03ce0079376": {"node_ids": ["3f7cf6b2-255f-47cc-af52-7266f17db243"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\11.txt", "file_name": "11.txt", "file_type": "text/plain", "file_size": 903, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0b29e417-ccc3-4b22-8dfd-39da9db03f96": {"node_ids": ["6361f438-1636-431c-a9e3-6451497ade6f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\110.txt", "file_name": "110.txt", "file_type": "text/plain", "file_size": 694, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "731197b0-1ce1-4ff6-8452-a3c6ba3e9bc8": {"node_ids": ["4d6aeb60-b634-4d07-a04a-172a35461673"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\111.txt", "file_name": "111.txt", "file_type": "text/plain", "file_size": 1041, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a272fd28-cfe0-494c-8745-5c6fdbf79897": {"node_ids": ["2c42ca9d-ce06-4c1e-9a5d-6747b1734d40"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\112.txt", "file_name": "112.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "40ed341b-1507-4cae-9d07-e6e9baf669b7": {"node_ids": ["eff87b85-cdc5-4e04-8ec2-0fc9418e8053", "53bd7ae5-5941-477a-ba1a-db0deb515cd9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\113.txt", "file_name": "113.txt", "file_type": "text/plain", "file_size": 1146, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "301cd385-0111-44ca-9961-a276a29c7e02": {"node_ids": ["d51298aa-aa0d-47ec-a224-ef5bdf6d4949"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\114.txt", "file_name": "114.txt", "file_type": "text/plain", "file_size": 984, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "82d4e3c2-b6cc-418b-941c-4e5692901e27": {"node_ids": ["44c55cae-f536-4a38-9177-afe958025933"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\115.txt", "file_name": "115.txt", "file_type": "text/plain", "file_size": 677, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9069c6f4-c561-45c6-8347-d92d766bf645": {"node_ids": ["81263731-90e8-47fd-9da3-9ef79436b245"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\116.txt", "file_name": "116.txt", "file_type": "text/plain", "file_size": 578, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "cb254119-f0ab-428e-a4d9-804706c1269c": {"node_ids": ["b23d7c4c-e038-4a00-abeb-2de0501ff9d2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\117.txt", "file_name": "117.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d847c661-f669-4bae-b731-b9cdfe9cfdbf": {"node_ids": ["6ef4627c-fc8d-4e15-8ffd-278b12d8f36d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\118.txt", "file_name": "118.txt", "file_type": "text/plain", "file_size": 506, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e5297a92-0aab-4af3-baa7-56b2bfe617bf": {"node_ids": ["4cd11473-6533-4e0d-aff6-bc811e79378f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\119.txt", "file_name": "119.txt", "file_type": "text/plain", "file_size": 1047, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "dcd9e6bc-bad2-43ca-b2e2-8999349f1e00": {"node_ids": ["655f1b90-c6e0-4099-b28a-aeafa6225868", "81cdda33-ed53-4777-b414-95ef6943af02", "150f858c-ebb7-4b34-875f-26f5a4af09a2", "18473db6-4bcb-4daf-a165-600f8551e081"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\12.txt", "file_name": "12.txt", "file_type": "text/plain", "file_size": 1295, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "df3de19e-b8da-4d9d-998b-4f2e16fc0998": {"node_ids": ["70c8a4bc-f01e-4542-8f6a-dfd0ca11a234"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\120.txt", "file_name": "120.txt", "file_type": "text/plain", "file_size": 660, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e201afc0-6bc7-446c-b9e6-e0e132d9b77c": {"node_ids": ["77669bd8-7407-4c75-8c1d-de4ed1cfb681"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\121.txt", "file_name": "121.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f3fb87eb-71b8-4f8d-b9b1-2d37466816bd": {"node_ids": ["8edd86e4-b1ed-4738-8bd9-4e03b664490d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\122.txt", "file_name": "122.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9e85ca8c-b81e-4143-a7e2-5e66679a2913": {"node_ids": ["8999210f-60e5-4433-8832-0e20f0ce5eae"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\123.txt", "file_name": "123.txt", "file_type": "text/plain", "file_size": 685, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "367ec997-53eb-4fd9-85e0-5edfc8935240": {"node_ids": ["c0d71673-7bf9-4f7e-9655-3980e80ae62a", "894cc91f-110d-4104-a004-129253335b25"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\124.txt", "file_name": "124.txt", "file_type": "text/plain", "file_size": 1184, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "73f46aef-770e-4d59-818c-7d15d8e6b2b9": {"node_ids": ["f092cc31-f987-4770-8a56-cf8f8d8c1584"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\125.txt", "file_name": "125.txt", "file_type": "text/plain", "file_size": 1055, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a9ad1924-220e-40f2-bda0-6d0e6eae7aa0": {"node_ids": ["02e60a50-9761-4ac2-a284-8a7d11f05729"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\126.txt", "file_name": "126.txt", "file_type": "text/plain", "file_size": 1023, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e9ae78a1-ffdf-4399-843d-6ce5b5247d7c": {"node_ids": ["08130c11-d1bb-4f01-a4f4-45675abe7b9d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\127.txt", "file_name": "127.txt", "file_type": "text/plain", "file_size": 978, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "3990527f-3cec-47ae-82a2-741edf3041bb": {"node_ids": ["23d2654f-051e-479c-a882-9f2c8ccb80f8"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\128.txt", "file_name": "128.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "baafa8fc-3198-4410-8c79-7c0c121b2935": {"node_ids": ["089cb881-ea18-4d61-9e5f-f5d43c0de8f0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\129.txt", "file_name": "129.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7212107f-33c2-4ab2-9949-7116da9c38a0": {"node_ids": ["5fbf122a-b3c9-48a1-8a27-4a7e977a1a03"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\13.txt", "file_name": "13.txt", "file_type": "text/plain", "file_size": 793, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "59f980eb-bd73-4503-8be4-33e3ebd5e985": {"node_ids": ["3ca150dc-dd0f-4f9f-8cba-1de45bf3294b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\130.txt", "file_name": "130.txt", "file_type": "text/plain", "file_size": 1053, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "fb7d2daa-0898-48dd-bfb3-a034cf33a7b9": {"node_ids": ["3691e635-ea60-4d2c-9ed7-35137dd55fed"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\131.txt", "file_name": "131.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e765345f-1a30-4e05-8cb5-af9b246e3f8b": {"node_ids": ["bbb7aa70-540a-4167-bc7e-685b18e878ab"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\132.txt", "file_name": "132.txt", "file_type": "text/plain", "file_size": 875, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "db4fc2c8-86d1-4f48-b0f1-02a702b3aa3c": {"node_ids": ["cacedebd-a22f-4f68-a04d-33daba44a92d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\133.txt", "file_name": "133.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b43f6d2c-aa71-4cc2-bef0-0c9f7a0b9cdf": {"node_ids": ["fb098947-0625-4970-8a89-eccdc3f10e27"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\134.txt", "file_name": "134.txt", "file_type": "text/plain", "file_size": 661, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "122052c7-8a0c-4c26-ae97-7309864955b9": {"node_ids": ["728b8825-0bf0-4a95-a1f9-0cbccee398db"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\135.txt", "file_name": "135.txt", "file_type": "text/plain", "file_size": 356, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7de105af-319a-4cee-96d2-dfa5b88cf140": {"node_ids": ["9a216bc2-0426-4aa7-bcea-5856638efc00"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\136.txt", "file_name": "136.txt", "file_type": "text/plain", "file_size": 332, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8b7f361e-3568-45ec-b415-6135472503f8": {"node_ids": ["e289446f-61ad-44f1-8b85-f3855bb569a9", "5feffe8c-e4f6-433d-bc4b-128f9da33b58", "670b3b83-d006-468d-99c5-d0fb1f101eda", "adf58f5c-3667-4a70-a67d-eafd6ebc7c38"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\137.txt", "file_name": "137.txt", "file_type": "text/plain", "file_size": 1208, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4f692231-45d4-4f4b-a63b-b06028d512e9": {"node_ids": ["2e8c09c3-c8f2-4b79-b176-943d00c715ec"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\138.txt", "file_name": "138.txt", "file_type": "text/plain", "file_size": 784, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b3e56570-1c06-4ee2-8547-00212530db25": {"node_ids": ["58e0b3b0-7348-4cef-8884-86ed8ff27651"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\139.txt", "file_name": "139.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d9f35f6d-bcd8-4023-b9d2-e0398bf942f8": {"node_ids": ["45f9c967-3361-4118-9a2d-9141eaacc236"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\14.txt", "file_name": "14.txt", "file_type": "text/plain", "file_size": 288, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2cb2a6f8-ba4d-4f32-bc2b-43fcb62583cd": {"node_ids": ["0784aad8-4ec9-4654-b590-73023b0eadf6"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\140.txt", "file_name": "140.txt", "file_type": "text/plain", "file_size": 936, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7b34569e-b2e0-49ba-b688-14f3ab9c1a9f": {"node_ids": ["060a5051-5217-4504-b8a3-82a61fe369d6"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\141.txt", "file_name": "141.txt", "file_type": "text/plain", "file_size": 700, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "89aa0982-d92b-435c-b00b-131dde63e9b6": {"node_ids": ["eb13c929-acb8-4349-beca-024cea5a60fb"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\142.txt", "file_name": "142.txt", "file_type": "text/plain", "file_size": 1008, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "892183e9-fea9-4b7d-ad31-6ea876f5a171": {"node_ids": ["0e652061-42f8-49b0-92ca-ffa4cd3bf00c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\143.txt", "file_name": "143.txt", "file_type": "text/plain", "file_size": 668, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "5659ed1c-d7a7-42bf-b884-e9e048361786": {"node_ids": ["b5d8630e-70a9-489f-99f8-27a214bfebd4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\144.txt", "file_name": "144.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "09380548-18aa-4184-ab0c-2454dd0927dd": {"node_ids": ["d07b641f-711d-48e3-9d1a-69b93eca2b4a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\145.txt", "file_name": "145.txt", "file_type": "text/plain", "file_size": 997, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "36a12fe1-c86e-4bc5-996a-f7d09ee9fee9": {"node_ids": ["6ae764a0-a39b-41e6-b96f-f0deb4dec986"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\146.txt", "file_name": "146.txt", "file_type": "text/plain", "file_size": 1054, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "affb6e22-ed28-418a-9f50-05c93afd0763": {"node_ids": ["14cfd544-b38e-4c64-89ad-2edebe155068"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\147.txt", "file_name": "147.txt", "file_type": "text/plain", "file_size": 782, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "30ce481d-8967-4d3f-832a-c6ade99b46e0": {"node_ids": ["02fb04fb-69dd-4584-8f9e-584843f8f772"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\148.txt", "file_name": "148.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "dc9c5841-73b5-4f72-b615-be4b1ba51faa": {"node_ids": ["ca1bfd86-7c1c-436b-9d01-64d9c9b66a86"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\149.txt", "file_name": "149.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d13ae74a-8073-453b-8099-5558a4293888": {"node_ids": ["7be92c7b-ea10-4014-9e09-9530b76b92e4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\15.txt", "file_name": "15.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d6bbd1c2-97f4-4eff-bb6b-014db6b718d3": {"node_ids": ["55f77059-cd17-4b44-ac14-81f5483eb85c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\150.txt", "file_name": "150.txt", "file_type": "text/plain", "file_size": 649, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "61dcf961-7eca-4fef-b222-a6c86e9310b8": {"node_ids": ["88c13260-64a0-45b7-86e2-8b91f958b4bb"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\151.txt", "file_name": "151.txt", "file_type": "text/plain", "file_size": 724, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0000001c-2244-46db-8588-916c3be6d671": {"node_ids": ["87be98ee-aa47-4d7f-ade8-f2257ccf98f9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\152.txt", "file_name": "152.txt", "file_type": "text/plain", "file_size": 665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e18191f7-ad15-4f14-8c45-a7f6879d485e": {"node_ids": ["7c9df2ee-beb6-40b7-ac9b-1e76f7023ec2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\153.txt", "file_name": "153.txt", "file_type": "text/plain", "file_size": 517, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d89ad8cd-a5b3-4c66-8626-63e7e25878f6": {"node_ids": ["dbd1ff79-0952-4c42-828a-e475a0632f01"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\154.txt", "file_name": "154.txt", "file_type": "text/plain", "file_size": 528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a3cb979b-edef-4ad3-9457-0246e60c8f8e": {"node_ids": ["bd08f12e-b42a-485c-b313-5e98f5139222"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\155.txt", "file_name": "155.txt", "file_type": "text/plain", "file_size": 759, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c03a63ed-4b7b-4f31-99b7-6b0475267417": {"node_ids": ["67fd19c3-6488-4156-ad13-7c82780cb3bf"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\156.txt", "file_name": "156.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "27f9f4d0-045c-4dec-bc9d-5f2b33bc10c1": {"node_ids": ["d05eef7f-8a9e-4439-b57b-1ee13b19c2f2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\157.txt", "file_name": "157.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a264463d-7e21-4ae7-8366-e236e310daa8": {"node_ids": ["4b27b84e-2eea-4b04-b54f-ac5e36da422a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\158.txt", "file_name": "158.txt", "file_type": "text/plain", "file_size": 745, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "38f0bd10-1ce2-4015-bb6a-6f6ff085216d": {"node_ids": ["ed5d40ed-0b1c-476f-b7d6-08e5ab8e71b7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\159.txt", "file_name": "159.txt", "file_type": "text/plain", "file_size": 78, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "462d6170-8c76-444e-b308-e4900cdf71fe": {"node_ids": ["c4e0ddf3-f606-4dde-afc0-c74e995aa2c4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\16.txt", "file_name": "16.txt", "file_type": "text/plain", "file_size": 682, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "dc1181aa-9fc5-4538-986d-617ea6ee6245": {"node_ids": ["6e3162c8-e330-4c45-83ad-29d986a4d643"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\160.txt", "file_name": "160.txt", "file_type": "text/plain", "file_size": 289, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "716ce4a5-53ce-4e42-a5de-c0c5dfd9f60e": {"node_ids": ["60e86002-b3e8-4d07-a76d-de68a9610980"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\161.txt", "file_name": "161.txt", "file_type": "text/plain", "file_size": 355, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e83a0796-4953-4f1e-bf3d-3333d778ef97": {"node_ids": ["947af18e-4ae9-4e47-9f6f-d5603aea7fbb"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\162.txt", "file_name": "162.txt", "file_type": "text/plain", "file_size": 469, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "63c60423-ec78-4506-bce9-0bd23fa1c367": {"node_ids": ["ab44ceb6-b2c5-447a-9a53-312e4b59fcfd"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\163.txt", "file_name": "163.txt", "file_type": "text/plain", "file_size": 704, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "49dccfd9-335c-480d-870c-8943fae4b87d": {"node_ids": ["d1bd299c-5eb4-4f0b-bd7d-c312becf0764"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\164.txt", "file_name": "164.txt", "file_type": "text/plain", "file_size": 835, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "010a51e2-c61f-42e3-bad5-64c74c7c09aa": {"node_ids": ["894a342f-1b66-4722-8b0b-30922c727dc6"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\165.txt", "file_name": "165.txt", "file_type": "text/plain", "file_size": 391, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "78bf74e2-f33f-4af2-a01c-f9caa88df9fd": {"node_ids": ["4ecba323-fa85-43fb-aae2-15a011e9629f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\166.txt", "file_name": "166.txt", "file_type": "text/plain", "file_size": 672, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "87ee2a70-6212-4e20-ba00-4de444782e49": {"node_ids": ["a1c1a2a3-9ce7-4897-a5ca-cbb7e0e9f35e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\167.txt", "file_name": "167.txt", "file_type": "text/plain", "file_size": 715, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4871d211-2c6b-42a2-b9ce-0d377c287e1e": {"node_ids": ["6139311c-5093-4c2b-9987-653262cedf51"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\168.txt", "file_name": "168.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "37b7993b-8c2e-40f9-963e-9b6f99317064": {"node_ids": ["4c3e10b1-7452-42ef-813a-9be497e7f577"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\169.txt", "file_name": "169.txt", "file_type": "text/plain", "file_size": 938, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "934d0538-759a-44c8-80f7-eadfde381392": {"node_ids": ["662b426c-ac08-4b38-bf62-d893ae49a335"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\17.txt", "file_name": "17.txt", "file_type": "text/plain", "file_size": 412, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7d114870-ea8a-4678-b740-30cc2f4dd978": {"node_ids": ["750e9770-2ebb-497e-8e74-4870bab1bb88"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\170.txt", "file_name": "170.txt", "file_type": "text/plain", "file_size": 380, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4acd7e19-c74d-4352-a754-6c0abbe6974d": {"node_ids": ["44343dfa-b0ef-496e-97c4-68cb93913d54"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\171.txt", "file_name": "171.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "789ceda0-8b18-4d86-8e7b-dc5a1e73ce97": {"node_ids": ["70398f22-bcf9-4975-a14c-19f0e8731b8d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\172.txt", "file_name": "172.txt", "file_type": "text/plain", "file_size": 394, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a7b4fc38-010b-4fad-be2e-cb1516718134": {"node_ids": ["788994aa-9da5-4e99-814a-42a7249566cf"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\173.txt", "file_name": "173.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "cd80c7a4-e1a6-49b9-923c-7b9d7312f54e": {"node_ids": ["92ac68fb-6eed-4e8a-a2f9-6e04d7a0f589"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\174.txt", "file_name": "174.txt", "file_type": "text/plain", "file_size": 764, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "afb3ead4-8175-4f44-985f-641cbabbf28c": {"node_ids": ["e1699c17-785f-4806-8489-40e367456d8f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\175.txt", "file_name": "175.txt", "file_type": "text/plain", "file_size": 851, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "157ba0a8-bdfd-4208-9cbe-69a021d510f7": {"node_ids": ["bea38b7b-5255-404b-bd3c-e5d140c72a80"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\176.txt", "file_name": "176.txt", "file_type": "text/plain", "file_size": 590, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "670c0479-fd8f-4b3e-adb4-d20c8cde8510": {"node_ids": ["fc64a91c-d893-49fa-9a3d-dd3e3a716670"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\177.txt", "file_name": "177.txt", "file_type": "text/plain", "file_size": 775, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "dee94cef-3594-48b7-ac4d-209f1fa4dc1f": {"node_ids": ["c6cd7bb9-e4ce-45e7-baac-23d3c3467108"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\178.txt", "file_name": "178.txt", "file_type": "text/plain", "file_size": 534, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2d41b507-718b-40fa-8d38-9aa41c328430": {"node_ids": ["37393fa8-02db-4f56-9ffc-4b5eddc8e996"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\179.txt", "file_name": "179.txt", "file_type": "text/plain", "file_size": 399, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "13162bcf-3ee3-4920-b0ed-18649219bbf4": {"node_ids": ["ecdf3d9f-62f7-4686-bec7-19df5e419bd4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\18.txt", "file_name": "18.txt", "file_type": "text/plain", "file_size": 526, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2ccaae91-a34f-4989-92dd-625d712654f6": {"node_ids": ["9f9f19cf-4aa5-4a58-a08c-81283e904931"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\180.txt", "file_name": "180.txt", "file_type": "text/plain", "file_size": 362, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e2534ab4-7337-4321-bbb2-bff3c7e5327f": {"node_ids": ["ac8d788b-8b84-410a-bfda-1ad5777a38ce", "952220d7-bd0f-4664-a658-1c2acb371e2a", "0786c093-8f72-4fa3-be4c-67c39d1cf21c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\181.txt", "file_name": "181.txt", "file_type": "text/plain", "file_size": 1287, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4ec41745-8d3c-4639-8ace-b7b8e8dcef4e": {"node_ids": ["f70fd2b5-fcba-4b8b-a160-f6e8f38f73bd"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\182.txt", "file_name": "182.txt", "file_type": "text/plain", "file_size": 732, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d4c8753b-98f9-4463-9544-9c6650380038": {"node_ids": ["11ce88d1-df24-479b-b1b4-deeb7da2c84f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\183.txt", "file_name": "183.txt", "file_type": "text/plain", "file_size": 398, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0dc9ecce-6402-4382-9260-3330b72c19f3": {"node_ids": ["598a7c8c-c7d4-43c3-809f-f66819cc0fa8"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\184.txt", "file_name": "184.txt", "file_type": "text/plain", "file_size": 733, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e368e7f8-2a49-4fb6-81e8-7635954a667f": {"node_ids": ["bf831700-8db1-4bd1-876f-67ddd104a245"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\185.txt", "file_name": "185.txt", "file_type": "text/plain", "file_size": 90, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "89ee3ec4-6c4e-43b7-8c13-6d24b39fa6a4": {"node_ids": ["3c3bbe37-895d-41d7-8422-46840ffa2840"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\186.txt", "file_name": "186.txt", "file_type": "text/plain", "file_size": 772, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "236e95b0-c5ef-4e84-a2a9-90c9aef518d2": {"node_ids": ["6301be1f-5c4e-4d5b-9247-6c63361cb5f4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\187.txt", "file_name": "187.txt", "file_type": "text/plain", "file_size": 844, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d997bfe3-1c1a-4b3f-87fe-74e8e20fa41b": {"node_ids": ["958b8bb8-fac7-41fa-bbd7-d6535173c3b3", "2a40ff73-4abf-410c-b2b7-be5835d0c0f8", "2ad2bfe7-2c5e-4b3b-b398-b0e7e0d9533e", "6816778e-e7ae-4578-b77b-e7ad59928a26", "acce73f6-d7c0-40f8-95b7-07873412ebc7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\188.txt", "file_name": "188.txt", "file_type": "text/plain", "file_size": 1665, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6d89dfac-cd2c-4930-9e00-798a1a6d809f": {"node_ids": ["4745de73-139f-42ec-86eb-0c2eee49d58d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\189.txt", "file_name": "189.txt", "file_type": "text/plain", "file_size": 609, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "5dab64b8-9499-4b96-8bc9-daa95a7b6f48": {"node_ids": ["312a26f9-6326-4455-9802-447184d75875"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\19.txt", "file_name": "19.txt", "file_type": "text/plain", "file_size": 507, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "264916e0-4c2e-4106-a71c-8fb69e8ef786": {"node_ids": ["3351d336-6e7d-4e87-afc8-521de6999497"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\190.txt", "file_name": "190.txt", "file_type": "text/plain", "file_size": 1012, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "df6f39d5-7d24-41bb-a50c-2a0781b36859": {"node_ids": ["f36b09a1-13e3-4d48-b7a3-bb2fddfdf65c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\191.txt", "file_name": "191.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8fcec702-fb6e-4017-81a5-166f163b922c": {"node_ids": ["83aa5243-3c09-4926-95e2-c3a7c6ce7afe"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\192.txt", "file_name": "192.txt", "file_type": "text/plain", "file_size": 477, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b7f1251e-a42d-46f9-8496-e1b620f6364d": {"node_ids": ["fab305d7-dafd-4290-9a89-44ce176c7933"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\193.txt", "file_name": "193.txt", "file_type": "text/plain", "file_size": 598, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6d16cda4-babc-4bc4-960a-1bacfab43bda": {"node_ids": ["e84d2644-cf4a-4715-babb-8c696b2a15a1", "6cac3d7f-275a-4e5e-8787-c629b18b29cd", "0f8c3664-2f3b-4840-9662-a99d1b2a60e0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\194.txt", "file_name": "194.txt", "file_type": "text/plain", "file_size": 1120, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "22c55992-fb13-40f8-806b-0611273c8547": {"node_ids": ["a6446494-aaf3-4198-9520-6963b190e3d9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\195.txt", "file_name": "195.txt", "file_type": "text/plain", "file_size": 85, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "24dc7cf5-5234-4aed-868c-d22f29185bae": {"node_ids": ["24ab2465-3c3a-4d45-b72f-2a48c253b351"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\196.txt", "file_name": "196.txt", "file_type": "text/plain", "file_size": 562, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a5b1707b-c225-41a7-90ea-20a7fe5a7c72": {"node_ids": ["63656294-e3d9-4d92-acb4-fbc4e5ba8701"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\197.txt", "file_name": "197.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2b6ec820-c642-45a6-b29e-74c002eea37b": {"node_ids": ["7f8403a5-a055-4c8e-b968-fb65ddd81724"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\198.txt", "file_name": "198.txt", "file_type": "text/plain", "file_size": 719, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9330043a-e89b-4a01-bd74-7ee21012814d": {"node_ids": ["1cca2cef-ea53-4391-9ede-4cfd66ee2781", "ae4fe739-1d37-43aa-95b2-f023540b8c1c", "c1eca416-d4b7-44d1-bc88-2321df80aff0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\199.txt", "file_name": "199.txt", "file_type": "text/plain", "file_size": 1132, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "1b2ba341-d75d-4f78-a10c-634f35ab3bab": {"node_ids": ["752c5dea-e847-4b7a-ba6e-f88fc44a3e46"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\2.txt", "file_name": "2.txt", "file_type": "text/plain", "file_size": 671, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c7f6fea0-7202-41e8-b8a8-6941cb6d0b60": {"node_ids": ["62c2013e-4ec7-43d5-9d5f-f48aa75e8c24"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\20.txt", "file_name": "20.txt", "file_type": "text/plain", "file_size": 873, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ee049f5d-5fa1-41ba-867e-f34dbbc1b52a": {"node_ids": ["8ddc91e7-660b-4569-b184-8e72c9b45557"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\200.txt", "file_name": "200.txt", "file_type": "text/plain", "file_size": 566, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2605691e-e220-4ece-a16c-f715af41d148": {"node_ids": ["f36c5620-cb17-49a3-844b-9748ef2a0751"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\201.txt", "file_name": "201.txt", "file_type": "text/plain", "file_size": 822, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2eadce5d-225b-4b96-baf5-d94f122f10c3": {"node_ids": ["26823ed9-fdfa-459d-af1d-029b47342a51"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\202.txt", "file_name": "202.txt", "file_type": "text/plain", "file_size": 79, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "58468981-c200-480c-afe3-3d74915299c0": {"node_ids": ["04ecdf90-0ecd-41d9-8d58-bcfa8b494f7d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\203.txt", "file_name": "203.txt", "file_type": "text/plain", "file_size": 1020, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6a62499a-831f-423d-a292-0f923580462e": {"node_ids": ["75d590e1-289b-4a34-8810-2f0e6cefbc29"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\204.txt", "file_name": "204.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0c04190e-dc1d-4cf4-ad1b-4b4b2219d1ea": {"node_ids": ["ca672886-aefa-4d24-bc2b-1b07acc31ae9", "ed9dd215-94d9-45d1-9aa9-413e5e884fe4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\205.txt", "file_name": "205.txt", "file_type": "text/plain", "file_size": 1119, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e469725b-c578-4a35-8d2a-f04eb507b78c": {"node_ids": ["eb91bf48-e379-4f69-ba31-5b99556a9066"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\206.txt", "file_name": "206.txt", "file_type": "text/plain", "file_size": 746, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "23350226-4c59-40e4-918e-83833f39ef1e": {"node_ids": ["f6b4e491-e748-4eaa-b25c-9ad59b7f243b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\207.txt", "file_name": "207.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9de24fff-46db-406e-8f9f-5d9fdabd019c": {"node_ids": ["b4323255-e2a1-4cc9-acec-24a2eb3f55f1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\208.txt", "file_name": "208.txt", "file_type": "text/plain", "file_size": 263, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6da94435-69a2-4804-b6db-5650b426e935": {"node_ids": ["4c17069b-fc3f-4dca-ad04-760ff48b11d1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\209.txt", "file_name": "209.txt", "file_type": "text/plain", "file_size": 739, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0e315a7f-841b-42e9-8caa-7da2831a2a23": {"node_ids": ["daf99d3c-e5ee-46b5-bfc7-b3df897585db", "134b5d8f-36eb-4103-b62d-a076642f0697"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\21.txt", "file_name": "21.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b7d0d0c3-f210-4a45-88f8-3cd5930ddfa7": {"node_ids": ["f25f9bc9-0825-4180-b32d-58ac30620618"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\210.txt", "file_name": "210.txt", "file_type": "text/plain", "file_size": 996, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "da8a2c30-5865-4755-9d0c-39b1f3447d96": {"node_ids": ["e2c5e5f8-b6eb-4c42-bc5b-94d6343a0699"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\211.txt", "file_name": "211.txt", "file_type": "text/plain", "file_size": 852, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "abf962b4-bb99-43a7-bdaa-4bc4281791ee": {"node_ids": ["1a0f882f-b024-4086-baba-6f005b77c993"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\212.txt", "file_name": "212.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "49bf831c-28bd-4d22-b81e-edd06f3606d4": {"node_ids": ["58e09746-c6a2-4c4a-ad7a-7e16a3c092d9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\213.txt", "file_name": "213.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "473b15d6-3cfd-4388-ae8f-cb48b8d7f2f5": {"node_ids": ["1369991c-bb24-4d9f-b3a3-bbac43b344fb", "bdcc1de5-1d06-404d-b7d0-dec8cb8bbeb1", "8645e359-c24e-4831-9e00-2982201276a2", "3da233cb-2dcd-42dc-aa35-7ab800d73106", "3e84cf80-c3aa-4b98-ba68-f222210f952d", "ac9f7bfc-f801-4f82-bbe2-5b99c7af3adb", "c532ca25-eb20-40e6-9c02-fa71216ac39f", "c114f88a-5e71-4d16-aa73-f6248f66031d", "266070a7-8688-4e82-8382-6902cc9617c5"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\214.txt", "file_name": "214.txt", "file_type": "text/plain", "file_size": 2045, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a6ec49f4-2721-4ba8-9946-4b52de895769": {"node_ids": ["90d22f7e-09fc-47f4-9fd4-529445041d19"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\215.txt", "file_name": "215.txt", "file_type": "text/plain", "file_size": 797, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6dba962f-eda8-4374-a375-69ac4d51998d": {"node_ids": ["b5d73d73-5606-4f11-b112-441c7f84422b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\216.txt", "file_name": "216.txt", "file_type": "text/plain", "file_size": 1011, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0c722044-a4cb-40a2-9f4c-6a9b44d69d94": {"node_ids": ["8e233ad7-b95e-4c19-aaa6-a99adea87c30"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\217.txt", "file_name": "217.txt", "file_type": "text/plain", "file_size": 1090, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "14d39e40-bfdc-4c06-aadc-0e77c175a183": {"node_ids": ["658b7a90-6d07-4f5b-93ca-cd718096f955"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\218.txt", "file_name": "218.txt", "file_type": "text/plain", "file_size": 480, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "93f4ac72-646a-448b-8d9f-940d48b8a556": {"node_ids": ["bb30e67e-bfda-4f39-a79f-fe614fb6dcc0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\219.txt", "file_name": "219.txt", "file_type": "text/plain", "file_size": 883, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "cf1b32f9-cfc3-470e-8f36-a1fee2153929": {"node_ids": ["0cd70036-1699-4a65-bc8f-f90410dafb88"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\22.txt", "file_name": "22.txt", "file_type": "text/plain", "file_size": 546, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c9affb97-0c29-4021-baf2-863825851d58": {"node_ids": ["4a830415-566a-418c-94d4-887cb4f8992d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\220.txt", "file_name": "220.txt", "file_type": "text/plain", "file_size": 431, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e493a907-1754-4919-bb87-2f7bf5cde5f9": {"node_ids": ["607e6268-383e-4a35-aacc-2189bc3559a5"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\221.txt", "file_name": "221.txt", "file_type": "text/plain", "file_size": 870, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d4b63884-e2d3-4a10-80c7-7874da26bddf": {"node_ids": ["afa355a0-2718-4c88-914b-0bbb313b3eb2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\222.txt", "file_name": "222.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "99a871ed-8ab0-4918-9dc5-c75a6957d853": {"node_ids": ["990324bc-d980-4d29-9d83-53322d829963"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\223.txt", "file_name": "223.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "83412d26-6498-4f0d-8b4c-a5fedef10173": {"node_ids": ["d31e390b-7222-4b4b-9072-97ac49471559"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\224.txt", "file_name": "224.txt", "file_type": "text/plain", "file_size": 417, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "18b97950-5b23-48c2-abaf-b032f82d760c": {"node_ids": ["c76c1843-995f-4b4d-9bfd-2c0bb26a1e37"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\225.txt", "file_name": "225.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7186d9f6-cba7-47a8-a2e8-57cb21e56e79": {"node_ids": ["9b6f58dd-24dc-49b3-b5b5-2dd196dcab86"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\226.txt", "file_name": "226.txt", "file_type": "text/plain", "file_size": 725, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "001e396d-cd75-431f-a765-e29eb8ed0bf1": {"node_ids": ["818fabd7-f5db-42ec-8561-43baa12fd2d8"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\227.txt", "file_name": "227.txt", "file_type": "text/plain", "file_size": 625, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4cb0c00b-c699-4718-8736-9d0c2c471abc": {"node_ids": ["d5bee6a0-8fa4-46b4-b8f4-4982961e11b2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\228.txt", "file_name": "228.txt", "file_type": "text/plain", "file_size": 893, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8474b7ab-88ce-423b-972e-e5a20c38d4a6": {"node_ids": ["ca6c2b87-7a3d-4b54-8935-7421283a887c", "1513ae2b-b41a-43e5-9b4c-538bcd249223"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\229.txt", "file_name": "229.txt", "file_type": "text/plain", "file_size": 1247, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6244de17-417c-453b-be2b-a68eb683fc36": {"node_ids": ["bc490d04-b15a-4c9e-b4fc-e711be129d27", "a6857a3f-7f01-40da-8000-50f17e9b0a73", "b307fe2e-9636-4211-87c7-f0da18f084cf", "a35a7f16-c612-4f17-aa65-3ad58eff5cd9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\23.txt", "file_name": "23.txt", "file_type": "text/plain", "file_size": 1528, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4c60d868-559f-42ab-bafb-61d5c7a734b5": {"node_ids": ["461436f4-1e93-4540-bef0-d2087c6b4523"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\230.txt", "file_name": "230.txt", "file_type": "text/plain", "file_size": 642, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d76129e3-edb6-4b27-b369-742c7e2e3ba5": {"node_ids": ["3c55ee3d-f8d2-4b7e-b9d6-4b15bb24ae50"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\231.txt", "file_name": "231.txt", "file_type": "text/plain", "file_size": 1051, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "744021de-da76-4255-9386-8184cbc246fb": {"node_ids": ["bce6c32e-3ab8-4192-8cab-fdc561c1fc9b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\232.txt", "file_name": "232.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a4a5584f-1c74-46d1-b56c-82e30d75c5cf": {"node_ids": ["ddace1f7-1087-4fa6-bf67-ceff6c13e973"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\233.txt", "file_name": "233.txt", "file_type": "text/plain", "file_size": 421, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ab6c6f17-4ab1-4329-b665-1ee1070e2ad2": {"node_ids": ["5a264cfe-cd3e-40ff-a268-063bbe0160d7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\234.txt", "file_name": "234.txt", "file_type": "text/plain", "file_size": 607, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8c186466-93be-4e75-bdb4-e9c3683f8678": {"node_ids": ["3e77ed13-c024-49e9-8e38-0e8903b819e2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\235.txt", "file_name": "235.txt", "file_type": "text/plain", "file_size": 483, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "067a8088-66ce-4ba6-aeee-0ae4e4bc2f5a": {"node_ids": ["07cd03f5-11f0-493e-83bc-dc6dd2c98688"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\236.txt", "file_name": "236.txt", "file_type": "text/plain", "file_size": 558, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f1379036-f5c2-4ce2-9dc4-1f07d99a8846": {"node_ids": ["e76b53dc-9a2e-449a-be82-3c86f3f6beec", "491469b2-53cd-4588-9489-7611d17f9583"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\237.txt", "file_name": "237.txt", "file_type": "text/plain", "file_size": 1215, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "de22733b-502e-4dbf-b070-9d41a6928b9b": {"node_ids": ["e1547094-a26e-4b38-9d84-1daab33ce0fa"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\238.txt", "file_name": "238.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7a41f28b-e2a8-478e-99c1-4ce440c2a4b7": {"node_ids": ["acc5b919-8e92-47ef-a49f-e0302a659093"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\239.txt", "file_name": "239.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4b27a818-bd96-4a1f-ace1-345e5d3f058d": {"node_ids": ["b7acdb0a-ec0f-40af-a3be-867695649b73"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\24.txt", "file_name": "24.txt", "file_type": "text/plain", "file_size": 729, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0b9e0307-70ee-4f2b-8019-fc806dd17dc3": {"node_ids": ["0354f21f-a5f7-4184-90fc-3f5e413361b7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\240.txt", "file_name": "240.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "fbffcb8c-4da0-4423-8925-30d3dbdce156": {"node_ids": ["c92ef262-2bb8-4865-a1e6-d557c81eaff8"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\241.txt", "file_name": "241.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7b25e27f-fa12-4e0f-9e53-4a61010bdcf1": {"node_ids": ["b8ac4fe7-8910-4928-92d9-de2ffc12988b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\242.txt", "file_name": "242.txt", "file_type": "text/plain", "file_size": 832, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "692f4a62-d411-4ef8-949c-b73941e653aa": {"node_ids": ["01b34120-fa60-436b-a079-1d24eac145ec"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\243.txt", "file_name": "243.txt", "file_type": "text/plain", "file_size": 522, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "583508e3-d4be-4c1b-85f3-5d557b1d2220": {"node_ids": ["d2d7a324-cac0-43d6-abf1-9841f613b5f2", "29bbb71b-7d11-42f4-92f3-3fc507391e2b", "dc3df753-959f-4793-8307-90f642bd3250"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\244.txt", "file_name": "244.txt", "file_type": "text/plain", "file_size": 1190, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c13c6518-9797-4e0d-9141-7049fe1346d5": {"node_ids": ["1c31bb63-2f5f-44f0-bca8-252890763a6e", "dda69810-d236-46a3-b769-60adeb812233"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\245.txt", "file_name": "245.txt", "file_type": "text/plain", "file_size": 1058, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d6b670f2-0215-4223-97e2-b2b3a1102108": {"node_ids": ["090b3b46-9a65-446b-addb-6c6f36d618cf"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\246.txt", "file_name": "246.txt", "file_type": "text/plain", "file_size": 916, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6b36ba21-c6ab-45e4-bf49-a849a2b98c73": {"node_ids": ["fe7a46cb-cbaa-44b7-a170-95fbceace055", "6d614a7c-db0e-4fad-b858-c94398bf472b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\247.txt", "file_name": "247.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e591e5d6-8d4e-45e5-a792-19676a6d9605": {"node_ids": ["708f2286-03a5-4523-a013-1b3b8409f3ba"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\248.txt", "file_name": "248.txt", "file_type": "text/plain", "file_size": 74, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "13983090-f65f-48b9-b71d-bd4c9aa8490d": {"node_ids": ["3c001b22-4e9c-4532-81d0-603b1df4c15c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\249.txt", "file_name": "249.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "72554f62-e08a-480d-8392-871cf190eb85": {"node_ids": ["1753368e-0d73-48e8-abb8-dd2c261a18ee"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\25.txt", "file_name": "25.txt", "file_type": "text/plain", "file_size": 390, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4fa9a752-9ff4-4586-9201-c2ebb5a2e67f": {"node_ids": ["728eaf11-fd8e-4045-84c5-85c6180a80d3"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\250.txt", "file_name": "250.txt", "file_type": "text/plain", "file_size": 792, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d3eb513b-74c4-466d-b043-b9fbb99eaea7": {"node_ids": ["2b91ef1a-bf99-4af1-bd0f-abd24fc7602d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\251.txt", "file_name": "251.txt", "file_type": "text/plain", "file_size": 579, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8658ee9e-5f94-4165-bde8-5eb944f66b4d": {"node_ids": ["01f74b0d-6c93-4516-a962-0d33fba1d09c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\252.txt", "file_name": "252.txt", "file_type": "text/plain", "file_size": 84, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "37041e6b-f3af-4bbf-83e7-7a6b054dd476": {"node_ids": ["7b8d66cb-acf6-4b2a-b811-87a71dd65c0a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\253.txt", "file_name": "253.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "40b2b11c-e13b-4b31-a3eb-e28c95e4bed2": {"node_ids": ["b43f319b-1c45-4a9d-909e-6822424e233d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\254.txt", "file_name": "254.txt", "file_type": "text/plain", "file_size": 749, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "bed6ce8a-dbe0-4ef7-8e1c-167947e586b9": {"node_ids": ["d0c4dd9e-ead3-4bff-a81a-979589f1187b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\255.txt", "file_name": "255.txt", "file_type": "text/plain", "file_size": 316, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e442320d-4037-4bc7-8595-edfa35ae9456": {"node_ids": ["87a92049-411f-4a25-86cc-8e830b8f4df5"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\256.txt", "file_name": "256.txt", "file_type": "text/plain", "file_size": 753, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b75d4795-94a5-4cf9-8df7-f7ed22423be6": {"node_ids": ["d76d8899-a808-4c5f-9856-5808f7399579"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\257.txt", "file_name": "257.txt", "file_type": "text/plain", "file_size": 504, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "410672fa-014f-4455-a82f-9e20ca8141a4": {"node_ids": ["c1b5c9fb-611f-4e05-9141-b7e38bb2d5fa"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\258.txt", "file_name": "258.txt", "file_type": "text/plain", "file_size": 612, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6369e180-1485-4fc7-9815-1239aa7b1246": {"node_ids": ["cc07e5f1-915d-4abf-96c6-f77dad5b3ee1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\259.txt", "file_name": "259.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4f8c9b27-f5c3-4269-ac33-c14d2e3186e5": {"node_ids": ["538782fc-a847-4a95-a9f6-d597c1505930"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\26.txt", "file_name": "26.txt", "file_type": "text/plain", "file_size": 773, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "653580ba-9804-4817-a658-1d654d434395": {"node_ids": ["9eb084ea-b1c4-4dd6-be9e-0566427f9af7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\260.txt", "file_name": "260.txt", "file_type": "text/plain", "file_size": 834, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a7e3d947-82f2-4775-9941-b5643f9482b0": {"node_ids": ["11804f52-b7fd-4f5a-831e-9fa4b2626a83"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\261.txt", "file_name": "261.txt", "file_type": "text/plain", "file_size": 557, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "229093f3-3138-4816-9550-b7625c8dd1f9": {"node_ids": ["a4d7367d-b46b-417a-9f1d-b7e0a97768cf"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\262.txt", "file_name": "262.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "3384521b-7f77-4c93-b6fb-f3e87ba96198": {"node_ids": ["b1c21496-cf4f-4114-a988-f5ca08cd6a00", "e6c59747-7be1-466f-a30d-b783577c59a6"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\263.txt", "file_name": "263.txt", "file_type": "text/plain", "file_size": 1025, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8b1b3d92-6a39-44d3-a7d3-297ccdd96c5a": {"node_ids": ["b8534770-760a-4fa3-a0b0-163b6825dfd0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\264.txt", "file_name": "264.txt", "file_type": "text/plain", "file_size": 1204, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7dc4c0d8-e421-4b38-a621-5b14d3ceddac": {"node_ids": ["723ed975-1add-4c2d-a413-14d22565705a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\265.txt", "file_name": "265.txt", "file_type": "text/plain", "file_size": 961, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d4782ab0-30f2-416f-9324-abf42622fcf8": {"node_ids": ["8ea4a482-450e-4b9e-99eb-dd4453e2f14d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\266.txt", "file_name": "266.txt", "file_type": "text/plain", "file_size": 980, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0f799979-c08b-4342-bc19-8dea7255a769": {"node_ids": ["b547964c-2e71-40cf-9855-cbe8afc06e55"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\267.txt", "file_name": "267.txt", "file_type": "text/plain", "file_size": 92, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ea472040-355b-407c-b0dd-021b5ccaae9a": {"node_ids": ["527bfde7-d7c4-49a1-ab33-a428273daf2f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\268.txt", "file_name": "268.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "21abde37-b7f3-4706-bd70-608bd96f42ad": {"node_ids": ["23737fb5-fee8-4ede-b4b8-59910f8bc373"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\269.txt", "file_name": "269.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "fc13cfb7-e4b7-48d7-a96e-fe1551a66467": {"node_ids": ["78abc551-2069-4142-9e8e-67ac0033ebdc"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\27.txt", "file_name": "27.txt", "file_type": "text/plain", "file_size": 585, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "02145cbd-934f-4faf-93d3-383d83cc3fa4": {"node_ids": ["d34af291-fa2c-44c6-82eb-48f0b90a7f4e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\270.txt", "file_name": "270.txt", "file_type": "text/plain", "file_size": 761, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8664dcc4-e923-484f-bcee-cfda6f77f0a9": {"node_ids": ["655a484a-ed22-4625-85f7-ba57c6accb8d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\271.txt", "file_name": "271.txt", "file_type": "text/plain", "file_size": 658, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ce26248e-e970-4ba2-8d9b-c5a97084f5de": {"node_ids": ["ca92d59f-ac8e-478c-8b18-ef6ae4f9d309", "576a3288-4706-49d4-8706-0ddc6a170fa0", "d04c16a7-c570-4492-98b5-d13820af0251"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\272.txt", "file_name": "272.txt", "file_type": "text/plain", "file_size": 1218, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ea3dfee1-4caf-4a42-a3ae-770e5e05bbc2": {"node_ids": ["656f8edd-bc45-4ad6-a095-1c80b30338e9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\273.txt", "file_name": "273.txt", "file_type": "text/plain", "file_size": 803, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "83e97aec-bd7b-4da0-acac-b3a27a37e031": {"node_ids": ["5e87cf59-bf8b-4d35-9b9a-e785182844b5"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\274.txt", "file_name": "274.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0c77cd49-6487-41fb-a280-74d979e8a698": {"node_ids": ["6fd501c7-b082-464c-8d24-49f44e45c9ee"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\275.txt", "file_name": "275.txt", "file_type": "text/plain", "file_size": 853, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0da429c1-bb29-42a2-b88c-7aed00f6d1de": {"node_ids": ["9112d35e-cd82-46e3-b2e4-1ba6c83d0727"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\276.txt", "file_name": "276.txt", "file_type": "text/plain", "file_size": 91, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f9fdb3ae-ad98-4985-a026-e07b64492b74": {"node_ids": ["a6033f94-ced0-4d14-a8d2-ad05b4264bc3"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\277.txt", "file_name": "277.txt", "file_type": "text/plain", "file_size": 646, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "884a6e5e-8476-4039-8546-7d19b636f3c5": {"node_ids": ["7e31d398-fdb0-440e-85ed-7f841f82c8f7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\278.txt", "file_name": "278.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b85444e2-1908-4b33-97c6-fffa69287381": {"node_ids": ["d1dd32eb-446a-4033-a773-ce6e50f810fe"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\279.txt", "file_name": "279.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "14263c8f-90e0-464c-8555-610a44d14da1": {"node_ids": ["bdfb1451-9c27-42b5-87ea-2d0ab8eed262"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\28.txt", "file_name": "28.txt", "file_type": "text/plain", "file_size": 914, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c2bc1ebb-5c27-4528-ab24-5d371fd4d681": {"node_ids": ["58ae4b66-3599-4009-a72c-6b466942acdf"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\280.txt", "file_name": "280.txt", "file_type": "text/plain", "file_size": 843, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "40c1e146-3641-495a-87fc-df10d4045174": {"node_ids": ["f7cea8ce-08eb-4014-b482-3430ae73c40c", "e7f89d06-8a09-4ca1-aeaf-e265e00c7bca"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\281.txt", "file_name": "281.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "67e1f39b-a8fc-4a1d-9564-c2704fc50f83": {"node_ids": ["5166aef1-3aea-448a-9814-83a8f5493185"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\282.txt", "file_name": "282.txt", "file_type": "text/plain", "file_size": 849, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f8a7ae6a-91f7-4efb-8c9c-8922385a814b": {"node_ids": ["3aca073e-96bf-435d-8e74-d173f9d5ca15"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\283.txt", "file_name": "283.txt", "file_type": "text/plain", "file_size": 505, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "dc6d24de-47ad-48e0-bd7a-77ce5666fbcc": {"node_ids": ["bb82a763-a146-44b4-9edb-210134feef54", "377a4c7d-dbf6-4dc4-ae4b-11918c037062", "87f260b9-5a4b-4692-a875-606efce54f02", "7ae5ef6f-8505-4454-a4ab-d4ae4bb89511"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\284.txt", "file_name": "284.txt", "file_type": "text/plain", "file_size": 1386, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7f7267bc-a13a-43cf-9ef7-db2c9ff1c252": {"node_ids": ["cee8624d-44b8-4b31-9025-1ed16e48d090"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\285.txt", "file_name": "285.txt", "file_type": "text/plain", "file_size": 673, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "61e4a964-0740-48d8-8268-f714df1b37f6": {"node_ids": ["a5073d26-4061-4667-87a2-954e6ec492bb"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\286.txt", "file_name": "286.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "dd75d9cc-0703-4376-9260-1172239c346c": {"node_ids": ["bb4f1aee-5c59-4b42-a2dd-c2e15e16eb0e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\287.txt", "file_name": "287.txt", "file_type": "text/plain", "file_size": 752, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "498780bd-084c-436b-b101-114bd40fc00f": {"node_ids": ["f29e0489-f5b6-40a1-80a0-75959ada2449"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\288.txt", "file_name": "288.txt", "file_type": "text/plain", "file_size": 1035, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "aaf932b9-ae21-4b9f-8ae2-e3ab1a1c574a": {"node_ids": ["123f2acf-5b38-41a4-bad2-0e38e247eb1a", "fa9b1d42-6863-4bea-92c3-459afb3e14c9", "f92f839f-0a59-4333-baa4-c896467827f7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\289.txt", "file_name": "289.txt", "file_type": "text/plain", "file_size": 1279, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "63ad6d6e-f431-4dd2-834c-381c4698680c": {"node_ids": ["26479a0b-868b-4989-aeb6-009f32b9a5c0", "d77deb0a-cdbb-4438-bc81-2bba3f6396f7", "bb507157-ddc1-43b6-b3c5-db02cc20adfd", "7c3f7ee5-af53-4668-9ff7-cf2cbbc7e7ed", "9074351a-a1d1-41be-bb1a-579f59bf5e81"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\29.txt", "file_name": "29.txt", "file_type": "text/plain", "file_size": 1508, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a7b811d0-431d-4178-aaf5-69971b5cef81": {"node_ids": ["ad2adaaa-9363-4636-8d8e-3f32488e03b2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\290.txt", "file_name": "290.txt", "file_type": "text/plain", "file_size": 487, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "98b7a51e-d10e-4478-bcf1-11a2c20fb61b": {"node_ids": ["0c4db018-701d-4b13-9c02-d4f26681f132"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\291.txt", "file_name": "291.txt", "file_type": "text/plain", "file_size": 841, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ad0d3a70-5a2a-4f0a-8714-24ea680cd55d": {"node_ids": ["6d7522ab-208b-4313-af5c-7bb15e9d2242"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\292.txt", "file_name": "292.txt", "file_type": "text/plain", "file_size": 848, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "228b53cf-a254-4aec-87a0-a7d8de7477a1": {"node_ids": ["861f1f8b-77a1-49f7-9af4-1fc2d4f8090e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\293.txt", "file_name": "293.txt", "file_type": "text/plain", "file_size": 76, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2461ef18-ddee-46ea-8a3c-09b960d4b6b4": {"node_ids": ["babcb2dc-0fcb-4374-a043-58409b0ca870", "2fbd0573-143a-4e00-b824-8cb012b46335"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\294.txt", "file_name": "294.txt", "file_type": "text/plain", "file_size": 1170, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0075cbf1-8468-4518-b3fe-a27aeb62c3db": {"node_ids": ["9b5660a1-c47f-4e39-9201-ca9c195b791a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\295.txt", "file_name": "295.txt", "file_type": "text/plain", "file_size": 82, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "1bd5f3e2-0a5c-49f8-9a8b-67b5da7d4e2c": {"node_ids": ["00a6d016-18e1-48c5-8e20-df850088a030"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\296.txt", "file_name": "296.txt", "file_type": "text/plain", "file_size": 468, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "df301cba-68c1-432b-8dd8-bb0de0bd1872": {"node_ids": ["fe6f6bac-9d08-4d81-9090-b7f63c989442"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\297.txt", "file_name": "297.txt", "file_type": "text/plain", "file_size": 525, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ff31062d-2f43-423a-bf02-d48212ee969b": {"node_ids": ["70bef62f-fc34-413d-9105-5f9204a08c25"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\298.txt", "file_name": "298.txt", "file_type": "text/plain", "file_size": 1118, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b444d494-b7dd-47ba-9d4a-1d2a3e0c66cd": {"node_ids": ["a8a9e263-e33e-4ccf-bf0a-f7df175e5b84", "bd84b1cd-4ce6-4bc5-b9f6-c8cbafddb0ac", "fbd78def-1a86-4c3b-b87f-8059aec7b3a9", "cbc1ff14-3215-4567-8d4c-53fd3947c25a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\299.txt", "file_name": "299.txt", "file_type": "text/plain", "file_size": 1165, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "fd0d0fcc-2b2e-4b73-b503-deb56f81463e": {"node_ids": ["99291768-b043-47c2-ad4f-88a454c82fd2", "d88b889e-113b-46a5-b08c-30fb90880dcc"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\3.txt", "file_name": "3.txt", "file_type": "text/plain", "file_size": 1139, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "63428642-3509-467a-ba84-47ea8d8e92ad": {"node_ids": ["ee148675-b135-4828-929e-bf31b2f79e5d"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\30.txt", "file_name": "30.txt", "file_type": "text/plain", "file_size": 878, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "6313a2c7-5be6-422e-a676-771d1ba19f96": {"node_ids": ["bb31603c-5f65-4ed5-aa47-f9e969bf9ef6", "a600ce7d-addd-471c-b829-f99b0c11c8e2", "099098be-9ab2-45ce-938c-0f958a7ee92c", "740189d2-e803-49a7-b6d5-8b8d294da86b", "c138bc0f-ab83-4dc8-9696-71cd77552027", "9549336e-1d13-44b3-9425-b6211314e0ed"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\300.txt", "file_name": "300.txt", "file_type": "text/plain", "file_size": 1779, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "228687d9-185d-42d4-bf1b-6a0ebf05ed48": {"node_ids": ["bc912460-0b4d-4998-8ab3-8b241449f455"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\301.txt", "file_name": "301.txt", "file_type": "text/plain", "file_size": 348, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4d41022e-a1c8-48c4-8925-cd58306a3947": {"node_ids": ["9269b9de-9c68-497d-ba6a-bb26a2269dfe"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\31.txt", "file_name": "31.txt", "file_type": "text/plain", "file_size": 466, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9e6772e8-479e-497b-83ce-b5728aafc463": {"node_ids": ["aa7af594-4220-4ce8-9c84-a5c46cb526d7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\32.txt", "file_name": "32.txt", "file_type": "text/plain", "file_size": 83, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d2ef52d7-8f67-44d8-be0b-1539687c0ef2": {"node_ids": ["1086f5af-2027-46dc-9976-a84b4b4c8f43"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\33.txt", "file_name": "33.txt", "file_type": "text/plain", "file_size": 547, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2f4609be-679b-454d-9662-bf69d41bf921": {"node_ids": ["36fa2c74-d7bd-4a0a-8b97-5d088a6c25c6"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\34.txt", "file_name": "34.txt", "file_type": "text/plain", "file_size": 935, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e4597f15-99cf-435f-9a04-ed4c844c96a3": {"node_ids": ["786b47bf-75a5-4fc9-b93e-c51f6919985a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\35.txt", "file_name": "35.txt", "file_type": "text/plain", "file_size": 490, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "fedf7284-8bd7-449d-8583-15c4741e9a40": {"node_ids": ["de632f38-0ca3-4a6f-9b3b-b237ce14ed74"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\36.txt", "file_name": "36.txt", "file_type": "text/plain", "file_size": 499, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "15ea1a97-9daf-40f6-b912-518052c443f5": {"node_ids": ["08478107-c0d1-437b-8c33-b8d84b1e647f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\37.txt", "file_name": "37.txt", "file_type": "text/plain", "file_size": 669, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d82a2a6b-826d-4e9e-815f-2e66499598bd": {"node_ids": ["6b889bf1-fd55-491c-ad76-7c279aa4f7c6", "c1b74f2b-e4e0-4e9e-a306-aad72168e436", "5bc47470-bed2-4eb9-a850-352d5923ea7d", "5fa2089b-4515-4f60-ad2e-9e409a4ad72e", "de6deba2-c3f5-41c7-8a27-9cdff1a5ac7b", "75b63164-962c-48de-8378-246464d714ab", "2907a7dc-a527-4850-bc63-db7a68f6abd9", "37d3d69e-55cf-4f4e-b4db-238afc02b8b9", "2fbb8bb7-fe2a-4b31-85b4-1c544de51090"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\38.txt", "file_name": "38.txt", "file_type": "text/plain", "file_size": 2127, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a49a291e-81cf-44f8-ab6a-29ed0d9e2a7e": {"node_ids": ["3039edf2-acef-4655-b867-bb6228b5042e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\39.txt", "file_name": "39.txt", "file_type": "text/plain", "file_size": 376, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a999894d-7b98-4e77-b962-13f1a9ed7ddc": {"node_ids": ["4f7eb92b-6b71-45b2-a50f-364f6993b98d", "01649a08-dd5b-495f-aa60-8a461196d3a3", "06344451-03dd-4161-8b01-059e32e1ad43", "c99e8179-bdca-4610-95d2-69e7ec922d5d", "05d4e123-c41d-495f-b941-c0978c7d90be", "4ddad4dc-19f2-48c5-819b-34739d83325a", "16010fc3-becf-43c1-b25d-100b868973fb"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\4.txt", "file_name": "4.txt", "file_type": "text/plain", "file_size": 1817, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "60ade05e-ea05-4c4a-923e-714df5e6b162": {"node_ids": ["f16d9244-dcc6-412b-ae05-a0f8b79c18ff", "ba3d462e-0434-43a1-959d-7b7bf1ee7491", "be2af7a5-883e-40fb-830e-eb0635b0f400", "d2a6678a-3dd6-4775-aa08-c51ae83b4734", "6f46ba24-327d-413d-a6a0-fc13f01f61f4", "ce70a585-6e18-4f30-b5be-ea101e738c13", "b8ed2b9c-5b6b-48d7-b4e9-cee69dd75c61", "e78a53d3-a097-42a3-9d11-8e7d6d4539a6", "e1eafa26-9e1e-4623-a04b-ede8deee9484", "b8b5a20a-fa76-4984-b285-fe5fd39a4ed6", "e75b966d-b1ee-49ae-abd0-1f58622f3e37", "fa391b96-75df-4892-9890-35d1308fdfe8", "d351b4fa-bb02-43e3-a9a7-bed30990573f", "37fb48ae-48d8-4988-b14a-664f0745d96f", "75378eb4-acd8-4eeb-a183-65f23b9637cc"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\40.txt", "file_name": "40.txt", "file_type": "text/plain", "file_size": 2716, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "af4db40e-101d-4953-9ef1-9bae0a723f27": {"node_ids": ["c42224a6-c112-4000-b057-66731c528d69"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\41.txt", "file_name": "41.txt", "file_type": "text/plain", "file_size": 358, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "40130f56-0714-4ad9-9a60-30154ed89569": {"node_ids": ["82fca539-d612-4664-b565-99467442cf4f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\42.txt", "file_name": "42.txt", "file_type": "text/plain", "file_size": 856, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "25a12674-4683-44b2-90a8-fc3ca4814280": {"node_ids": ["3eee76be-6875-4ed1-92a4-1ae0f8799f3c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\43.txt", "file_name": "43.txt", "file_type": "text/plain", "file_size": 615, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "3cab912f-730d-4daa-82b6-c812d016f809": {"node_ids": ["072c5eb8-d4c3-4cf3-9220-02b33f813271"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\44.txt", "file_name": "44.txt", "file_type": "text/plain", "file_size": 943, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "723f36c7-767e-4956-986c-7c26a50a3958": {"node_ids": ["a47dd41e-f7f4-409e-96a0-1c74f339b9f9"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\45.txt", "file_name": "45.txt", "file_type": "text/plain", "file_size": 740, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "501117bb-59b3-4f0e-8c4b-d9fa547ef48e": {"node_ids": ["d275ded3-2c1c-48de-813e-a9f529828db1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\46.txt", "file_name": "46.txt", "file_type": "text/plain", "file_size": 81, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9a20f4ac-f1fa-441a-8512-1f1fccb73a5d": {"node_ids": ["602a8439-a01d-48cc-ab4d-44216bade9d0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\47.txt", "file_name": "47.txt", "file_type": "text/plain", "file_size": 712, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "906298b4-363f-467d-bb6a-e30c9f7a047d": {"node_ids": ["f2932a65-667a-4131-ac06-896b1377cda7"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\48.txt", "file_name": "48.txt", "file_type": "text/plain", "file_size": 639, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4fcb5c77-561a-4bf9-a03a-b9960b0a6e8f": {"node_ids": ["95070a48-5cbc-4949-be75-e520eec6a29c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\49.txt", "file_name": "49.txt", "file_type": "text/plain", "file_size": 648, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8c122559-c8be-416d-a1c5-36181ac1ee65": {"node_ids": ["90f77b72-e08a-492e-9f57-54a84f7ae51e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\5.txt", "file_name": "5.txt", "file_type": "text/plain", "file_size": 808, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "122baf94-a4e4-4a13-b77c-03ff283b931c": {"node_ids": ["9705f8f0-5fbb-43b5-9cd6-1765d1a5fe80"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\50.txt", "file_name": "50.txt", "file_type": "text/plain", "file_size": 1076, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "94880efd-8fbb-469f-9dca-545dd3277949": {"node_ids": ["760e0fe7-7e00-4f44-a628-453262461e91"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\51.txt", "file_name": "51.txt", "file_type": "text/plain", "file_size": 805, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "08354a60-7e6d-40d4-a8cb-37270b552ef1": {"node_ids": ["b167d900-2bfd-4dd4-a2d3-e7cf546af900", "c0624af0-9600-4752-8d8f-82a1d8da62c1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\52.txt", "file_name": "52.txt", "file_type": "text/plain", "file_size": 1429, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d9c3b170-0bbd-4182-be76-ff3b15d7cc91": {"node_ids": ["be6cf76b-2076-444f-9022-f723ef281940"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\53.txt", "file_name": "53.txt", "file_type": "text/plain", "file_size": 891, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "af821be8-f438-40d9-b136-b205dbf07270": {"node_ids": ["eacfa080-5667-4bdb-8293-a56c66005fe0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\54.txt", "file_name": "54.txt", "file_type": "text/plain", "file_size": 606, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "add56e7b-15f4-4e48-b9a3-e63af372e931": {"node_ids": ["10b3f876-9d79-434d-8359-db4fbf1bea18"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\55.txt", "file_name": "55.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "25decb75-1742-45d0-b05f-f433d285fc43": {"node_ids": ["21aebf4d-30f3-43f4-8800-e4adb871a059"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\56.txt", "file_name": "56.txt", "file_type": "text/plain", "file_size": 730, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a4e2410e-cb0f-48de-b8b4-0bb4a201f854": {"node_ids": ["4543bc8a-81e6-4642-9d42-34357ca621b2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\57.txt", "file_name": "57.txt", "file_type": "text/plain", "file_size": 72, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e7b8cb34-e822-4487-aa4e-f0530099b8b4": {"node_ids": ["cdfe54d9-4e53-4637-8812-d5c5457aa426"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\58.txt", "file_name": "58.txt", "file_type": "text/plain", "file_size": 811, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "61dd2dff-57e9-42e1-b727-cadc5badbd9a": {"node_ids": ["7974f9a4-5225-497e-b66f-9cb7668ef9e6", "3c2525a9-8b6e-40dc-85de-1d8e056c7a21"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\59.txt", "file_name": "59.txt", "file_type": "text/plain", "file_size": 1074, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "48947c14-e046-4c7e-965b-ae53e9da43ef": {"node_ids": ["8edde0a3-71da-4dd3-826d-021ad09369f6"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\6.txt", "file_name": "6.txt", "file_type": "text/plain", "file_size": 821, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "60f7fb2d-59b7-408d-b53b-2563e901b26a": {"node_ids": ["7a0a7bdd-02cd-4a7f-ba4f-e19f7b6ebdb1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\60.txt", "file_name": "60.txt", "file_type": "text/plain", "file_size": 80, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ca6985b7-73e4-4d36-abb0-0f7e317f1196": {"node_ids": ["b7d1dce6-73de-473f-9321-ab6e56218c85"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\61.txt", "file_name": "61.txt", "file_type": "text/plain", "file_size": 75, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a5f58532-b563-42cb-97b8-180547eae720": {"node_ids": ["30d4c0bd-28ae-4e72-b345-09e13a88fcef"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\62.txt", "file_name": "62.txt", "file_type": "text/plain", "file_size": 991, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "5047b9fb-a464-4744-ad60-6223e2b36fc0": {"node_ids": ["bdc5a975-4138-44dd-8968-f7cd77d8c145"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\63.txt", "file_name": "63.txt", "file_type": "text/plain", "file_size": 68, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4a8bfb97-6ec5-43db-b980-1dc2c1ac6544": {"node_ids": ["fe285b24-4ca3-4c1a-80c2-fd9fca7d6d02"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\64.txt", "file_name": "64.txt", "file_type": "text/plain", "file_size": 1117, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "1bc7ae7c-fd0b-4120-a5a1-44880b1d2f43": {"node_ids": ["e8cc1d8e-ef60-4ce4-99cb-683bf811dd45"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\65.txt", "file_name": "65.txt", "file_type": "text/plain", "file_size": 858, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2a310b6c-d302-4fda-ba62-15c8d623ed6e": {"node_ids": ["027932e1-9599-48cb-9a5c-35a493fe5cc1"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\66.txt", "file_name": "66.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d872e017-c394-4b92-85c9-fed2a7d72483": {"node_ids": ["ebe20b30-9ae6-4253-adbc-027035bf395b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\67.txt", "file_name": "67.txt", "file_type": "text/plain", "file_size": 1059, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f057991d-4465-4fcc-b9d5-4afbf680e892": {"node_ids": ["27452635-9ecc-4605-a4e8-e82428b5ff51", "335aebce-6f88-4b1c-a55a-ac2b7c5bba62", "616a6886-8d9d-4792-a214-501ce218c32f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\68.txt", "file_name": "68.txt", "file_type": "text/plain", "file_size": 1178, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b25d4a08-14cd-4e16-86c0-cb0ad3112787": {"node_ids": ["bd3ec216-1bd5-4277-8a3b-596ce5856833"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\69.txt", "file_name": "69.txt", "file_type": "text/plain", "file_size": 766, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "0f5d66d8-7b45-4e0d-a988-3ef1de75b0db": {"node_ids": ["6ecd05ea-d5b6-4472-94f7-96b40c20afac"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\7.txt", "file_name": "7.txt", "file_type": "text/plain", "file_size": 777, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "3c564ca4-ed6a-4aa7-81e9-4d09d27db662": {"node_ids": ["cbca2355-9e03-4578-86b4-d69eff1bd261"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\70.txt", "file_name": "70.txt", "file_type": "text/plain", "file_size": 548, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "3ae318a9-5e4c-4570-ba7a-016dc0483cc9": {"node_ids": ["dd7e5132-590f-454f-8a8e-0fa1c18fcf2b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\71.txt", "file_name": "71.txt", "file_type": "text/plain", "file_size": 71, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "95c64dc9-e4d0-48a0-a35f-8cbd0295e49a": {"node_ids": ["19bc7068-b965-4215-964d-47d2ccffd2a3", "3d7ce476-afb9-4530-8ec3-60b33d664879"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\72.txt", "file_name": "72.txt", "file_type": "text/plain", "file_size": 1129, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c0b3bf09-dfcd-49f7-84b0-d4c2d938405c": {"node_ids": ["0fbcc06c-8e51-4b83-bfb1-1e97e23d06a4"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\73.txt", "file_name": "73.txt", "file_type": "text/plain", "file_size": 888, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9100a7ef-bb38-4af8-b781-2a2da09b770e": {"node_ids": ["bf5a925b-fb04-4948-bc3b-e31c0cffb13b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\74.txt", "file_name": "74.txt", "file_type": "text/plain", "file_size": 799, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "80e1e671-52b4-4881-8395-75c43e1f5873": {"node_ids": ["832789b9-32b8-4748-8740-71a63a7212e3"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\75.txt", "file_name": "75.txt", "file_type": "text/plain", "file_size": 906, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b18e20bb-2c35-4550-a71a-a0eeb94fb2f2": {"node_ids": ["81b72515-dcd4-4464-bf2f-2684154095ee"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\76.txt", "file_name": "76.txt", "file_type": "text/plain", "file_size": 687, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f76d1e22-b47b-4e06-a0b2-7b42081c8e69": {"node_ids": ["7648935b-7a08-4c30-a23f-d542b50f87ab"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\77.txt", "file_name": "77.txt", "file_type": "text/plain", "file_size": 770, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "01a714d9-e060-4d29-908d-1e12c2ef9f19": {"node_ids": ["71beb6e9-5333-428f-bea2-84a500615725"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\78.txt", "file_name": "78.txt", "file_type": "text/plain", "file_size": 884, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2230f129-480d-4a25-8cbd-5f93a11c60c1": {"node_ids": ["1591a5ac-8621-4e2f-a312-4f2beb74fa2f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\79.txt", "file_name": "79.txt", "file_type": "text/plain", "file_size": 882, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "2878648d-faea-4d33-984f-71bb6b8d2481": {"node_ids": ["ab46a04b-cfe3-4df6-bbd2-fa6288e9dee0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\8.txt", "file_name": "8.txt", "file_type": "text/plain", "file_size": 200, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7f7bb37f-c1b1-49d1-addc-c75f8024b6f3": {"node_ids": ["c8b84100-1703-4246-828f-dc6f642bf091"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\80.txt", "file_name": "80.txt", "file_type": "text/plain", "file_size": 73, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d8e4cac6-b7dc-4a85-aef4-f4f2dcbc7251": {"node_ids": ["6bd1fadb-3633-4415-984f-a37f69b7b86b"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\81.txt", "file_name": "81.txt", "file_type": "text/plain", "file_size": 731, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "9d19ab6d-0912-46f6-8a4d-310f45cc45cb": {"node_ids": ["0847261c-16c9-4f84-9390-0b187f15d5b3", "3fe7a9ad-7333-4c02-b0a8-3f326e351a47", "8ec304d0-3da3-4f69-8c06-81a7c0e073bf"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\82.txt", "file_name": "82.txt", "file_type": "text/plain", "file_size": 1322, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "52dec9ab-f3e4-4763-9cd8-d383f66aedec": {"node_ids": ["3acd92b5-55a2-47f0-b82f-937fa5d93cc2"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\83.txt", "file_name": "83.txt", "file_type": "text/plain", "file_size": 77, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "1474d637-b80e-4c6c-8a72-79247cdb139c": {"node_ids": ["82e7b984-c674-4519-b6f3-ec26b5ef15b0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\84.txt", "file_name": "84.txt", "file_type": "text/plain", "file_size": 713, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "c2b741a4-1417-44b2-a4cd-1e0b040c3b87": {"node_ids": ["adb003e6-404a-42b7-acd9-8091e0915c89"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\85.txt", "file_name": "85.txt", "file_type": "text/plain", "file_size": 584, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "ba841022-58ec-44ca-a43b-39961be87b33": {"node_ids": ["2b8b6c74-f610-4a30-9510-b50dce0e6ad0"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\86.txt", "file_name": "86.txt", "file_type": "text/plain", "file_size": 1078, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "84ef711b-781e-425b-9aa4-c5587f9b95da": {"node_ids": ["005d0d86-2d03-48a7-b96d-b0a1e3ec0464"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\87.txt", "file_name": "87.txt", "file_type": "text/plain", "file_size": 586, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "7e6eac33-6916-41b4-8be7-1a725f838684": {"node_ids": ["2fac5f96-bcb2-49c7-8c8a-590981146891"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\88.txt", "file_name": "88.txt", "file_type": "text/plain", "file_size": 864, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "f3ee54c0-04da-45f1-9329-437b41468049": {"node_ids": ["c4b5b2e3-99fd-4527-a94c-a983ed5225e3"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\89.txt", "file_name": "89.txt", "file_type": "text/plain", "file_size": 393, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "4b1fd6b4-7d4e-41fa-ad42-a9fbe68f7bf2": {"node_ids": ["24a13d41-d222-45da-8048-f5b2359dd043"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\9.txt", "file_name": "9.txt", "file_type": "text/plain", "file_size": 423, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "30dcdd05-eeb3-4645-a18c-d36473030b1c": {"node_ids": ["c78bb835-c426-49e4-86e1-2fde5db2e31f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\90.txt", "file_name": "90.txt", "file_type": "text/plain", "file_size": 815, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "1cea5fff-9f8f-46de-b7d9-43f9d831eefd": {"node_ids": ["9efbbbe5-3dbd-43aa-bf38-e07d6846062e"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\91.txt", "file_name": "91.txt", "file_type": "text/plain", "file_size": 802, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "b1951614-9d46-45d6-9977-1315fb47372e": {"node_ids": ["62895b8b-0db8-448e-bb80-6ba652d21441"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\92.txt", "file_name": "92.txt", "file_type": "text/plain", "file_size": 349, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "d9f7c5fa-4284-407e-b043-f602c5f22629": {"node_ids": ["ac3585b6-c617-469b-8f97-918b88a25e0d", "49d3f981-e034-4fb0-aa02-7ea418171032", "7962e109-337f-40be-b319-4d3b447d5807"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\93.txt", "file_name": "93.txt", "file_type": "text/plain", "file_size": 1072, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e0d3428d-f1d4-4954-9770-b1acb9f38528": {"node_ids": ["3d790a79-cff2-4554-8f83-919bbb75db5a"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\94.txt", "file_name": "94.txt", "file_type": "text/plain", "file_size": 202, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "92f96cc5-fb08-431c-8a82-2243eba56518": {"node_ids": ["ee1f883c-a7ed-404f-8177-d4360d4ac04c"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\95.txt", "file_name": "95.txt", "file_type": "text/plain", "file_size": 962, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "e3f87ce7-f81a-4860-9272-74316bb0e0fb": {"node_ids": ["ae0a7dd8-543a-4a38-97e9-ef9bf190fd8f"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\96.txt", "file_name": "96.txt", "file_type": "text/plain", "file_size": 234, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "51a6cfaf-27f9-47e8-9134-bb649e724485": {"node_ids": ["d22d01d8-0e3e-46d7-a880-0382d7624d37", "d94545c6-1461-4d44-9ace-e676c17bef44", "5d9cb584-ffc7-4acc-ba26-4c50b4a0da62"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\97.txt", "file_name": "97.txt", "file_type": "text/plain", "file_size": 1315, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "a703a98c-0cf4-4245-ba39-b360db3499c1": {"node_ids": ["e0b732ec-9a87-4e74-a73a-933ae89d0aed"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\98.txt", "file_name": "98.txt", "file_type": "text/plain", "file_size": 855, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}, "8469d484-e5c1-4c43-a63c-540e9f6ce250": {"node_ids": ["03f1a526-d53f-4f12-8f1c-7806f0070334", "4bed57c7-f2a4-42ea-8fbe-c9ecc91a3189"], "metadata": {"file_path": "C:\\Users\\Nisch\\OneDrive\\Documents\\HiWi Job\\sdsc-cataglog\\sdsc_cataglog\\data\\github_repositories\\99.txt", "file_name": "99.txt", "file_type": "text/plain", "file_size": 1216, "creation_date": "2024-04-29", "last_modified_date": "2024-04-29"}}}}