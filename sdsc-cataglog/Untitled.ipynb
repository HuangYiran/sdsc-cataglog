{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f865e5-abb6-4e17-9ff7-5967a9d601e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/tm/hj7422/conda/envs/vicuna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from git_request import search_top_starred_repositories, search_best_match_repositories\n",
    "from openml_request import search_top_related_local_datasets_with_chatgpt, search_newest_datasets, search_top_related_local_datasets_with_cs\n",
    "from gpt_request import get_response_from_chatgpt_with_context\n",
    "from localgpt_request import load_text_generation_pipeline\n",
    "\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0742fd92-0331-48cc-9878-493f9d26d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these package\n",
    "import requests\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752eb5f-8848-4d14-ab0b-15bdaf932a9e",
   "metadata": {},
   "source": [
    "we can also seperate these parckage and import them when necessary. Maybe add some explaination of the function when import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878c7f69-e21a-4bd5-a218-2bae9c0cc733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sdsc_cataglog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c71c70-a287-42f1-a776-8d9b6205fb65",
   "metadata": {},
   "source": [
    "## Test text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b5756a3-16c7-445a-98e7-a4ca8be51f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" #一般\n",
    "#model_name = \"Open-Orca/Mistral-7B-OpenOrca\" # 333s 结果还行\n",
    "#model_name = \"HuggingFaceH4/zephyr-7b-alpha\" # 这个挺好，时间上也是很长 151\n",
    "#model_name = \"meta-llama/Llama-2-7b\" # no config.json\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" #等待时间过长\n",
    "#model_name = \"replit/replit-code-v1-3b\" # ReplitLMTokenizer not found\n",
    "#model_name = \"liuhaotian/llava-v1.5-13b\" # keyerror llava\n",
    "#model_name = \"stabilityai/stablelm-3b-4e1t\" # str object is not callable\n",
    "#model_name = \"lmsys/vicuna-7b-v1.5\" 不行\n",
    "#model_name = \"teknium/Mistral-Trismegistus-7B\" 等待时间过长\n",
    "#model_name = \"gpt2\"  效果不行\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\" # 挺好，就是等得久152s\n",
    "#TO test: liuhaotian/llava-v1.5-13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a00283-6e61-4903-b8f2-799ddbff0130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.77s/it]\n",
      "/pfs/data5/home/kit/tm/hj7422/conda/envs/vicuna/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72.68264412879944"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "nlp = load_text_generation_pipeline()\n",
    "t2 = time.time()\n",
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366ecaa-b1d6-49b4-a4ed-946d35d550cb",
   "metadata": {},
   "source": [
    "Here we will download and built a local gpt model for text generation. The model we selected is about 15GB, so it will take a while. \n",
    "\n",
    "introduce about the nlp and what kinds of model we will use and why here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67085662-e3d8-4f81-a732-2a066ae479a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_case_e = 'This task was associated with two difficulties: 1) the requirement of interpretability limits the choice of models and 2) the large number of attributes stored as text complicates the analysis and at the same time affects the performance of some models.There are many complex models whose decisions are difficult to explain, deep neural networks are one of the typical examples. Attributes stored using text are mostly contextual and can be handled well by semantic analysis, but it requires a large amount of data and time and is not interpretable. Instead, some simple methods can add or remove information.'\n",
    "use_case = 'Diese Aufgabe war mit zwei Schwierigkeiten verbunden: 1) die Anforderung der Interpretierbarkeit schränkt die Auswahl der Modelle ein und 2) die große Anzahl als Text gespeicherter Merkmale erschwert die Analyse und beeinträchtigt gleichzeitig die Leistung einiger Modelle.Es gibt viele komplexe Modelle, deren Entscheidungen schwer zu erklären sind, tiefe neuronale Netze sind eines der typischen Beispiele dafür. Attribute, die mit Hilfe von Text gespeichert werden, sind meist kontextabhängig und können durch semantische Analyse gut behandelt werden, aber sie erfordert eine große Menge Daten und Zeit und ist nicht interpretierbar. Stattdessen können bei einigen einfachen Methoden Informationen hinzugefügt oder entfernt werden.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563de4dd-f8f0-4ce2-b760-4e63023a5e48",
   "metadata": {},
   "source": [
    "Here is the use case i get from our website. Since the llm is not as powerful as the one online. Only english work here.\n",
    "\n",
    "Maybe add some explaination of the use case here. What is the use case about and what kinds of challenges is he faccing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1e2f95-5313-4229-bebc-b536e2955b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.83806991577148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp(question = QA_input['question'], context = QA_input['context'])\n",
    "# \n",
    "t1 = time.time()\n",
    "rsl = nlp(\"I got a use case: \" + use_case_e + \" I can summarise the chanllenges mentioned in this use case with three keywords: \",\n",
    "          num_return_sequences = 1,\n",
    "          # temperature = 1.0,\n",
    "          # num_beams = 5, # \n",
    "          # top_k =  50\n",
    "         )\n",
    "t2 = time.time()\n",
    "t2-t1\n",
    "#nlp(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563bf900-106d-4130-be65-f90a43d4a537",
   "metadata": {},
   "source": [
    "we will limit some function of the model, so that the model will not take too long to wait.\n",
    "\n",
    "explain some parameter and their effect here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf22bc9-b7fc-4cf2-9ab1-562172df67c4",
   "metadata": {},
   "source": [
    "The most popular size of llm used locally is 3b, 7b, 30b, 70b. Here, 3b means the model contain 3 billion parameters. The larger the model, the better its performance. The size of chatgpt we used online is larger than 70b. Therefore, it's performance is better than the one we use here. For this presentation, due to the limitation of applied resource, we use a llm with about 7 billion parameters. The size of the model is about 15GB. And one single request takes about 2 mins. This prcess can be accelerated through torchserver and parallelization. \n",
    "\n",
    "talk about the acceleration story...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073809b0-f2da-4762-832e-92f8db494efe",
   "metadata": {},
   "source": [
    "The llm model here is text generation. text generation model always add text to the original data which make the extraction of the answer difficult. To simplify the task, we need to add a special sign at the end of the given sentence. This sign should not affect the meaning of the text and should not be contain in the test. We use '::' here. \n",
    "\n",
    "talk about the promt generation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d948697b-8e08-4ed4-9bd0-1ebee218cec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'I got a use case: This task was associated with two difficulties: 1) the requirement of interpretability limits the choice of models and 2) the large number of attributes stored as text complicates the analysis and at the same time affects the performance of some models.There are many complex models whose decisions are difficult to explain, deep neural networks are one of the typical examples. Attributes stored using text are mostly contextual and can be handled well by semantic analysis, but it requires a large amount of data and time and is not interpretable. Instead, some simple methods can add or remove information. I can summarise the chanllenges mentioned in this use case with three keywords: 1) interpretability, 2) complexity, and 3) textual attributes.\\n\\n\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aea801a-be9f-4556-b987-60dc05d54e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interpretability', 'complexity', 'textual attributes.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\d+\\.|\\)\\s+([^0-9|,]+)(?=\\d+|\\.|\\s|,|$)'\n",
    "keywords = re.findall(pattern, rsl[0]['generated_text'].strip().split('keywords:')[-1].strip())\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a496fca-6ecb-46d2-b3cb-fc0d60dfabfe",
   "metadata": {},
   "source": [
    "Since the output of the llm model are always not uniform. In this example, it use ')' after the number, however, sometimes it use the '.'. This can enlarge the difficult to extract the keyword. Here we use the 're' package to do the job.\n",
    "\n",
    "Add some explaination about the re pattern used here...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341e6d5-fec6-42a1-a435-1578b7e92fc7",
   "metadata": {},
   "source": [
    "Go back to our main story line. Here, for each keyward, we want to ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71ca002e-4cf2-4844-b2c8-5d8bc0f3656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# def search_best_match_repositories(keyword, num = 5):\n",
    "#     url = \"https://api.github.com/search/repositories\"\n",
    "#     headers = {\n",
    "#         \"Accept\": \"application/vnd.github.v3+json\"\n",
    "#     }\n",
    "#     params = {\n",
    "#         \"q\": keyword,\n",
    "#         \"sort\": \"best_match\",  # 按星数排序\n",
    "#         \"order\": \"desc\",  # 降序排列\n",
    "#         \"per_page\": num    # 获取前5个结果\n",
    "#     }\n",
    "#     response = requests.get(url, headers=headers, params=params)\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         repositories = data[\"items\"]\n",
    "#         repo_urls = []\n",
    "#         readme_urls = []\n",
    "#         for repo in repositories:\n",
    "#             repo_url = repo[\"html_url\"]\n",
    "#             repo_urls.append(repo_url)\n",
    "#             readme_urls.append(repo_url + \"/blob/master/README.md\")\n",
    "#             #stars_count = repo[\"stargazers_count\"]\n",
    "\n",
    "#         return repo_urls, readme_urls\n",
    "#     else:\n",
    "#         print(\"Error:\", response.status_code)\n",
    "#     return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78138917-d588-4e7f-892c-e964dd7e322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the interpretability preblem, we recommend the following tools:\n",
      "Repository URL: https://github.com/jphall663/awesome-machine-learning-interpretability\n",
      "README URL: https://github.com/jphall663/awesome-machine-learning-interpretability/blob/master/README.md\n",
      "Repository URL: https://github.com/MAIF/shapash\n",
      "README URL: https://github.com/MAIF/shapash/blob/master/README.md\n",
      "Repository URL: https://github.com/h2oai/mli-resources\n",
      "README URL: https://github.com/h2oai/mli-resources/blob/master/README.md\n",
      "Repository URL: https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python\n",
      "README URL: https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python/blob/master/README.md\n",
      "Repository URL: https://github.com/interpretml/interpret-community\n",
      "README URL: https://github.com/interpretml/interpret-community/blob/master/README.md\n",
      "--------------------------------------------------\n",
      "For the complexity preblem, we recommend the following tools:\n",
      "Repository URL: https://github.com/PyCQA/mccabe\n",
      "README URL: https://github.com/PyCQA/mccabe/blob/master/README.md\n",
      "Repository URL: https://github.com/Mortal/complexity\n",
      "README URL: https://github.com/Mortal/complexity/blob/master/README.md\n",
      "Repository URL: https://github.com/codebasics/data-structures-algorithms-python\n",
      "README URL: https://github.com/codebasics/data-structures-algorithms-python/blob/master/README.md\n",
      "Repository URL: https://github.com/tonybaloney/wily\n",
      "README URL: https://github.com/tonybaloney/wily/blob/master/README.md\n",
      "Repository URL: https://github.com/minimaxir/simpleaichat\n",
      "README URL: https://github.com/minimaxir/simpleaichat/blob/master/README.md\n",
      "--------------------------------------------------\n",
      "For the textual attributes. preblem, we recommend the following tools:\n",
      "Repository URL: https://github.com/wafer110/Python-NLP-Analyze_TextualData_on_Reddit_Comments\n",
      "README URL: https://github.com/wafer110/Python-NLP-Analyze_TextualData_on_Reddit_Comments/blob/master/README.md\n",
      "Repository URL: https://github.com/TESNIERES/python-for-data-analysis\n",
      "README URL: https://github.com/TESNIERES/python-for-data-analysis/blob/master/README.md\n",
      "Repository URL: https://github.com/nurfateemah03/YelpDataBusinessAttributes\n",
      "README URL: https://github.com/nurfateemah03/YelpDataBusinessAttributes/blob/master/README.md\n",
      "Repository URL: https://github.com/GioLeiren/Scene-Attribute-data-analysis\n",
      "README URL: https://github.com/GioLeiren/Scene-Attribute-data-analysis/blob/master/README.md\n",
      "Repository URL: https://github.com/naseeraslam/AddressBook-Assignment-Python\n",
      "README URL: https://github.com/naseeraslam/AddressBook-Assignment-Python/blob/master/README.md\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "    git_urls, readme_urls = search_best_match_repositories(keyword+' python') # +' problem'\n",
    "    if git_urls is not None:\n",
    "        print(f\"For the {keyword} preblem, we recommend the following tools:\")\n",
    "        for git_url, readme_url in zip(git_urls, readme_urls):\n",
    "            print(\"Repository URL:\", git_url)\n",
    "            print(\"README URL:\", readme_url)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9dd560-88f4-41c8-a436-a87719469c17",
   "metadata": {},
   "source": [
    "There are two different ways to get the advises. The first one is used the api provied by the corresponding website. The example above is using the git api to get the relavant tools. We can see that it works good for the interpretability keywords. The advised tools can help to make the code or data more interpretable. It is due to the core of the search stragety implemented by the git api tool. A normal git repository contains information include: 'readme', 'keywords', and 'about'. The api will looking for the given keyword in these files and send the repository back when it is hit. However, it will not help for the second keyword 'complexity' since the word itself miss the context information. According to a single keyword it is hard to understand what problem we are dealing with. In this case, we will preper the second way to get the recommendation, using the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5501f9d0-2900-4847-b523-f8b706351e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "For the textual attributes preblem, we recommend the following datasets:\n",
      "Dataset Name: analcatdata_authorship\n",
      "Dataset description The dataset analcatdata_authorship contains information related to the authorship of various texts. It includes a total of 841 instances, each representing a text document. There are five attributes included in this dataset, which are used to categorize the texts. These attributes include the age of the text, the origin of the text, the type of text, its length, and its author. The dataset is intended to be used for authorship attribution analysis or any other related research that involves text analysis, classification, or identification of authorship patterns.\n",
      "Dataset URL: https://www.openml.org/d/970.0\n",
      "Dataset Name: page-blocks\n",
      "Dataset description The dataset \"page-blocks\" consists of scanned grayscale images of document pages. Each image is divided into equally sized blocks and labeled as either a text or non-text block. The dataset contains a total of 5,473 instances, with approximately 4% being non-text blocks and 96% being text blocks. Each instance is represented by 10 attributes, including the block width, height, the number of horizontal and vertical lines, and the xy ratio. The aim of this dataset is to classify blocks as text or non-text based on these attributes. This dataset can be useful for developing machine learning algorithms for document analysis and OCR (optical character recognition) tasks.\n",
      "Dataset URL: https://www.openml.org/d/30.0\n",
      "Dataset Name: albert\n",
      "Dataset description The dataset ALBERT is a language model that is pre-trained on a vast amount of text data. It has been specifically designed to understand written text and perform various language-related tasks. ALBERT is a BERT-based model, which means it incorporates a bidirectional transformer architecture to capture the context of words and generate representations. It has been trained on a wide range of tasks, including sentence-level and document-level classification, question answering, and natural language inference. ALBERT achieves state-of-the-art performance on multiple benchmarks and outperforms other language models. It is widely used for tasks like text classification, information retrieval, and text generation.\n",
      "Dataset URL: https://www.openml.org/d/41147.0\n",
      "Dataset Name: analcatdata_authorship\n",
      "Dataset description The dataset analcatdata_authorship contains information related to authorship of three novels written by different authors. It includes a total of 841 instances and 70 variables. The dataset provides various features such as character and word frequency, presence of specific words, average word length, and data regarding the authors themselves. The dataset aims to explore the differences in writing styles among the authors and to predict the authorship of unknown texts based on these variables. It can be used for tasks such as text classification, authorship attribution, and computational stylistics. The dataset is available for analysis and experimentation in the field of natural language processing and machine learning.\n",
      "Dataset URL: https://www.openml.org/d/458.0\n",
      "Dataset Name: w2a\n",
      "Dataset description The dataset w2a is a collection of documents representing web pages with their corresponding attributes. It includes a total of 14 attributes that describe different aspects of each webpage, such as the URL, page text, page title, anchor text, and anchor URL. The dataset contains a total of 51 instances, each representing a different webpage. These webpages cover a wide range of topics, including technology, education, and entertainment, among others. The dataset w2a is commonly used for various research purposes, including web page classification, information retrieval, and understanding page relevance.\n",
      "Dataset URL: https://www.openml.org/d/1582.0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#for keyword in keywords:\n",
    "keyword = 'textual attributes'\n",
    "dataset_names, dataset_ids, dataset_description, dataset_urls = search_top_related_local_datasets_with_cs([keyword])\n",
    "print(f\"For the {keyword} preblem, we recommend the following datasets:\")\n",
    "for ds_name, ds_des, ds_url in zip(dataset_names, dataset_description, dataset_urls):\n",
    "    print(\"Dataset Name:\", ds_name)\n",
    "    print(\"Dataset description\", ds_des)\n",
    "    print(\"Dataset URL:\", ds_url)\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d9943f-4065-4d58-a27f-b0d64776f520",
   "metadata": {},
   "source": [
    "In this example, we use the embedding to get the recommendation on dataset. The working process is as follow, 1. we download the descprition of all the dataset and generate word embedding according to the description. We save the dataset, id, descrption and embedding on the local disk. 2. we get the key word or problem description, use the llm to generate embedding of the given information. 3. We go through the local dataset and compare the embedding similarity. 4. We rank the similarity and get the top k as feedback. There are several advantage of using such process. 1. The keyword doesn't have to hided in the name or description. It's important since different word may have the same meansing e.g., explanability and interpretability. 2. We support to use the problem description as input, which may contain some important context information. 3. it doesn't rely on certain api. Taking openml api for example, it doesn't support blur search. 4. It can be achieved off-line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88855ac9-ce57-4daf-8b61-da5a03f2f7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicuna",
   "language": "python",
   "name": "vicuna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
