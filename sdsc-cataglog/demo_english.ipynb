{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e1eb9a-d9f0-4885-8aed-58a2e464f513",
   "metadata": {
    "tags": []
   },
   "source": [
    "# topic and background\n",
    "The main purpose of this project is to implement an application that can output github tools that correspond to the user input, in the case of the current challenge faced, and that help to deal with this challenge.\n",
    "\n",
    "We all know that the now popular chatgpt has shown a strong knowledge base in various fields. Especially when combined with bing search, it is able to provide corresponding websites directly on demand. But chatgpt has some limitations:\n",
    "1. chatGPT knowledge is trained on data before September 2021, so there is no way to provide information after that date.\n",
    "2. chatGPT has no way to analyze complex logical relationships. 3.\n",
    "3. ChatGPT cannot list cited sources, and its reliability is based on the reliability of the source information, which may be inherently wrong, inconsistent, or incorrect or contradictory after being combined by ChatGPT.\n",
    "\n",
    "The above three points lead to the fact that there is no way to accomplish the purpose of this project by using chatgpt directly. We can start by looking at this challenge that we face in this presentation as follows:\n",
    "“Geht es darum, die Sicherheit von Radwegen zu ermitteln, ist das Forschungspotential riesig. Ziel des SDSC-BW-Projekts war es, dieses zu erkunden. Ob der zahlreichen Problematiken gestaltete sich bereits die Erstellung eines Rahmenwerks schwierig. An erster Stelle standen die vielfältigen, teils verwirrenden Datenquellen – darunter unterschiedliche Websites, die Daten auf ihre eigene Weise abspeichern.”\n",
    "\n",
    "Results using chatgpt:\n",
    "The first is the response obtained using bing:\n",
    "\n",
    "<img src=\"./images/bing_result.png\" alt=\"feedback from bing search\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e78e7e-f1c9-454e-b4ba-4d08172bf6f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "As we can see, there is no way for chaptgpt to give valid feedback, and the web page it provides is not directly related to our purpose.\n",
    "\n",
    "The feedback obtained using the ChatGPT web page dialogue would be better:\n",
    "\n",
    "<div style=\"display: inline-block;\"><img src=\"./images/chatgpt_result.png\" alt=\"feedback from chatgpt chatbot\" width=\"700\" height=\"500\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf6bb8-dd78-4268-a653-5b670b77010e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Although chatgpt4 can summarize the challenges faced, the tools provided are less focused and have been developed over a longer period of time. In addition, there are many additional descriptions that make the responses look complicated.\n",
    "\n",
    "# method\n",
    "To address the problems of chatgpt, such as lack of ability to analyze complex input, complex responses, inability to provide real-time tools, and possible errors in the links provided, our core idea is to\n",
    "1. decompose the requirements and only ask simple questions to gpt at a time\n",
    "2. restrict the output so that the output is brief and linked to the topic\n",
    "3. use github api to get the latest github, to ensure the popularity and effectiveness of the tool\n",
    "\n",
    "Here we start to show how we do it, first of all, load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f6688b-130a-431d-a127-bd2c2462a19e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from git_request import search_top_starred_repositories\n",
    "from gpt_request import get_response_from_chatgpt_with_context\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479fbe7b-cef2-459a-99a5-92343ebb69d5",
   "metadata": {},
   "source": [
    "Where git_request and gpt_request are both custom packages for wanting github server to request github list and asking openai to request chatgpt service respectively.\n",
    "pandas is a data analysis tool and request is a network access tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9af270-3974-4054-965a-0a3445c39881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input the use case:  Geht es darum, die Sicherheit von Radwegen zu ermitteln, ist das Forschungspotential riesig. Ziel des SDSC-BW-Projekts war es, dieses zu erkunden. Ob der zahlreichen Problematiken gestaltete sich bereits die Erstellung eines Rahmenwerks schwierig. An erster Stelle standen die vielfältigen, teils verwirrenden Datenquellen – darunter unterschiedliche Websites, die Daten auf ihre eigene Weise abspeichern.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The theme of the use case is: Radwegsicherheit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_case = input(\"Please input the use case: \")\n",
    "\n",
    "# get the theme from the user case\n",
    "prompt = f\"What is the theme studied in the following use case, please answer only with a keyword less then 20 letter: {use_case}\"\n",
    "context = []\n",
    "response, context = get_response_from_chatgpt_with_context(prompt, context)\n",
    "print(f\"\\nThe theme of the use case is: {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa85abf-d541-4b07-990d-592bbbe6fa50",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we let gpt analyze the topic corresponding to the input, and we saw that gpt was able to do this well. Here, to limit the brevity of the answer, we restricted the output of gpt by adding the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214ef94c-dcc7-4433-8d23-b9d66b8bc086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main challenge facing are: \n",
      "Multiple data sources, confusing data, complex framework.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the challenge from the user case\n",
    "prompt = f\"According to the given description, what is the main problem faced by this study, please answer with 3 keywords and without explanation\"\n",
    "response, context = get_response_from_chatgpt_with_context(prompt, context)\n",
    "print(f\"The main challenge facing are: \\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805714c-1297-4cc1-a8b2-640700cf8956",
   "metadata": {},
   "source": [
    "We then asked for a summary of the three challenges faced given the topic. Because this challenge is to be applied to a subsequent github search, we asked gpt to respond using keywords and without giving an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e932e864-ec26-40f4-9061-a0a6f6bf216a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping is needed to extract and collect data from various online sources, including websites, in order to obtain a comprehensive dataset for analyzing and assessing the safety of bike lanes.\n",
      "For this, we recommend the following tools:\n",
      "Repository URL: https://github.com/khuyentran1401/Data-science\n",
      "README URL: https://github.com/khuyentran1401/Data-science/blob/master/README.md\n",
      "Repository URL: https://github.com/stanfordjournalism/search-script-scrape\n",
      "README URL: https://github.com/stanfordjournalism/search-script-scrape/blob/master/README.md\n",
      "Repository URL: https://github.com/hhursev/recipe-scrapers\n",
      "README URL: https://github.com/hhursev/recipe-scrapers/blob/master/README.md\n",
      "Repository URL: https://github.com/scrapy/parsel\n",
      "README URL: https://github.com/scrapy/parsel/blob/master/README.md\n",
      "Repository URL: https://github.com/damklis/DataEngineeringProject\n",
      "README URL: https://github.com/damklis/DataEngineeringProject/blob/master/README.md\n",
      "--------------------------------------------------\n",
      "Data integration is required to consolidate and harmonize the extracted data from different sources and formats with different levels of quality and completeness. The integrated dataset enables a more accurate and comprehensive analysis of the bike lane safety.\n",
      "For this, we recommend the following tools:\n",
      "Repository URL: https://github.com/airbytehq/airbyte\n",
      "README URL: https://github.com/airbytehq/airbyte/blob/master/README.md\n",
      "Repository URL: https://github.com/mage-ai/mage-ai\n",
      "README URL: https://github.com/mage-ai/mage-ai/blob/master/README.md\n",
      "Repository URL: https://github.com/django-import-export/django-import-export\n",
      "README URL: https://github.com/django-import-export/django-import-export/blob/master/README.md\n",
      "Repository URL: https://github.com/lancedb/lance\n",
      "README URL: https://github.com/lancedb/lance/blob/master/README.md\n",
      "Repository URL: https://github.com/cdapio/cdap\n",
      "README URL: https://github.com/cdapio/cdap/blob/master/README.md\n",
      "--------------------------------------------------\n",
      "Data analysis is essential to interpret and visualize the integrated and harmonized data. It provides insights into the safety of the bike lanes, identifies the problem areas, and enables policymakers to make data-driven decisions to improve the safety of bike lanes.\n",
      "For this, we recommend the following tools:\n",
      "Repository URL: https://github.com/fighting41love/funNLP\n",
      "README URL: https://github.com/fighting41love/funNLP/blob/master/README.md\n",
      "Repository URL: https://github.com/pandas-dev/pandas\n",
      "README URL: https://github.com/pandas-dev/pandas/blob/master/README.md\n",
      "Repository URL: https://github.com/wesm/pydata-book\n",
      "README URL: https://github.com/wesm/pydata-book/blob/master/README.md\n",
      "Repository URL: https://github.com/davisking/dlib\n",
      "README URL: https://github.com/davisking/dlib/blob/master/README.md\n",
      "Repository URL: https://github.com/tangyudi/Ai-Learn\n",
      "README URL: https://github.com/tangyudi/Ai-Learn/blob/master/README.md\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ask for keywords for python tools\n",
    "prompt = f\"I want to search for python tools for the above problem by keywords, what keywords should I use, please give me 3 suggestions and speperate them with semicolon, without explanation\"\n",
    "response, context = get_response_from_chatgpt_with_context(prompt, context)\n",
    "keywords = response.split(\";\")\n",
    "\n",
    "# list the advised git repos\n",
    "for keyword in keywords:\n",
    "    prompt = f\"Please explain why data {keyword} is needed in the context of the use case above, and please answer in less than 50 words\"\n",
    "    response, context = get_response_from_chatgpt_with_context(prompt, context)\n",
    "    print(response)\n",
    "\n",
    "    git_urls, readme_urls = search_top_starred_repositories(keyword+' python')\n",
    "    if git_urls is not None:\n",
    "        print(\"For this, we recommend the following tools:\")\n",
    "        for git_url, readme_url in zip(git_urls, readme_urls):\n",
    "            print(\"Repository URL:\", git_url)\n",
    "            print(\"README URL:\", readme_url)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4863475-ddba-4e10-95e8-84aeee5f98b7",
   "metadata": {},
   "source": [
    "Based on each keyword provided above, we find matching tools from the github server and return the top 5 results ranked according to how widely they are used. Since the results are provided through the github server, this ensures the reliability of the results. In addition, because we match by summarized keywords, the results obtained are closely related to our research topic.\n",
    "\n",
    "The keyword search is based on the following three contents:\n",
    "1. the name and description of the repository\n",
    "2. source code and file contents\n",
    "3. issue\n",
    "\n",
    "This ensures that the repository can be searched even if the repository owner's keywords are set incorrectly, and because the keywords are summarized by chatgpt, this ensures that the keywords obtained do not contain any out-of-the-ordinary words.\n",
    "\n",
    "# localization\n",
    "Of course, there are disadvantages to the above operation. For example, the keywords must appear in the content of the repository; they must be connected to the network.\n",
    "\n",
    "The first problem can be solved by word embedding. By word embedding the keywords and the readme file of the repository, we can find the matching repository by comparing the similarity of the two embeddings. The second problem can be solved by local deployment of gpt. openai's chatgpt-4 is not yet open source, but there are already many open source alternatives. We can achieve our goal by deploying these alternatives locally.\n",
    "\n",
    "To do this we first load two custom methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dddad50-6e33-43b0-a751-d583e306faf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from generate_local_github_database import download_and_save_git_stared_reposiories\n",
    "from git_request import search_top_related_local_repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ef818-ca54-4e3a-b916-8c4bd3b08850",
   "metadata": {
    "tags": []
   },
   "source": [
    "download_and_save_git_stared_reposiories implements the operation of downloading the specified repository information and saving it locally. search_top_related_local_repositories implements the deployment of local gpt, the implementation of word embedding and the search of local databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c523dca-d08e-4e3a-ac18-163bdc718646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping is necessary to collect data from various online sources, including websites, for analyzing the safety of bike lanes. It automates the data collection process and creates a comprehensive dataset for further analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/kit/tm/hj7422/conda/envs/vicuna/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n",
      "Unable to connect optimized C data functions [No module named 'clickhouse_connect.driverc.buffer'], falling back to pure Python\n",
      "Unable to connect ClickHouse Connect C to Numpy API [No module named 'clickhouse_connect.driverc.npconv'], falling back to pure Python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this, we recommend the following tools:\n",
      "Repository URL: https://github.com/RaRe-Technologies/gensim\n",
      "README URL: https://github.com/RaRe-Technologies/gensim/blob/master/README.md\n",
      "Repository URL: https://github.com/explosion/sense2vec\n",
      "README URL: https://github.com/explosion/sense2vec/blob/master/README.md\n",
      "Repository URL: https://github.com/BrikerMan/Kashgari\n",
      "README URL: https://github.com/BrikerMan/Kashgari/blob/master/README.md\n",
      "Repository URL: https://github.com/gnes-ai/gnes\n",
      "README URL: https://github.com/gnes-ai/gnes/blob/master/README.md\n",
      "Repository URL: https://github.com/github/semantic\n",
      "README URL: https://github.com/github/semantic/blob/master/README.md\n",
      "--------------------------------------------------\n",
      "Data wrangling is necessary to clean, transform, and harmonize the extracted dataset from different online sources. It enables data cleaning, data manipulation, and integration to ensure consistency, quality, and accuracy of the data used for analyzing the safety of bike lanes.\n",
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "For this, we recommend the following tools:\n",
      "Repository URL: https://github.com/cpitclaudel/dBoost\n",
      "README URL: https://github.com/cpitclaudel/dBoost/blob/master/README.md\n",
      "Repository URL: https://github.com/amundsen-io/amundsen\n",
      "README URL: https://github.com/amundsen-io/amundsen/blob/master/README.md\n",
      "Repository URL: https://github.com/doccano/doccano\n",
      "README URL: https://github.com/doccano/doccano/blob/master/README.md\n",
      "Repository URL: https://github.com/dmlc/gluon-nlp\n",
      "README URL: https://github.com/dmlc/gluon-nlp/blob/master/README.md\n",
      "Repository URL: https://github.com/flairNLP/flair\n",
      "README URL: https://github.com/flairNLP/flair/blob/master/README.md\n",
      "--------------------------------------------------\n",
      "Data visualization is necessary to present the analyzed data in a clear and visually appealing format. It helps to identify trends, patterns, and insight to stakeholders, policymakers, and residents, promoting data-driven decision-making to improve the safety of bike lanes.\n",
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n",
      "For this, we recommend the following tools:\n",
      "Repository URL: https://github.com/PAIR-code/facets\n",
      "README URL: https://github.com/PAIR-code/facets/blob/master/README.md\n",
      "Repository URL: https://github.com/amundsen-io/amundsen\n",
      "README URL: https://github.com/amundsen-io/amundsen/blob/master/README.md\n",
      "Repository URL: https://github.com/bokeh/bokeh\n",
      "README URL: https://github.com/bokeh/bokeh/blob/master/README.md\n",
      "Repository URL: https://github.com/raghakot/keras-vis\n",
      "README URL: https://github.com/raghakot/keras-vis/blob/master/README.md\n",
      "Repository URL: https://github.com/yosinski/deep-visualization-toolbox\n",
      "README URL: https://github.com/yosinski/deep-visualization-toolbox/blob/master/README.md\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ask for keywords for python tools\n",
    "prompt = f\"I want to search for python tools for the above problem by keywords, what keywords should I use, please give me 3 suggestions and speperate them with semicolon, without explanation\"\n",
    "response, context = get_response_from_chatgpt_with_context(prompt, context)\n",
    "keywords = response.split(\";\")\n",
    "\n",
    "# list the advised git repos\n",
    "for keyword in keywords:\n",
    "    prompt = f\"Please explain why data {keyword} is needed in the context of the use case above, and please answer in less than 50 words\"\n",
    "    response, context = get_response_from_chatgpt_with_context(prompt, context)\n",
    "    print(response)\n",
    "\n",
    "    git_urls, readme_urls = search_top_related_local_repositories(keyword, database_path = './data/repositories.csv')\n",
    "    if git_urls is not None:\n",
    "        print(\"For this, we recommend the following tools:\")\n",
    "        for git_url, readme_url in zip(git_urls, readme_urls):\n",
    "            print(\"Repository URL:\", git_url)\n",
    "            print(\"README URL:\", readme_url)\n",
    "        print('-'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f730443-ba04-426d-832c-52326827a293",
   "metadata": {},
   "source": [
    "We can see that it works well locally as well. Also, since the test database we applied contains only two hundred different repositories and is more centralized, the results do not look as good as the online ones. But this can be solved by increasing the local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0746de5-8289-47ab-8a5b-cc22adddc193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicuna",
   "language": "python",
   "name": "vicuna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
